{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Forest Carbon CodeFest March 12-14, 2024 Forests have the potential to regulate a changing climate through uptake of carbon dioxide. However, this function is being compromised by increasing wildfires and other disturbances, particularly in the western U.S. The goal of the Forest Carbon Codefest is for teams to use cyberinfrastructure and a curated set of data to define their own scientific questions that advance our understanding of aboveground forest carbon dynamics as they relate to forest disturbance. Teams will then collaboratively answer that question in the form of a fully open, reproducible, and complete code script. At the end of the event, projects will be judged by the participant community and facilitation team, with a prize awarded to each member of the winning team. Our passionate cohort of participants for the event includes individuals from academia, land management, and industry, across a range of career stages. Code generated for the event can be found on our GitHub repo (or, in the case of team code products, linked on the 'Teams' page of this site), and is available for use under an MIT license. Details Event living agenda is linked here Where In-person at the University of Colorado, Boulder When March 12-14, 2024 Event Documents Event overview & official rules Project evaluation criteria Code of conduct Intellectual property participant agreement Guidelines for intellectual contributions and credit This event is hosted by ESIIL and the CU Boulder CIRES Earth Lab. It is funded by the National Science Foundation (via awards numbers DBI-2153040, DEB-1846384, and DEB-2017889), and subject to the NSF\u2019s terms and conditions","title":"Home"},{"location":"#forest-carbon-codefest","text":"March 12-14, 2024 Forests have the potential to regulate a changing climate through uptake of carbon dioxide. However, this function is being compromised by increasing wildfires and other disturbances, particularly in the western U.S. The goal of the Forest Carbon Codefest is for teams to use cyberinfrastructure and a curated set of data to define their own scientific questions that advance our understanding of aboveground forest carbon dynamics as they relate to forest disturbance. Teams will then collaboratively answer that question in the form of a fully open, reproducible, and complete code script. At the end of the event, projects will be judged by the participant community and facilitation team, with a prize awarded to each member of the winning team. Our passionate cohort of participants for the event includes individuals from academia, land management, and industry, across a range of career stages. Code generated for the event can be found on our GitHub repo (or, in the case of team code products, linked on the 'Teams' page of this site), and is available for use under an MIT license.","title":"Forest Carbon CodeFest"},{"location":"#details","text":"","title":"Details"},{"location":"#event-living-agenda-is-linked-here","text":"","title":"Event living agenda is linked here"},{"location":"#where","text":"In-person at the University of Colorado, Boulder","title":"Where"},{"location":"#when","text":"March 12-14, 2024","title":"When"},{"location":"#event-documents","text":"Event overview & official rules Project evaluation criteria Code of conduct Intellectual property participant agreement Guidelines for intellectual contributions and credit This event is hosted by ESIIL and the CU Boulder CIRES Earth Lab. It is funded by the National Science Foundation (via awards numbers DBI-2153040, DEB-1846384, and DEB-2017889), and subject to the NSF\u2019s terms and conditions","title":"Event Documents"},{"location":"breakout/","text":"Breakout prompts & dedicated working space Dedicated working space Each team will have a room that has been reserved for their use at all team times. Those rooms will shift for each breakout time, shown below. However, your team is also welcome to explore the building and find other spaces that make you more comfortable or creative. For example, you may want to check out: The SEEC Cafe dining area (north end of the building, lots of windows in the eating room!) The SEEC lobby (both north and south) The Earth Lab conference room (S340). Note that this room may sometimes be reserved by Earth Lab staff. The southern end of the first floor, S148 Outside if it is sunny! The grass or SEEC courtyard! You are also welcome to use other rooms if they are available, but please be aware that other classes, study groups, or workshops may have reserved them and kick you out. Day 1 Team Dedicated Space Morning team time Team 1 S221 Team 2 S149 Team 3 C325 Team 4 S240 Team 5 Viz Studio (S372A) Team 6 Viz Studio (S372B) Afternoon team time Team 1 Viz Studio (S372B) Team 2 S221 Team 3 S149 Team 4 C325 Team 5 S240 Team 6 Viz Studio (S372A) Day 2 Team Dedicated Space Morning team time Team 1 Viz Studio (S372A) Team 2 Viz Studio (S372B) Team 3 S221 Team 4 S149 Team 5 C325 Team 6 S240 Afternoon team time Team 1 S240 Team 2 Viz Studio (S372A) Team 3 Viz Studio (S372B) Team 4 S221 Team 5 S149 Team 6 C325 Day 3 Team Dedicated Space Morning team time Team 1 C325 Team 2 S240 Team 3 Viz Studio (S372A) Team 4 Viz Studio (S372B) Team 5 S221 Team 6 S149 Breakout Prompts For ease of access to breakout group prompts throughout the codefest. Breakout #0: Virtual meeting #3 Introduce yourselves! Please briefly share: Your preferred name and where you are currently based A skill or area of expertise that you feel you are bringing to the table Something you are worried about regarding the codefest Something that brings you joy What is a topic that you are excited to investigate for two days related to forest carbon in the Southern Rocky mountains? What datasets are you excited to potentially use? (Tuesday morning you will have ~2.5 hours to continue brainstorming, with a draft question ready by noon! So don't stress, this is just a first opportunity to get a sense of what your team is generally excited about.) Breakout #1: Day 1 morning team time In-person introductions Who are you and why are you excited to be here? Establish team norms Note-taking and documenting the flow of ideas Expectations for work outside of official event hours Brainstorm What will your team project be for the next 2.5 days?! This should be a specific scientific question related to forest carbon in the Southern Rocky Mountains that you think is potentially answerable (at least in a very rough form) by the end of the event. Think about\u2026 What are you each excited about and what skills do you have around the table that can be leveraged? What datasets are you familiar with and/or excited to work with? Spin up some instances and get familiar with the data! Evaluation criteria (linked on the website!) Bring back One spokesperson to talk for 1 minute Your specific, answerable scientific question One \u2018need\u2019 that you see, whether that is help accessing an additional dataset, guidance on a dataset already available, or just your first step to get cracking Breakout #2: Day 1 afternoon team time Establish: How are you going to divide work, responsibilities, and code workflows? How are you going to manage people working in different coding languages? Take time to explore the datasets you intend to use and make sure you know how to work with and visualize them. Map out an initial workflow. What are the steps you will need to take to get from start to 'finish'? Begin work! Breakout #3: Day 2 morning team time Code, code, code! Focus on concrete, tractable problems, and don't get sucked into unneccessary coding or debugging. Is there an easier or faster way to answer your question? Graphics and deliverables are your friend! Demonstrate the progress you're making and remember to document what you are doing and WHY you're making the decisions you are. Keep your repo up to date! Breakout #4: Day 2 afternoon team time Prepare your deliverables. What figures are necessary to tell the story of your project? What do you want in your presentation and on your team website? Breakout #5: Day 3 morning team time Finalize all deliverables, push them to your website & GitHub, and finalize your presentation!","title":"Breakout prompts"},{"location":"breakout/#breakout-prompts-dedicated-working-space","text":"","title":"Breakout prompts &amp; dedicated working space"},{"location":"breakout/#dedicated-working-space","text":"Each team will have a room that has been reserved for their use at all team times. Those rooms will shift for each breakout time, shown below. However, your team is also welcome to explore the building and find other spaces that make you more comfortable or creative. For example, you may want to check out: The SEEC Cafe dining area (north end of the building, lots of windows in the eating room!) The SEEC lobby (both north and south) The Earth Lab conference room (S340). Note that this room may sometimes be reserved by Earth Lab staff. The southern end of the first floor, S148 Outside if it is sunny! The grass or SEEC courtyard! You are also welcome to use other rooms if they are available, but please be aware that other classes, study groups, or workshops may have reserved them and kick you out. Day 1 Team Dedicated Space Morning team time Team 1 S221 Team 2 S149 Team 3 C325 Team 4 S240 Team 5 Viz Studio (S372A) Team 6 Viz Studio (S372B) Afternoon team time Team 1 Viz Studio (S372B) Team 2 S221 Team 3 S149 Team 4 C325 Team 5 S240 Team 6 Viz Studio (S372A) Day 2 Team Dedicated Space Morning team time Team 1 Viz Studio (S372A) Team 2 Viz Studio (S372B) Team 3 S221 Team 4 S149 Team 5 C325 Team 6 S240 Afternoon team time Team 1 S240 Team 2 Viz Studio (S372A) Team 3 Viz Studio (S372B) Team 4 S221 Team 5 S149 Team 6 C325 Day 3 Team Dedicated Space Morning team time Team 1 C325 Team 2 S240 Team 3 Viz Studio (S372A) Team 4 Viz Studio (S372B) Team 5 S221 Team 6 S149","title":"Dedicated working space"},{"location":"breakout/#breakout-prompts","text":"For ease of access to breakout group prompts throughout the codefest.","title":"Breakout Prompts"},{"location":"breakout/#breakout-0-virtual-meeting-3","text":"Introduce yourselves! Please briefly share: Your preferred name and where you are currently based A skill or area of expertise that you feel you are bringing to the table Something you are worried about regarding the codefest Something that brings you joy What is a topic that you are excited to investigate for two days related to forest carbon in the Southern Rocky mountains? What datasets are you excited to potentially use? (Tuesday morning you will have ~2.5 hours to continue brainstorming, with a draft question ready by noon! So don't stress, this is just a first opportunity to get a sense of what your team is generally excited about.)","title":"Breakout #0: Virtual meeting #3"},{"location":"breakout/#breakout-1-day-1-morning-team-time","text":"In-person introductions Who are you and why are you excited to be here? Establish team norms Note-taking and documenting the flow of ideas Expectations for work outside of official event hours Brainstorm What will your team project be for the next 2.5 days?! This should be a specific scientific question related to forest carbon in the Southern Rocky Mountains that you think is potentially answerable (at least in a very rough form) by the end of the event. Think about\u2026 What are you each excited about and what skills do you have around the table that can be leveraged? What datasets are you familiar with and/or excited to work with? Spin up some instances and get familiar with the data! Evaluation criteria (linked on the website!) Bring back One spokesperson to talk for 1 minute Your specific, answerable scientific question One \u2018need\u2019 that you see, whether that is help accessing an additional dataset, guidance on a dataset already available, or just your first step to get cracking","title":"Breakout #1: Day 1 morning team time"},{"location":"breakout/#breakout-2-day-1-afternoon-team-time","text":"Establish: How are you going to divide work, responsibilities, and code workflows? How are you going to manage people working in different coding languages? Take time to explore the datasets you intend to use and make sure you know how to work with and visualize them. Map out an initial workflow. What are the steps you will need to take to get from start to 'finish'? Begin work!","title":"Breakout #2: Day 1 afternoon team time"},{"location":"breakout/#breakout-3-day-2-morning-team-time","text":"Code, code, code! Focus on concrete, tractable problems, and don't get sucked into unneccessary coding or debugging. Is there an easier or faster way to answer your question? Graphics and deliverables are your friend! Demonstrate the progress you're making and remember to document what you are doing and WHY you're making the decisions you are. Keep your repo up to date!","title":"Breakout #3: Day 2 morning team time"},{"location":"breakout/#breakout-4-day-2-afternoon-team-time","text":"Prepare your deliverables. What figures are necessary to tell the story of your project? What do you want in your presentation and on your team website?","title":"Breakout #4: Day 2 afternoon team time"},{"location":"breakout/#breakout-5-day-3-morning-team-time","text":"Finalize all deliverables, push them to your website & GitHub, and finalize your presentation!","title":"Breakout #5: Day 3 morning team time"},{"location":"final-presentations/","text":"Final Presentations Final presentations from the Forest Carbon Codefest were given virtually and recorded. The recording is below.","title":"Final presentations"},{"location":"final-presentations/#final-presentations","text":"Final presentations from the Forest Carbon Codefest were given virtually and recorded. The recording is below.","title":"Final Presentations"},{"location":"teams/","text":"Links to team websites Group 1 Group 2 Group 3 Group 4 Group 5 Group 6 Full team list Participants in the Forest Carbon Codefest were divided into 6 teams of 5 participants. Those groups are listed below. First name Last name Email Team Tech Lead Charles Zhang charleszhang216@gmail.com 1 Alex Vierod alex.vierod@bezerocarbon.com 1 * Ethan Yackulic ethan@vibrantplanet.net 1 Nicole Hemming-Schroeder nicole.hemming-schroeder@colorado.edu 1 Leonard Strnad ljstrnadiii@gmail.com 2 * Mike Packard michael.packard@usda.gov 2 Mihir Bendre mihirobendre@utexas.edu 2 Sarah Hart Sarah.Hart@colostate.edu 2 Jiaming Lu jmlu@terpmail.umd.edu 3 * Martha Morrissey martha@pachama.com 3 Asha Paudel paudelasha@gmail.com 3 Erick Crockett erin.crockett@ubc.ca 3 Tatum VanHawkins Tatum.vanhawkins@gmail.com 3 Elizabeth Buhr buhre@caryinstitute.org 4 * Quintin (Hutch) Tyree qtyree@umass.edu 4 Max Joseph max.joseph@planet.com 4 Hilary Brumberg Hibr1472@colorado.edu 4 Kit Lewers krle4401@colorado.edu 4 Ryan McCarthy rcm@planet.com 5 * Rachel King rking@nceas.ucsb.edu 5 Julia Kent jkent@ucar.edu 5 Ashley Woolman ashley.woolman@colostate.edu 5 Shike Zhang zhangsk@umich.edu 5 Natalie Wiley natalie.wiley@brilliantearth.com 6 * Bre Powers brepowers@gmail.com 6 Kylen Solvik kyso1389@colorado.edu 6 Tyler Hoecker tyler.hoecker@vibrantplanet.net 6 Luis de Pablo Luis.dePablo@colorado.edu 6","title":"Codefest Teams"},{"location":"teams/#links-to-team-websites","text":"","title":"Links to team websites"},{"location":"teams/#group-1","text":"","title":"Group 1"},{"location":"teams/#group-2","text":"","title":"Group 2"},{"location":"teams/#group-3","text":"","title":"Group 3"},{"location":"teams/#group-4","text":"","title":"Group 4"},{"location":"teams/#group-5","text":"","title":"Group 5"},{"location":"teams/#group-6","text":"","title":"Group 6"},{"location":"teams/#full-team-list","text":"Participants in the Forest Carbon Codefest were divided into 6 teams of 5 participants. Those groups are listed below. First name Last name Email Team Tech Lead Charles Zhang charleszhang216@gmail.com 1 Alex Vierod alex.vierod@bezerocarbon.com 1 * Ethan Yackulic ethan@vibrantplanet.net 1 Nicole Hemming-Schroeder nicole.hemming-schroeder@colorado.edu 1 Leonard Strnad ljstrnadiii@gmail.com 2 * Mike Packard michael.packard@usda.gov 2 Mihir Bendre mihirobendre@utexas.edu 2 Sarah Hart Sarah.Hart@colostate.edu 2 Jiaming Lu jmlu@terpmail.umd.edu 3 * Martha Morrissey martha@pachama.com 3 Asha Paudel paudelasha@gmail.com 3 Erick Crockett erin.crockett@ubc.ca 3 Tatum VanHawkins Tatum.vanhawkins@gmail.com 3 Elizabeth Buhr buhre@caryinstitute.org 4 * Quintin (Hutch) Tyree qtyree@umass.edu 4 Max Joseph max.joseph@planet.com 4 Hilary Brumberg Hibr1472@colorado.edu 4 Kit Lewers krle4401@colorado.edu 4 Ryan McCarthy rcm@planet.com 5 * Rachel King rking@nceas.ucsb.edu 5 Julia Kent jkent@ucar.edu 5 Ashley Woolman ashley.woolman@colostate.edu 5 Shike Zhang zhangsk@umich.edu 5 Natalie Wiley natalie.wiley@brilliantearth.com 6 * Bre Powers brepowers@gmail.com 6 Kylen Solvik kyso1389@colorado.edu 6 Tyler Hoecker tyler.hoecker@vibrantplanet.net 6 Luis de Pablo Luis.dePablo@colorado.edu 6","title":"Full team list"},{"location":"virtual-meetings/","text":"Virtual meeting recordings There are three virtual meetings associated with the Forest Carbon Codefest. The recordings of those meetings are available here: Virtual meeting #1 This orientation meeting focused a number of broad underlying themes related to data, open science, and teamwork. Introduction to ESIIL and the event Working in diverse groups and what makes good teams Data sovereignty and responsible use Fundamentals of open science Virtual meeting #2 This training focused on the use of markdown, github, and cyverse for cloud-based collaboration and documentation of codefest projects. The spreadsheet to record your github username is here . Note that it may take a few days to get your username added to the github team required to push and pull from the team repos. Virtual meeting #3 This orientation is focused on getting your mind working on what might be possible for your team to accomplish during the Forest Carbon Codefest. We discuss access to data that you have available, multiple different perspectives on forest carbon, and how you may want to pull it all together.","title":"Pre-event virtual meetings"},{"location":"virtual-meetings/#virtual-meeting-recordings","text":"There are three virtual meetings associated with the Forest Carbon Codefest. The recordings of those meetings are available here:","title":"Virtual meeting recordings"},{"location":"virtual-meetings/#virtual-meeting-1","text":"This orientation meeting focused a number of broad underlying themes related to data, open science, and teamwork. Introduction to ESIIL and the event Working in diverse groups and what makes good teams Data sovereignty and responsible use Fundamentals of open science","title":"Virtual meeting #1"},{"location":"virtual-meetings/#virtual-meeting-2","text":"This training focused on the use of markdown, github, and cyverse for cloud-based collaboration and documentation of codefest projects. The spreadsheet to record your github username is here . Note that it may take a few days to get your username added to the github team required to push and pull from the team repos.","title":"Virtual meeting #2"},{"location":"virtual-meetings/#virtual-meeting-3","text":"This orientation is focused on getting your mind working on what might be possible for your team to accomplish during the Forest Carbon Codefest. We discuss access to data that you have available, multiple different perspectives on forest carbon, and how you may want to pull it all together.","title":"Virtual meeting #3"},{"location":"additional-resources/bilingualism_md/","text":"R and Python bilingualism Welcome to the R and Python bilingualism reference guide! If you\u2019re fluent in one of these languages but hesitant to learn the other, you\u2019re in the right place. The good news is that there are many similarities between R and Python that make it easy to switch between the two. Both R and Python are widely used in data science and are open-source, meaning that they are free to use and constantly being improved by the community. They both have extensive libraries for data analysis, visualization, and machine learning. In fact, many of the libraries in both languages have similar names and functions, such as Pandas in Python and data.table in R. While there are differences between the two languages, they can complement each other well. Python is versatile and scalable, making it ideal for large and complex projects such as web development and artificial intelligence. R, on the other hand, is known for its exceptional statistical capabilities and is often used in data analysis and modeling. Visualization is also easier in R, making it a popular choice for creating graphs and charts. By learning both R and Python, you\u2019ll be able to take advantage of the strengths of each language and create more efficient and robust data analysis workflows. Don\u2019t let the differences between the two languages intimidate you - once you become familiar with one, learning the other will be much easier. So, whether you\u2019re a Python enthusiast looking to expand your statistical analysis capabilities, or an R user interested in exploring the world of web development and artificial intelligence, this guide will help you become bilingual in R and Python. Install packages In R, packages can be installed from CRAN repository by using the install.packages() function: R code: # Install the dplyr package from CRAN install.packages(\"dplyr\") In Python, packages can be installed from the Anaconda repository by using the conda install command: Python code: # Install the pandas package from Anaconda !conda install pandas Loading libraries in R and Python In R, libraries can be loaded in the same way as before, using the library() function: R code: # Load the dplyr library library(dplyr) In Python, libraries can be loaded in the same way as before, using the import statement. Here\u2019s an example: Python code: # Load the pandas library import pandas as pd Note that the package or library must be installed from the respective repository before it can be loaded. Also, make sure you have the correct repository specified in your system before installing packages. By default, R uses CRAN as its primary repository, whereas Anaconda uses its own repository by default. reticulate The reticulate package lets you run both R and Python together in the R environment. R libraries are stored and managed in a repository called CRAN. You can download R packages with the install.packages() function install.packages(\"reticulate\") You only need to install packages once, but you need to mount those packages with the library() function each time you open R. library(reticulate) Python libraries are stored and managed in a few different libraries and their dependencies are not regulated as strictly as R libraries are in CRAN. It\u2019s easier to publish a python package but it can also be more cumbersome for users because you need to manage dependencies yourself. You can download python packages using both R and Python code py_install(\"laspy\") ## + '/Users/ty/opt/miniconda3/bin/conda' 'install' '--yes' '--prefix' '/Users/ty/opt/miniconda3/envs/earth-analytics-python' '-c' 'conda-forge' 'laspy' Now, let\u2019s create a Python list and assign it to a variable py_list: R code: py_list <- r_to_py(list(1, 2, 3)) We can now print out the py_list variable in Python using the py_run_string() function: R code: py_run_string(\"print(r.py_list)\") This will output [1, 2, 3] in the Python console. Now, let\u2019s create an R vector and assign it to a variable r_vec: R code: r_vec <- c(4, 5, 6) We can now print out the r_vec variable in R using the py$ syntax to access Python variables: R code: print(py$py_list) This will output [1, 2, 3] in the R console. We can also call Python functions from R using the py_call() function. For example, let\u2019s call the Python sum() function on the py_list variable and assign the result to an R variable r_sum: R code: r_sum <- py_call(\"sum\", args = list(py_list)) We can now print out the r_sum variable in R: R code: print(r_sum) This will output 6 in the R console. Load packages and change settings options(java.parameters = \"-Xmx5G\") library(r5r) library(sf) library(data.table) library(ggplot2) library(interp) library(dplyr) library(osmdata) library(ggthemes) library(sf) library(data.table) library(ggplot2) library(akima) library(dplyr) library(raster) library(osmdata) library(mapview) library(cowplot) library(here) library(testthat) import sys sys.argv.append([\"--max-memory\", \"5G\"]) import pandas as pd import geopandas import matplotlib.pyplot as plt import numpy as np import plotnine import contextily as cx import r5py import seaborn as sns R and Python are two popular programming languages used for data analysis, statistics, and machine learning. Although they share some similarities, there are some fundamental differences between them. Here\u2019s an example code snippet in R and Python to illustrate some of the differences: R Code: # Create a vector of numbers from 1 to 10 x <- 1:10 # Compute the mean of the vector mean_x <- mean(x) # Print the result print(mean_x) ## [1] 5.5 Python Code: # Import the numpy library for numerical operations import numpy as np # Create a numpy array of numbers from 1 to 10 x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) # Compute the mean of the array mean_x = np.mean(x) # Print the result print(mean_x) ## 5.5 In this example, we can see that there are several differences between R and Python: Syntax: R uses the assignment operator \\<- while Python uses the equals sign = for variable assignment. Libraries: Python relies heavily on external libraries such as numpy, pandas, and matplotlib for data analysis, while R has built-in functions for many data analysis tasks. Data types: R is designed to work with vectors and matrices, while Python uses lists and arrays. In the example above, we used the numpy library to create a numerical array in Python. Function names: Function names in R and Python can differ significantly. In the example above, we used the mean() function in R and the np.mean() function in Python to calculate the mean of the vector/array. These are just a few of the many differences between R and Python. Ultimately, the choice between the two languages will depend on your specific needs and preferences. Load saved data R Code: data(\"iris\") here() load(file=here(\"2_R_and_Py_bilingualism\", \"data\", \"iris_example_data.rdata\")) objects() Python code: Save data R Code: save(iris, file=here(\"2_R_and_Py_bilingualism\", \"data\", \"iris_example_data.rdata\")) write.csv(iris, file=here(\"2_R_and_Py_bilingualism\", \"data\", \"iris_example_data.csv\")) Python code: functions Both R and Python are powerful languages for writing functions that can take input, perform a specific task, and return output. R Code: # Define a function that takes two arguments and returns their sum sum_r <- function(a, b) { return(a + b) } # Call the function with two arguments and print the result result_r <- sum_r(3, 5) print(result_r) ## [1] 8 Python code: # Define a function that takes two arguments and returns their sum def sum_py(a, b): return a + b # Call the function with two arguments and print the result result_py = sum_py(3, 5) print(result_py) ## 8 In both cases, we define a function that takes two arguments and returns their sum. In R, we use the function keyword to define a function, while in Python, we use the def keyword. The function body in R is enclosed in curly braces, while in Python it is indented. There are a few differences in the syntax and functionality between the two approaches: Function arguments: In R, function arguments are separated by commas, while in Python they are enclosed in parentheses. The syntax for specifying default arguments and variable-length argument lists can also differ between the two languages. Return statement: In R, we use the return keyword to specify the return value of a function, while in Python, we simply use the return statement. Function names: Function names in R and Python can differ significantly. In the example above, we used the sum_r() function in R and the sum_py() function in Python to calculate the sum of two numbers. Data Plots R Code: # Load the \"ggplot2\" package for plotting library(ggplot2) # Generate some sample data x <- seq(1, 10, 1) y <- x + rnorm(10) # Create a scatter plot ggplot(data.frame(x, y), aes(x = x, y = y)) + geom_point() Python code: # Load the \"matplotlib\" library import matplotlib.pyplot as plt # Generate some sample data import numpy as np x = np.arange(1, 11) y = x + np.random.normal(0, 1, 10) #clear last plot plt.clf() # Create a scatter plot plt.scatter(x, y) plt.show() In both cases, we generate some sample data and create a scatter plot to visualize the relationship between the variables. There are a few differences in the syntax and functionality between the two approaches: Library and package names: In R, we use the ggplot2 package for plotting, while in Python, we use the matplotlib library. Data format: In R, we use a data frame to store the input data, while in Python, we use numpy arrays. Plotting functions: In R, we use the ggplot() function to create a new plot object, and then use the geom_point() function to create a scatter plot layer. In Python, we use the scatter() function from the matplotlib.pyplot module to create a scatter plot directly. Linear regression R Code: # Load the \"ggplot2\" package for plotting library(ggplot2) # Generate some sample data x <- seq(1, 10, 1) y <- x + rnorm(10) # Perform linear regression model_r <- lm(y ~ x) # Print the model summary summary(model_r) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.69344 -0.42336 0.08961 0.34778 1.56728 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) -0.1676 0.6781 -0.247 0.811 ## x 0.9750 0.1093 8.921 1.98e-05 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 0.9926 on 8 degrees of freedom ## Multiple R-squared: 0.9087, Adjusted R-squared: 0.8972 ## F-statistic: 79.59 on 1 and 8 DF, p-value: 1.976e-05 # Plot the data and regression line ggplot(data.frame(x, y), aes(x = x, y = y)) + geom_point() + geom_smooth(method = \"lm\", se = FALSE) ## `geom_smooth()` using formula = 'y ~ x' Python code: # Load the \"matplotlib\" and \"scikit-learn\" libraries import matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression # Generate some sample data import numpy as np x = np.arange(1, 11) y = x + np.random.normal(0, 1, 10) # Perform linear regression model_py = LinearRegression().fit(x.reshape(-1, 1), y) # Print the model coefficients print(\"Coefficients: \", model_py.coef_) ## Coefficients: [1.15539692] print(\"Intercept: \", model_py.intercept_) #clear last plot ## Intercept: -1.1291396173221218 plt.clf() # Plot the data and regression line plt.scatter(x, y) plt.plot(x, model_py.predict(x.reshape(-1, 1)), color='red') plt.show() In both cases, we generate some sample data with a linear relationship between x and y, and then perform a simple linear regression to estimate the slope and intercept of the line. We then plot the data and regression line to visualize the fit. There are a few differences in the syntax and functionality between the two approaches: Library and package names: In R, we use the lm() function from the base package to perform linear regression, while in Python, we use the LinearRegression() class from the scikit-learn library. Additionally, we use the ggplot2 package in R for plotting, while we use the matplotlib library in Python. Data format: In R, we can specify the dependent and independent variables in the formula used for regression. In Python, we need to reshape the input data to a two-dimensional array before fitting the model. Model summary: In R, we can use the summary() function to print a summary of the model, including the estimated coefficients, standard errors, and p-values. In Python, we need to print the coefficients and intercept separately. Random Forest R Code: # Load the \"randomForest\" package library(randomForest) # Load the \"iris\" dataset data(iris) # Split the data into training and testing sets set.seed(123) train_idx <- sample(1:nrow(iris), nrow(iris) * 0.7, replace = FALSE) train_data <- iris[train_idx, ] test_data <- iris[-train_idx, ] # Build a random forest model rf_model <- randomForest(Species ~ ., data = train_data, ntree = 500) # Make predictions on the testing set predictions <- predict(rf_model, test_data) # Calculate accuracy of the model accuracy <- sum(predictions == test_data$Species) / nrow(test_data) print(paste(\"Accuracy:\", accuracy)) ## [1] \"Accuracy: 0.977777777777778\" Python code: # Load the \"pandas\", \"numpy\", and \"sklearn\" libraries import pandas as pd import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split # Load the \"iris\" dataset iris = load_iris() # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=123) # Build a random forest model rf_model = RandomForestClassifier(n_estimators=500, random_state=123) rf_model.fit(X_train, y_train) # Make predictions on the testing set ## RandomForestClassifier(n_estimators=500, random_state=123) predictions = rf_model.predict(X_test) # Calculate accuracy of the model accuracy = sum(predictions == y_test) / len(y_test) print(\"Accuracy:\", accuracy) ## Accuracy: 0.9555555555555556 In both cases, we load the iris dataset and split it into training and testing sets. We then build a random forest model using the training data and evaluate its accuracy on the testing data. There are a few differences in the syntax and functionality between the two approaches: Library and package names: In R, we use the randomForest package to build random forest models, while in Python, we use the RandomForestClassifier class from the sklearn.ensemble module. We also use different libraries for loading and manipulating data (pandas and numpy in Python, and built-in datasets in R). Model parameters: The syntax for setting model parameters is slightly different in R and Python. For example, in R, we specify the number of trees using the ntree parameter, while in Python, we use the n_estimators parameter. Data format: In R, we use a data frame to store the input data, while in Python, we use numpy arrays. Basic streetmap from Open Street Map R Code: # Load the \"osmdata\" package for mapping library(osmdata) library(tmap) # Define the map location and zoom level bbox <- c(left = -0.16, bottom = 51.49, right = -0.13, top = 51.51) # Get the OpenStreetMap data osm_data <- opq(bbox) %>% add_osm_feature(key = \"highway\") %>% osmdata_sf() # Plot the map using tmap tm_shape(osm_data$osm_lines) + tm_lines() Python code: # Load the \"osmnx\" package for mapping import osmnx as ox # Define the map location and zoom level bbox = (51.49, -0.16, 51.51, -0.13) # Get the OpenStreetMap data osm_data = ox.graph_from_bbox(north=bbox[2], south=bbox[0], east=bbox[3], west=bbox[1], network_type='all') # Plot the map using osmnx ox.plot_graph(osm_data) ## (<Figure size 1600x1600 with 0 Axes>, <AxesSubplot:>) In both cases, we define the map location and zoom level, retrieve the OpenStreetMap data using the specified bounding box, and plot the map. The main differences between the two approaches are: Package names and syntax: In R, we use the osmdata package and its syntax to download and process the OpenStreetMap data, while in Python, we use the osmnx package and its syntax. Mapping libraries: In R, we use the tmap package to create a static map of the OpenStreetMap data, while in Python, we use the built-in ox.plot_graph function from the osmnx package to plot the map. CNN on Raster data R Code: # Load the \"keras\" package for building the CNN library(tensorflow) library(keras) # Load the \"raster\" package for working with raster data library(raster) # Load the \"magrittr\" package for pipe operator library(magrittr) # Load the data as a raster brick raster_data <- brick(\"raster_data.tif\") # Split the data into training and testing sets split_data <- sample(1:nlayers(raster_data), size = nlayers(raster_data)*0.8, replace = FALSE) train_data <- raster_data[[split_data]] test_data <- raster_data[[setdiff(1:nlayers(raster_data), split_data)]] # Define the CNN model model <- keras_model_sequential() %>% layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = \"relu\", input_shape = c(ncol(train_data), nrow(train_data), ncell(train_data))) %>% layer_max_pooling_2d(pool_size = c(2, 2)) %>% layer_dropout(rate = 0.25) %>% layer_flatten() %>% layer_dense(units = 128, activation = \"relu\") %>% layer_dropout(rate = 0.5) %>% layer_dense(units = nlayers(train_data), activation = \"softmax\") # Compile the model model %>% compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = \"accuracy\") # Train the model history <- model %>% fit(x = array(train_data), y = to_categorical(1:nlayers(train_data)), epochs = 10, validation_split = 0.2) # Evaluate the model model %>% evaluate(x = array(test_data), y = to_categorical(1:nlayers(test_data))) # Plot the model accuracy over time plot(history) Piping Piping is a powerful feature in both R and Python that allows for a more streamlined and readable code. However, the syntax for piping is slightly different between the two languages. In R, piping is done using the %>% operator from the magrittr package, while in Python, it is done using the | operator from the pandas package. Let\u2019s compare and contrast piping in R and Python with some examples: Piping in R In R, we can use the %>% operator to pipe output from one function to another, which can make our code more readable and easier to follow. Here\u2019s an example: R code: library(dplyr) # create a data frame df <- data.frame(x = c(1,2,3), y = c(4,5,6)) # calculate the sum of column x and y df %>% mutate(z = x + y) %>% summarize(sum_z = sum(z)) ## sum_z ## 1 21 In this example, we first create a data frame df with two columns x and y. We then pipe the output of df to mutate, which adds a new column z to the data frame that is the sum of x and y. Finally, we pipe the output to summarize, which calculates the sum of z and returns the result. Piping in Python In Python, we can use the | operator to pipe output from one function to another. However, instead of piping output from one function to another, we pipe a DataFrame to a method of the DataFrame. Here\u2019s an example: Python code: import pandas as pd # create a DataFrame df = pd.DataFrame({'x': [1,2,3], 'y': [4,5,6]}) # calculate the sum of column x and y (df.assign(z = df['x'] + df['y']) .agg(sum_z = ('z', 'sum'))) ## z ## sum_z 21 In this example, we first create a DataFrame df with two columns x and y. We then use the assign() method to add a new column z to the DataFrame that is the sum of x and y. Finally, we use the agg() method to calculate the sum of z and return the result. As we can see, the syntax for piping is slightly different between R and Python, but the concept remains the same. Piping can make our code more readable and easier to follow, which is an important aspect of creating efficient and effective code. R code: library(dplyr) library(ggplot2) iris %>% filter(Species == \"setosa\") %>% group_by(Sepal.Width) %>% summarise(mean.Petal.Length = mean(Petal.Length)) %>% mutate(Sepal.Width = as.factor(Sepal.Width)) %>% ggplot(aes(x = Sepal.Width, y = mean.Petal.Length)) + geom_bar(stat = \"identity\", fill = \"dodgerblue\") + labs(title = \"Mean Petal Length of Setosa by Sepal Width\", x = \"Sepal Width\", y = \"Mean Petal Length\") In this example, we start with the iris dataset and filter it to only include rows where the Species column is \u201csetosa\u201d. We then group the remaining rows by the Sepal.Width column and calculate the mean Petal.Length for each group. Next, we convert Sepal.Width to a factor variable to ensure that it is treated as a categorical variable in the visualization. Finally, we create a bar plot using ggplot2, with Sepal.Width on the x-axis and mean.Petal.Length on the y-axis. The resulting plot shows the mean petal length of setosa flowers for each sepal width category. Python code: import pandas as pd # Load the iris dataset and pipe it into the next function ( pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", header=None, names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']) # Select columns and pivot the dataset .loc[:, ['sepal_length', 'sepal_width', 'petal_length']] .melt(var_name='variable', value_name='value') # Group by variable and calculate mean .groupby('variable', as_index=False) .mean() # Filter for mean greater than 3.5 and sort by descending mean .query('value > 3.5') .sort_values('value', ascending=False) ) ## variable value ## 1 sepal_length 5.843333 ## 0 petal_length 3.758667 for loops Here is an example of a for loop in R: R code # Create a vector of numbers numbers <- c(1, 2, 3, 4, 5) # Use a for loop to print out each number in the vector for (i in numbers) { print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 In this example, the for loop iterates over each element in the numbers vector, assigning the current element to the variable i. The print(i) statement is then executed for each iteration, outputting the value of i. Here is the equivalent example in Python: Python code # Create a list of numbers numbers = [1, 2, 3, 4, 5] # Use a for loop to print out each number in the list for i in numbers: print(i) ## 1 ## 2 ## 3 ## 4 ## 5 In Python, the for loop iterates over each element in the numbers list, assigning the current element to the variable i. The print(i) statement is then executed for each iteration, outputting the value of i. Both languages also support nested for loops, which can be used to perform iterations over multiple dimensions, such as looping through a 2D array. Parallel Parallel computing is a technique used to execute multiple computational tasks simultaneously, which can significantly reduce the time required to complete a task. Both R and Python have built-in support for parallel computing, although the approaches are slightly different. In this answer, we will compare and contrast the parallel computing capabilities of R and Python, and provide working examples in code. Parallel computing in R In R, there are several packages that support parallel computing, such as parallel, foreach, and doParallel. The parallel package provides basic functionality for parallel computing, while foreach and doParallel provide higher-level abstractions that make it easier to write parallel code. Here is an example of using the foreach package to execute a loop in parallel: R code: library(foreach) library(doParallel) # Set up a parallel backend with 4 workers cl <- makeCluster(4) registerDoParallel(cl) # Define a function to apply in parallel myfunc <- function(x) { # some computation here return(x^2) } # Generate some data mydata <- 1:1000 # Apply the function to the data in parallel result <- foreach(i = mydata) %dopar% { myfunc(i) } # Stop the cluster stopCluster(cl) In this example, we use the makeCluster() function to set up a cluster with 4 workers, and the registerDoParallel() function to register the cluster as the parallel backend for foreach. We then define a function myfunc() that takes an input x and returns x^2. We generate some data mydata and use foreach to apply myfunc() to each element of mydata in parallel, using the %dopar% operator. R Tidyverse parallel In R Tidyverse, we can use the furrr package for parallel computing. Here\u2019s an example of using furrr to parallelize a map function: R Tidy code: library(tidyverse) library(furrr) # Generate a list of numbers numbers <- 1:10 # Use the future_map function from furrr to parallelize the map function plan(multisession) squares <- future_map(numbers, function(x) x^2) In this example, we first load the Tidyverse and furrr libraries. We then generate a list of numbers from 1 to 10. We then use the plan function to set the parallelization strategy to \u201cmultisession\u201d, which will use multiple CPU cores to execute the code. Finally, we use the future_map function from furrr to apply the function x^2 to each number in the list in parallel. Parallel computing in Python In Python, the standard library includes the multiprocessing module, which provides basic support for parallel computing. Additionally, there are several third-party packages that provide higher-level abstractions, such as joblib and dask. Here is an example of using the multiprocessing module to execute a loop in parallel: Python code: def square(x): return x**2 from multiprocessing import Pool # Generate a list of numbers numbers = list(range(1, 11)) # Use the map function and a pool of workers to parallelize the square function with Pool() as pool: squares = pool.map(square, numbers) print(squares) In this example, we define a function myfunc() that takes an input x and returns x^2. We generate some data mydata and use the Pool class from the multiprocessing module to set up a pool of 4 workers. We then use the map() method of the Pool class to apply myfunc() to each element of mydata in parallel. Comparison and contrast Both R and Python have built-in support for parallel computing, with similar basic functionality for creating and managing parallel processes. However, the higher-level abstractions differ between the two languages. In R, the foreach package provides a high-level interface that makes it easy to write parallel code, while in Python, the multiprocessing module provides a basic interface that can be extended using third-party packages like joblib and dask. Additionally, Python has better support for distributed computing using frameworks like Apache Spark, while R has better support for shared-memory parallelism using tools like data.table and ff. Data wrangling Data wrangling is an important part of any data analysis project, and both R and Python provide tools and libraries for performing this task. In this answer, we will compare and contrast data wrangling in R\u2019s tidyverse and Python\u2019s pandas library, with working examples in code. Data Wrangling in R Tidyverse The tidyverse is a collection of R packages designed for data science, and it includes several packages that are useful for data wrangling. One of the most popular packages is dplyr, which provides a grammar of data manipulation for data frames. Here is an example of using dplyr to filter, mutate, and summarize a data frame: R code library(dplyr) # Load data data(mtcars) # Filter for cars with more than 100 horsepower mtcars %>% filter(hp > 100) %>% # Add a new column with fuel efficiency in km per liter mutate(kmpl = 0.425 * mpg) %>% # Group by number of cylinders and summarize group_by(cyl) %>% summarize(mean_hp = mean(hp), mean_kmpl = mean(kmpl)) ## # A tibble: 3 \u00d7 3 ## cyl mean_hp mean_kmpl ## <dbl> <dbl> <dbl> ## 1 4 111 11.0 ## 2 6 122. 8.39 ## 3 8 209. 6.42 In this example, we first filter the mtcars data frame to only include cars with more than 100 horsepower. We then use mutate to create a new column with fuel efficiency in kilometers per liter. Finally, we group the data by the number of cylinders and calculate the mean horsepower and fuel efficiency. Data Wrangling in Python Pandas Pandas is a popular library for data manipulation in Python. It provides a data frame object similar to R\u2019s data frames, along with a wide range of functions for data wrangling. Here is an example of using pandas to filter, transform, and group a data frame: Python code: import pandas as pd # Load data mtcars = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/mtcars.csv') # Filter for cars with more than 100 horsepower filtered_mtcars = mtcars[mtcars['hp'] > 100] # Add a new column with fuel efficiency in km per liter filtered_mtcars['kmpl'] = 0.425 * filtered_mtcars['mpg'] # Group by number of cylinders and calculate mean horsepower and fuel efficiency grouped_mtcars = filtered_mtcars.groupby('cyl').agg({'hp': 'mean', 'kmpl': 'mean'}) In this example, we first load the mtcars data from a CSV file. We then filter the data to only include cars with more than 100 horsepower, using boolean indexing. We use the assign function to create a new column with fuel efficiency in kilometers per liter. Finally, we group the data by the number of cylinders and calculate the mean horsepower and fuel efficiency. Comparison Overall, both R\u2019s tidyverse and Python\u2019s pandas provide similar functionality for data wrangling. Both allow for filtering, transforming, and aggregating data frames. The syntax for performing these operations is slightly different between the two languages, with R using the %>% operator for chaining operations and Python using method chaining or the apply family of functions. One key difference between the two languages is that R\u2019s tidyverse provides a consistent grammar for data manipulation across its various packages, making it easier to learn and use. However, Python\u2019s pandas library has a larger developer community and is more versatile for use in other applications, such as web development or machine learning. In conclusion, both R and Python provide powerful tools for data wrangling, and the choice between the two ultimately depends on the specific needs of the user and their familiarity Data from API Retrieving data from an API is a common task in both R and Python. Here are examples of how to retrieve data from an API in both languages: Python To retrieve data from an API in Python, we can use the requests library. Here\u2019s an example of how to retrieve weather data from the OpenWeatherMap API: Python code: import requests url = 'https://api.openweathermap.org/data/2.5/weather?q=London,uk&appid=API_KEY' response = requests.get(url) data = response.json() print(data) This code retrieves the current weather data for London from the OpenWeatherMap API. We first construct the API URL with the location and API key, then use the requests.get() function to make a request to the API. We then extract the JSON data from the response using the .json() method and print the resulting data. R In R, we can use the httr package to retrieve data from an API. Here\u2019s an example of how to retrieve weather data from the OpenWeatherMap API in R: R code: library(httr) url <- 'https://api.openweathermap.org/data/2.5/weather?q=London,uk&appid=API_KEY' response <- GET(url) data <- content(response, 'text') print(data) This code is similar to the Python code above. We first load the httr library, then construct the API URL and use the GET() function to make a request to the API. We then extract the data from the response using the content() function and print the resulting data. Retrieving Data from an API in R Tidyverse In R Tidyverse, we can use the httr and jsonlite packages to retrieve and process data from an API. R code: # Load required packages library(httr) library(jsonlite) # Define API endpoint endpoint <- \"https://jsonplaceholder.typicode.com/posts\" # Retrieve data from API response <- GET(endpoint) # Extract content from response content <- content(response, \"text\") # Convert content to JSON json <- fromJSON(content) # Convert JSON to a data frame df <- as.data.frame(json) In the above example, we use the GET() function from the httr package to retrieve data from an API endpoint, and the content() function to extract the content of the response. We then use the fromJSON() function from the jsonlite package to convert the JSON content to a list, and the as.data.frame() function to convert the list to a data frame. Retrieving Data from an API in Python In Python, we can use the requests library to retrieve data from an API, and the json library to process the JSON data. Python code: # Load required libraries import requests import json # Define API endpoint endpoint = \"https://jsonplaceholder.typicode.com/posts\" # Retrieve data from API response = requests.get(endpoint) # Extract content from response content = response.content # Convert content to JSON json_data = json.loads(content) # Convert JSON to a list of dictionaries data = [dict(row) for row in json_data] In the above example, we use the get() function from the requests library to retrieve data from an API endpoint, and the content attribute to extract the content of the response. We then use the loads() function from the json library to convert the JSON content to a list of dictionaries. Comparison Both R Tidyverse and Python provide powerful tools for retrieving and processing data from an API. In terms of syntax, the two languages are somewhat similar. In both cases, we use a library to retrieve data from the API, extract the content of the response, and then process the JSON data. However, there are some differences in the specific functions and methods used. For example, in R Tidyverse, we use the content() function to extract the content of the response, whereas in Python, we use the content attribute. Additionally, in R Tidyverse, we use the fromJSON() function to convert the JSON data to a list, whereas in Python, we use the loads() function. Census data Retrieving USA census data in R, R Tidy, and Python can be done using different packages and libraries. Here are some working examples in code for each language: R: To retrieve census data in R, we can use the tidycensus package. Here\u2019s an example of how to retrieve the total population for the state of California: R code: library(tidycensus) library(tidyverse) # Set your Census API key census_api_key(\"your_api_key\") # Get the total population for the state of California ca_pop <- get_acs( geography = \"state\", variables = \"B01003_001\", state = \"CA\" ) %>% rename(total_population = estimate) %>% select(total_population) # View the result ca_pop R Tidy: To retrieve census data in R Tidy, we can also use the tidycensus package. Here\u2019s an example of how to retrieve the total population for the state of California using pipes and dplyr functions: R tidy code: library(tidycensus) library(tidyverse) # Set your Census API key census_api_key(\"your_api_key\") # Get the total population for the state of California ca_pop <- get_acs( geography = \"state\", variables = \"B01003_001\", state = \"CA\" ) %>% rename(total_population = estimate) %>% select(total_population) # View the result ca_pop Python: To retrieve census data in Python, we can use the census library. Here\u2019s an example of how to retrieve the total population for the state of California: Python code: from census import Census from us import states import pandas as pd # Set your Census API key c = Census(\"your_api_key\") # Get the total population for the state of California ca_pop = c.acs5.state((\"B01003_001\"), states.CA.fips, year=2019) # Convert the result to a Pandas DataFrame ca_pop_df = pd.DataFrame(ca_pop) # Rename the column ca_pop_df = ca_pop_df.rename(columns={\"B01003_001E\": \"total_population\"}) # Select only the total population column ca_pop_df = ca_pop_df[[\"total_population\"]] # View the result ca_pop_df Lidar data To find Lidar data in R and Python, you typically need to start by identifying sources of Lidar data and then accessing them using appropriate packages and functions. Here are some examples of how to find Lidar data in R and Python: R: Identify sources of Lidar data: The USGS National Map Viewer provides access to Lidar data for the United States. You can also find Lidar data on state and local government websites, as well as on commercial data providers\u2019 websites. Access the data: You can use the lidR package in R to download and read Lidar data in the LAS format. For example, the following code downloads and reads Lidar data for a specific area: R code: library(lidR) # Download Lidar data LASfile <- system.file(\"extdata\", \"Megaplot.laz\", package=\"lidR\") lidar <- readLAS(LASfile) # Visualize the data plot(lidar) Python: Identify sources of Lidar data: The USGS 3DEP program provides access to Lidar data for the United States. You can also find Lidar data on state and local government websites, as well as on commercial data providers\u2019 websites. Access the data: You can use the pylastools package in Python to download and read Lidar data in the LAS format. For example, the following code downloads and reads Lidar data for a specific area: Python code: py_install(\"requests\") py_install(\"pylas\") py_install(\"laspy\") import requests from pylas import read import laspy import numpy as np # Download Lidar data url = \"https://s3-us-west-2.amazonaws.com/usgs-lidar-public/USGS_LPC_CA_SanFrancisco_2016_LAS_2018.zip\" lasfile = \"USGS_LPC_CA_SanFrancisco_2016_LAS_2018.las\" r = requests.get(url, allow_redirects=True) open(lasfile, 'wb').write(r.content) # Read the data lidar = read(lasfile) # Visualize the data laspy.plot.plot(lidar) Data for black lives Data for Black Lives ( https://d4bl.org/ ) is a movement that uses data science to create measurable change in the lives of Black people. While the Data for Black Lives website provides resources, reports, articles, and datasets related to racial equity, it doesn\u2019t provide a direct API for downloading data. Instead, you can access the Data for Black Lives GitHub repository ( https://github.com/Data4BlackLives ) to find datasets and resources to work with. In this example, we\u2019ll use a sample dataset available at https://github.com/Data4BlackLives/covid-19/tree/master/data . The dataset \u201cCOVID19_race_data.csv\u201d contains COVID-19 race-related data. R: In R, we\u2019ll use the \u2018readr\u2019 and \u2018dplyr\u2019 packages to read, process, and analyze the dataset. R code: # Install and load necessary libraries library(readr) library(dplyr) # Read the CSV file url <- \"https://raw.githubusercontent.com/Data4BlackLives/covid-19/master/data/COVID19_race_data.csv\" data <- read_csv(url) # Basic information about the dataset print(dim(data)) print(head(data)) # Example analysis: calculate the mean of 'cases_total' by 'state' data %>% group_by(state) %>% summarize(mean_cases_total = mean(cases_total, na.rm = TRUE)) %>% arrange(desc(mean_cases_total)) Python: In Python, we\u2019ll use the \u2018pandas\u2019 library to read, process, and analyze the dataset. Python code: import pandas as pd # Read the CSV file url = \"https://raw.githubusercontent.com/Data4BlackLives/covid-19/master/data/COVID19_race_data.csv\" data = pd.read_csv(url) # Basic information about the dataset print(data.shape) print(data.head()) # Example analysis: calculate the mean of 'cases_total' by 'state' mean_cases_total = data.groupby(\"state\")[\"cases_total\"].mean().sort_values(ascending=False) print(mean_cases_total) In conclusion, both R and Python provide powerful libraries and tools for downloading, processing, and analyzing datasets, such as those found in the Data for Black Lives repository. The \u2018readr\u2019 and \u2018dplyr\u2019 libraries in R offer a simple and intuitive way to read and manipulate data, while the \u2018pandas\u2019 library in Python offers similar functionality with a different syntax. Depending on your preferred programming language and environment, both options can be effective in working with social justice datasets. Propublica Congress API The ProPublica Congress API provides information about the U.S. Congress members and their voting records. In this example, we\u2019ll fetch data about the current Senate members and calculate the number of members in each party. R: In R, we\u2019ll use the \u2018httr\u2019 and \u2018jsonlite\u2019 packages to fetch and process data from the ProPublica Congress API. R code: # load necessary libraries library(httr) library(jsonlite) # Replace 'your_api_key' with your ProPublica API key # # Fetch data about the current Senate members url <- \"https://api.propublica.org/congress/v1/117/senate/members.json\" response <- GET(url, add_headers(`X-API-Key` = api_key)) # Check if the request was successful if (http_status(response)$category == \"Success\") { data <- content(response, \"parsed\") members <- data$results[[1]]$members # Calculate the number of members in each party party_counts <- table(sapply(members, function(x) x$party)) print(party_counts) } else { print(http_status(response)$message) } ## ## D I ID R ## 49 1 2 51 Python: In Python, we\u2019ll use the \u2018requests\u2019 library to fetch data from the ProPublica Congress API and \u2018pandas\u2019 library to process the data. python code: # Install necessary libraries import requests import pandas as pd # Replace 'your_api_key' with your ProPublica API key api_key = \"your_api_key\" headers = {\"X-API-Key\": api_key} # Fetch data about the current Senate members url = \"https://api.propublica.org/congress/v1/117/senate/members.json\" response = requests.get(url, headers=headers) # Check if the request was successful if response.status_code == 200: data = response.json() members = data[\"results\"][0][\"members\"] # Calculate the number of members in each party party_counts = pd.DataFrame(members)[\"party\"].value_counts() print(party_counts) else: print(f\"Error: {response.status_code}\") In conclusion, both R and Python offer efficient ways to fetch and process data from APIs like the ProPublica Congress API. The \u2018httr\u2019 and \u2018jsonlite\u2019 libraries in R provide a straightforward way to make HTTP requests and parse JSON data, while the \u2018requests\u2019 library in Python offers similar functionality. The \u2018pandas\u2019 library in Python can be used for data manipulation and analysis, and R provides built-in functions like table() for aggregating data. Depending on your preferred programming language and environment, both options can be effective for working with the ProPublica Congress API. Nonprofit Explorer API by ProPublica The Nonprofit Explorer API by ProPublica provides data on tax-exempt organizations in the United States. In this example, we\u2019ll search for organizations with the keyword \u201ceducation\u201d and analyze the results. R: In R, we\u2019ll use the \u2018httr\u2019 and \u2018jsonlite\u2019 packages to fetch and process data from the Nonprofit Explorer API. R code: # Install and load necessary libraries library(httr) library(jsonlite) # Fetch data for organizations with the keyword \"education\" url <- \"https://projects.propublica.org/nonprofits/api/v2/search.json?q=education\" response <- GET(url) # Check if the request was successful if (http_status(response)$category == \"Success\") { data <- content(response, \"parsed\") organizations <- data$organizations # Count the number of organizations per state state_counts <- table(sapply(organizations, function(x) x$state)) print(state_counts) } else { print(http_status(response)$message) } ## ## AZ CA CO DC FL GA HI IL Indiana LA ## 3 22 6 5 3 2 1 2 1 1 ## MD MI MN MO MP MS NC NE NJ NM ## 1 2 5 3 1 1 2 2 2 1 ## NY OH OK Oregon PA TX UT VA WA WV ## 1 5 1 2 2 12 1 4 3 1 ## ZZ ## 2 Python: In Python, we\u2019ll use the \u2018requests\u2019 library to fetch data from the Nonprofit Explorer API and \u2018pandas\u2019 library to process the data. Python code: # Install necessary libraries import requests import pandas as pd # Fetch data for organizations with the keyword \"education\" url = \"https://projects.propublica.org/nonprofits/api/v2/search.json?q=education\" response = requests.get(url) # Check if the request was successful if response.status_code == 200: data = response.json() organizations = data[\"organizations\"] # Count the number of organizations per state state_counts = pd.DataFrame(organizations)[\"state\"].value_counts() print(state_counts) else: print(f\"Error: {response.status_code}\") ## CA 22 ## TX 12 ## CO 6 ## MN 5 ## OH 5 ## DC 5 ## VA 4 ## AZ 3 ## WA 3 ## MO 3 ## FL 3 ## IL 2 ## GA 2 ## NC 2 ## MI 2 ## Oregon 2 ## NE 2 ## ZZ 2 ## PA 2 ## NJ 2 ## HI 1 ## MS 1 ## NY 1 ## Indiana 1 ## NM 1 ## LA 1 ## UT 1 ## MD 1 ## MP 1 ## WV 1 ## OK 1 ## Name: state, dtype: int64 In conclusion, both R and Python offer efficient ways to fetch and process data from APIs like the Nonprofit Explorer API. The \u2018httr\u2019 and \u2018jsonlite\u2019 libraries in R provide a straightforward way to make HTTP requests and parse JSON data, while the \u2018requests\u2019 library in Python offers similar functionality. The \u2018pandas\u2019 library in Python can be used for data manipulation and analysis, and R provides built-in functions like table() for aggregating data. Depending on your preferred programming language and environment, both options can be effective for working with the Nonprofit Explorer API. Campaign Finance API by ProPublica The Campaign Finance API by the Federal Election Commission (FEC) provides data on campaign finance in U.S. federal elections. In this example, we\u2019ll fetch data about individual contributions for the 2020 election cycle and analyze the results. R: In R, we\u2019ll use the \u2018httr\u2019 and \u2018jsonlite\u2019 packages to fetch and process data from the Campaign Finance API. R code: # Install and load necessary libraries library(httr) library(jsonlite) # Fetch data about individual contributions for the 2020 election cycle url <- \"https://api.open.fec.gov/v1/schedules/schedule_a/?api_key='OGwpkX7tH5Jihs1qQcisKfVAMddJzmzouWKtKoby'&two_year_transaction_period=2020&sort_hide_null=false&sort_null_only=false&per_page=20&page=1\" response <- GET(url) # Check if the request was successful if (http_status(response)$category == \"Success\") { data <- content(response, \"parsed\") contributions <- data$results # Calculate the total contributions per state state_totals <- aggregate(contributions$contributor_state, by = list(contributions$contributor_state), FUN = sum) colnames(state_totals) <- c(\"State\", \"Total_Contributions\") print(state_totals) } else { print(http_status(response)$message) } ## [1] \"Client error: (403) Forbidden\" Python: In Python, we\u2019ll use the \u2018requests\u2019 library to fetch data from the Campaign Finance API and \u2018pandas\u2019 library to process the data. Python code: # Install necessary libraries import requests import pandas as pd # Fetch data about individual contributions for the 2020 election cycle url = \"https://api.open.fec.gov/v1/schedules/schedule_a/?api_key=your_api_key&two_year_transaction_period=2020&sort_hide_null=false&sort_null_only=false&per_page=20&page=1\" response = requests.get(url) # Check if the request was successful if response.status_code == 200: data = response.json() contributions = data[\"results\"] # Calculate the total contributions per state df = pd.DataFrame(contributions) state_totals = df.groupby(\"contributor_state\")[\"contribution_receipt_amount\"].sum() print(state_totals) else: print(f\"Error: {response.status_code}\") ## Error: 403 In conclusion, both R and Python offer efficient ways to fetch and process data from APIs like the Campaign Finance API. The \u2018httr\u2019 and \u2018jsonlite\u2019 libraries in R provide a straightforward way to make HTTP requests and parse JSON data, while the \u2018requests\u2019 library in Python offers similar functionality. The \u2018pandas\u2019 library in Python can be used for data manipulation and analysis, and R provides built-in functions like aggregate() for aggregating data. Depending on your preferred programming language and environment, both options can be effective for working with the Campaign Finance API. Note: Remember to replace your_api_key with your actual FEC API key in the code examples above. Historic Redlining Historic redlining data refers to data from the Home Owners\u2019 Loan Corporation (HOLC) that created residential security maps in the 1930s, which contributed to racial segregation and disinvestment in minority neighborhoods. One popular source for this data is the Mapping Inequality project ( https://dsl.richmond.edu/panorama/redlining/ ). In this example, we\u2019ll download historic redlining data for Philadelphia in the form of a GeoJSON file and analyze the data in R and Python. R: In R, we\u2019ll use the \u2018sf\u2019 and \u2018dplyr\u2019 packages to read and process the GeoJSON data. R code: # Install and load necessary libraries library(sf) library(dplyr) # Download historic redlining data for Philadelphia url <- \"https://dsl.richmond.edu/panorama/redlining/static/downloads/geojson/PAPhiladelphia1937.geojson\" philly_geojson <- read_sf(url) # Count the number of areas per HOLC grade grade_counts <- philly_geojson %>% group_by(holc_grade) %>% summarize(count = n()) plot(grade_counts) Python: In Python, we\u2019ll use the \u2018geopandas\u2019 library to read and process the GeoJSON data. Python code: # Install necessary libraries import geopandas as gpd # Download historic redlining data for Philadelphia url = \"https://dsl.richmond.edu/panorama/redlining/static/downloads/geojson/PAPhiladelphia1937.geojson\" philly_geojson = gpd.read_file(url) # Count the number of areas per HOLC grade grade_counts = philly_geojson[\"holc_grade\"].value_counts() print(grade_counts) ## B 28 ## D 26 ## C 18 ## A 10 ## Name: holc_grade, dtype: int64 In conclusion, both R and Python offer efficient ways to download and process historic redlining data in the form of GeoJSON files. The \u2018sf\u2019 package in R provides a simple way to read and manipulate spatial data, while the \u2018geopandas\u2019 library in Python offers similar functionality. The \u2018dplyr\u2019 package in R can be used for data manipulation and analysis, and Python\u2019s built-in functions like value_counts() can be used for aggregating data. Depending on your preferred programming language and environment, both options can be effective for working with historic redlining data. American Indian and Alaska Native Areas (AIANNH) In this example, we\u2019ll download and analyze the American Indian and Alaska Native Areas (AIANNH) TIGER/Line Shapefile from the U.S. Census Bureau. We\u2019ll download the data for the year 2020, and analyze the number of AIANNH per congressional district R: In R, we\u2019ll use the \u2018sf\u2019 and \u2018dplyr\u2019 packages to read and process the Shapefile data. R code: # Install and load necessary libraries library(sf) library(dplyr) # Download historic redlining data for Philadelphia url <- \"https://www2.census.gov/geo/tiger/TIGER2020/AIANNH/tl_2020_us_aiannh.zip\" temp_file <- tempfile(fileext = \".zip\") download.file(url, temp_file, mode = \"wb\") unzip(temp_file, exdir = tempdir()) # Read the Shapefile shapefile_path <- file.path(tempdir(), \"tl_2020_us_aiannh.shp\") aiannh <- read_sf(shapefile_path) # Count the number of AIANNH per congressional district state_counts <- aiannh %>% group_by(LSAD) %>% summarize(count = n()) print(state_counts[order(-state_counts$count),]) ## Simple feature collection with 26 features and 2 fields ## Geometry type: GEOMETRY ## Dimension: XY ## Bounding box: xmin: -174.236 ymin: 18.91069 xmax: -67.03552 ymax: 71.34019 ## Geodetic CRS: NAD83 ## # A tibble: 26 \u00d7 3 ## LSAD count geometry ## <chr> <int> <MULTIPOLYGON [\u00b0]> ## 1 79 221 (((-166.5331 65.33918, -166.5331 65.33906, -166.533 65.33699, -1\u2026 ## 2 86 206 (((-83.38811 35.46645, -83.38342 35.46596, -83.38316 35.46593, -\u2026 ## 3 OT 155 (((-92.32972 47.81374, -92.3297 47.81305, -92.32967 47.81196, -9\u2026 ## 4 78 75 (((-155.729 20.02457, -155.7288 20.02428, -155.7288 20.02427, -1\u2026 ## 5 85 46 (((-122.3355 37.95215, -122.3354 37.95206, -122.3352 37.95199, -\u2026 ## 6 92 35 (((-93.01356 31.56287, -93.01354 31.56251, -93.01316 31.56019, -\u2026 ## 7 88 25 (((-97.35299 36.908, -97.35291 36.90801, -97.35287 36.908, -97.3\u2026 ## 8 96 19 (((-116.48 32.63814, -116.48 32.63718, -116.4794 32.63716, -116.\u2026 ## 9 84 16 (((-105.5937 36.40379, -105.5937 36.40324, -105.5937 36.40251, -\u2026 ## 10 89 11 (((-95.91705 41.28037, -95.91653 41.28036, -95.91653 41.28125, -\u2026 ## # \u2139 16 more rows Python: In Python, we\u2019ll use the \u2018geopandas\u2019 library to read and process the Shapefile data. Python code: import geopandas as gpd import pandas as pd import requests import zipfile import os from io import BytesIO # Download historic redlining data for Philadelphia url = \"https://www2.census.gov/geo/tiger/TIGER2020/AIANNH/tl_2020_us_aiannh.zip\" response = requests.get(url) zip_file = zipfile.ZipFile(BytesIO(response.content)) # Extract Shapefile temp_dir = \"temp\" if not os.path.exists(temp_dir): os.makedirs(temp_dir) zip_file.extractall(path=temp_dir) shapefile_path = os.path.join(temp_dir, \"tl_2020_us_aiannh.shp\") # Read the Shapefile aiannh = gpd.read_file(shapefile_path) # Count the number of AIANNH per congressional district state_counts = aiannh.groupby(\"LSAD\").size().reset_index(name=\"count\") # Sort by descending count state_counts_sorted = state_counts.sort_values(by=\"count\", ascending=False) print(state_counts_sorted) ## LSAD count ## 2 79 221 ## 9 86 206 ## 25 OT 155 ## 1 78 75 ## 8 85 46 ## 15 92 35 ## 11 88 25 ## 19 96 19 ## 7 84 16 ## 12 89 11 ## 5 82 8 ## 3 80 7 ## 4 81 6 ## 21 98 5 ## 20 97 5 ## 13 90 4 ## 18 95 3 ## 6 83 3 ## 17 94 2 ## 16 93 1 ## 14 91 1 ## 10 87 1 ## 22 99 1 ## 23 9C 1 ## 24 9D 1 ## 0 00 1 In conclusion, both R and Python offer efficient ways to download and process AIANNH TIGER/Line Shapefile data from the U.S. Census Bureau. The \u2018sf\u2019 package in R provides a simple way to read and manipulate spatial data, while the \u2018geopandas\u2019 library in Python offers similar functionality. The \u2018dplyr\u2019 package in R can be used for data manipulation and analysis, and Python\u2019s built-in functions like value_counts() can be used for aggregating data. Depending on your preferred programming language and environment, both options can be effective for working with AIANNH data. Indian Entities Recognized and Eligible To Receive Services by BIA The Bureau of Indian Affairs (BIA) provides a PDF document containing a list of Indian Entities Recognized and Eligible To Receive Services. To analyze the data, we\u2019ll first need to extract the information from the PDF. In this example, we\u2019ll extract the names of the recognized tribes and count the number of tribes per state. R: In R, we\u2019ll use the \u2018pdftools\u2019 package to extract text from the PDF and the \u2018stringr\u2019 package to process the text data. R code: # Install and load necessary libraries library(pdftools) library(stringr) library(dplyr) # Download the BIA PDF url <- \"https://www.govinfo.gov/content/pkg/FR-2022-01-28/pdf/2022-01789.pdf\" temp_file <- tempfile(fileext = \".pdf\") download.file(url, temp_file, mode = \"wb\") # Extract text from the PDF pdf_text <- pdf_text(temp_file) tribe_text <- pdf_text[4:length(pdf_text)] # Define helper functions tribe_state_extractor <- function(text_line) { regex_pattern <- \"(.*),\\\\s+([A-Z]{2})$\" tribe_state <- str_match(text_line, regex_pattern) return(tribe_state) } is_valid_tribe_line <- function(text_line) { regex_pattern <- \"^\\\\d+\\\\s+\" return(!is.na(str_match(text_line, regex_pattern))) } # Process text data to extract tribes and states tribe_states <- sapply(tribe_text, tribe_state_extractor) valid_lines <- sapply(tribe_text, is_valid_tribe_line) tribe_states <- tribe_states[valid_lines, 2:3] # Count the number of tribes per state tribe_data <- as.data.frame(tribe_states) colnames(tribe_data) <- c(\"Tribe\", \"State\") state_counts <- tribe_data %>% group_by(State) %>% summarise(Count = n()) print(state_counts) ## # A tibble: 0 \u00d7 2 ## # \u2139 2 variables: State <chr>, Count <int> Python: In Python, we\u2019ll use the \u2018PyPDF2\u2019 library to extract text from the PDF and the \u2018re\u2019 module to process the text data. Python code: # Install necessary libraries import requests import PyPDF2 import io import re from collections import Counter # Download the BIA PDF url = \"https://www.bia.gov/sites/bia.gov/files/assets/public/raca/online-tribal-leaders-directory/tribal_leaders_2021-12-27.pdf\" response = requests.get(url) # Extract text from the PDF pdf_reader = PyPDF2.PdfFileReader(io.BytesIO(response.content)) tribe_text = [pdf_reader.getPage(i).extractText() for i in range(3, pdf_reader.numPages)] # Process text data to extract tribes and states tribes = [re.findall(r'^\\d+\\s+(.+),\\s+([A-Z]{2})', line) for text in tribe_text for line in text.split('\\n') if line] tribe_states = [state for tribe, state in tribes] # Count the number of tribes per state state_counts = Counter(tribe_states) print(state_counts) In conclusion, both R and Python offer efficient ways to download and process the list of Indian Entities Recognized and Eligible To Receive Services from the BIA. The \u2018pdftools\u2019 package in R provides a simple way to extract text from PDF files, while the \u2018PyPDF2\u2019 library in Python offers similar functionality. The \u2018stringr\u2019 package in R and the \u2018re\u2019 module in Python can be used to process and analyze text data. Depending on your preferred programming language and environment, both options can be effective for working with BIA data. National Atlas - Indian Lands of the United States dataset In this example, we will download and analyze the National Atlas - Indian Lands of the United States dataset in both R and Python. We will read the dataset and count the number of Indian lands per state. R: In R, we\u2019ll use the \u2018sf\u2019 package to read the Shapefile and the \u2018dplyr\u2019 package to process the data. R code: # Install and load necessary libraries library(sf) library(dplyr) # Download the Indian Lands dataset url <- \"https://prd-tnm.s3.amazonaws.com/StagedProducts/Small-scale/data/Boundaries/indlanp010g.shp_nt00968.tar.gz\" temp_file <- tempfile(fileext = \".tar.gz\") download.file(url, temp_file, mode = \"wb\") untar(temp_file, exdir = tempdir()) # Read the Shapefile shapefile_path <- file.path(tempdir(), \"indlanp010g.shp\") indian_lands <- read_sf(shapefile_path) # Count the number of Indian lands per state # state_counts <- indian_lands %>% # group_by(STATE) %>% # summarize(count = n()) plot(indian_lands) ## Warning: plotting the first 9 out of 23 attributes; use max.plot = 23 to plot ## all Python: In Python, we\u2019ll use the \u2018geopandas\u2019 and \u2018pandas\u2019 libraries to read the Shapefile and process the data. Python code: import geopandas as gpd import pandas as pd import requests import tarfile import os from io import BytesIO # Download the Indian Lands dataset url = \"https://prd-tnm.s3.amazonaws.com/StagedProducts/Small-scale/data/Boundaries/indlanp010g.shp_nt00966.tar.gz\" response = requests.get(url) tar_file = tarfile.open(fileobj=BytesIO(response.content), mode='r:gz') # Extract Shapefile temp_dir = \"temp\" if not os.path.exists(temp_dir): os.makedirs(temp_dir) tar_file.extractall(path=temp_dir) shapefile_path = os.path.join(temp_dir, \"indlanp010g.shp\") # Read the Shapefile indian_lands = gpd.read_file(shapefile_path) # Count the number of Indian lands per state state_counts = indian_lands.groupby(\"STATE\").size().reset_index(name=\"count\") print(state_counts) Both R and Python codes download the dataset and read the Shapefile using the respective packages. They then group the data by the \u2018STATE\u2019 attribute and calculate the count of Indian lands per state.","title":"R and Python bilingualism"},{"location":"additional-resources/bilingualism_md/#r-and-python-bilingualism","text":"Welcome to the R and Python bilingualism reference guide! If you\u2019re fluent in one of these languages but hesitant to learn the other, you\u2019re in the right place. The good news is that there are many similarities between R and Python that make it easy to switch between the two. Both R and Python are widely used in data science and are open-source, meaning that they are free to use and constantly being improved by the community. They both have extensive libraries for data analysis, visualization, and machine learning. In fact, many of the libraries in both languages have similar names and functions, such as Pandas in Python and data.table in R. While there are differences between the two languages, they can complement each other well. Python is versatile and scalable, making it ideal for large and complex projects such as web development and artificial intelligence. R, on the other hand, is known for its exceptional statistical capabilities and is often used in data analysis and modeling. Visualization is also easier in R, making it a popular choice for creating graphs and charts. By learning both R and Python, you\u2019ll be able to take advantage of the strengths of each language and create more efficient and robust data analysis workflows. Don\u2019t let the differences between the two languages intimidate you - once you become familiar with one, learning the other will be much easier. So, whether you\u2019re a Python enthusiast looking to expand your statistical analysis capabilities, or an R user interested in exploring the world of web development and artificial intelligence, this guide will help you become bilingual in R and Python.","title":"R and Python bilingualism"},{"location":"additional-resources/bilingualism_md/#install-packages","text":"In R, packages can be installed from CRAN repository by using the install.packages() function: R code: # Install the dplyr package from CRAN install.packages(\"dplyr\") In Python, packages can be installed from the Anaconda repository by using the conda install command: Python code: # Install the pandas package from Anaconda !conda install pandas Loading libraries in R and Python In R, libraries can be loaded in the same way as before, using the library() function: R code: # Load the dplyr library library(dplyr) In Python, libraries can be loaded in the same way as before, using the import statement. Here\u2019s an example: Python code: # Load the pandas library import pandas as pd Note that the package or library must be installed from the respective repository before it can be loaded. Also, make sure you have the correct repository specified in your system before installing packages. By default, R uses CRAN as its primary repository, whereas Anaconda uses its own repository by default.","title":"Install packages"},{"location":"additional-resources/bilingualism_md/#reticulate","text":"The reticulate package lets you run both R and Python together in the R environment. R libraries are stored and managed in a repository called CRAN. You can download R packages with the install.packages() function install.packages(\"reticulate\") You only need to install packages once, but you need to mount those packages with the library() function each time you open R. library(reticulate) Python libraries are stored and managed in a few different libraries and their dependencies are not regulated as strictly as R libraries are in CRAN. It\u2019s easier to publish a python package but it can also be more cumbersome for users because you need to manage dependencies yourself. You can download python packages using both R and Python code py_install(\"laspy\") ## + '/Users/ty/opt/miniconda3/bin/conda' 'install' '--yes' '--prefix' '/Users/ty/opt/miniconda3/envs/earth-analytics-python' '-c' 'conda-forge' 'laspy' Now, let\u2019s create a Python list and assign it to a variable py_list: R code: py_list <- r_to_py(list(1, 2, 3)) We can now print out the py_list variable in Python using the py_run_string() function: R code: py_run_string(\"print(r.py_list)\") This will output [1, 2, 3] in the Python console. Now, let\u2019s create an R vector and assign it to a variable r_vec: R code: r_vec <- c(4, 5, 6) We can now print out the r_vec variable in R using the py$ syntax to access Python variables: R code: print(py$py_list) This will output [1, 2, 3] in the R console. We can also call Python functions from R using the py_call() function. For example, let\u2019s call the Python sum() function on the py_list variable and assign the result to an R variable r_sum: R code: r_sum <- py_call(\"sum\", args = list(py_list)) We can now print out the r_sum variable in R: R code: print(r_sum) This will output 6 in the R console.","title":"reticulate"},{"location":"additional-resources/bilingualism_md/#load-packages-and-change-settings","text":"options(java.parameters = \"-Xmx5G\") library(r5r) library(sf) library(data.table) library(ggplot2) library(interp) library(dplyr) library(osmdata) library(ggthemes) library(sf) library(data.table) library(ggplot2) library(akima) library(dplyr) library(raster) library(osmdata) library(mapview) library(cowplot) library(here) library(testthat) import sys sys.argv.append([\"--max-memory\", \"5G\"]) import pandas as pd import geopandas import matplotlib.pyplot as plt import numpy as np import plotnine import contextily as cx import r5py import seaborn as sns R and Python are two popular programming languages used for data analysis, statistics, and machine learning. Although they share some similarities, there are some fundamental differences between them. Here\u2019s an example code snippet in R and Python to illustrate some of the differences: R Code: # Create a vector of numbers from 1 to 10 x <- 1:10 # Compute the mean of the vector mean_x <- mean(x) # Print the result print(mean_x) ## [1] 5.5 Python Code: # Import the numpy library for numerical operations import numpy as np # Create a numpy array of numbers from 1 to 10 x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) # Compute the mean of the array mean_x = np.mean(x) # Print the result print(mean_x) ## 5.5 In this example, we can see that there are several differences between R and Python: Syntax: R uses the assignment operator \\<- while Python uses the equals sign = for variable assignment. Libraries: Python relies heavily on external libraries such as numpy, pandas, and matplotlib for data analysis, while R has built-in functions for many data analysis tasks. Data types: R is designed to work with vectors and matrices, while Python uses lists and arrays. In the example above, we used the numpy library to create a numerical array in Python. Function names: Function names in R and Python can differ significantly. In the example above, we used the mean() function in R and the np.mean() function in Python to calculate the mean of the vector/array. These are just a few of the many differences between R and Python. Ultimately, the choice between the two languages will depend on your specific needs and preferences.","title":"Load packages and change settings"},{"location":"additional-resources/bilingualism_md/#load-saved-data","text":"R Code: data(\"iris\") here() load(file=here(\"2_R_and_Py_bilingualism\", \"data\", \"iris_example_data.rdata\")) objects() Python code:","title":"Load saved data"},{"location":"additional-resources/bilingualism_md/#save-data","text":"R Code: save(iris, file=here(\"2_R_and_Py_bilingualism\", \"data\", \"iris_example_data.rdata\")) write.csv(iris, file=here(\"2_R_and_Py_bilingualism\", \"data\", \"iris_example_data.csv\")) Python code:","title":"Save data"},{"location":"additional-resources/bilingualism_md/#functions","text":"Both R and Python are powerful languages for writing functions that can take input, perform a specific task, and return output. R Code: # Define a function that takes two arguments and returns their sum sum_r <- function(a, b) { return(a + b) } # Call the function with two arguments and print the result result_r <- sum_r(3, 5) print(result_r) ## [1] 8 Python code: # Define a function that takes two arguments and returns their sum def sum_py(a, b): return a + b # Call the function with two arguments and print the result result_py = sum_py(3, 5) print(result_py) ## 8 In both cases, we define a function that takes two arguments and returns their sum. In R, we use the function keyword to define a function, while in Python, we use the def keyword. The function body in R is enclosed in curly braces, while in Python it is indented. There are a few differences in the syntax and functionality between the two approaches: Function arguments: In R, function arguments are separated by commas, while in Python they are enclosed in parentheses. The syntax for specifying default arguments and variable-length argument lists can also differ between the two languages. Return statement: In R, we use the return keyword to specify the return value of a function, while in Python, we simply use the return statement. Function names: Function names in R and Python can differ significantly. In the example above, we used the sum_r() function in R and the sum_py() function in Python to calculate the sum of two numbers.","title":"functions"},{"location":"additional-resources/bilingualism_md/#data-plots","text":"R Code: # Load the \"ggplot2\" package for plotting library(ggplot2) # Generate some sample data x <- seq(1, 10, 1) y <- x + rnorm(10) # Create a scatter plot ggplot(data.frame(x, y), aes(x = x, y = y)) + geom_point() Python code: # Load the \"matplotlib\" library import matplotlib.pyplot as plt # Generate some sample data import numpy as np x = np.arange(1, 11) y = x + np.random.normal(0, 1, 10) #clear last plot plt.clf() # Create a scatter plot plt.scatter(x, y) plt.show() In both cases, we generate some sample data and create a scatter plot to visualize the relationship between the variables. There are a few differences in the syntax and functionality between the two approaches: Library and package names: In R, we use the ggplot2 package for plotting, while in Python, we use the matplotlib library. Data format: In R, we use a data frame to store the input data, while in Python, we use numpy arrays. Plotting functions: In R, we use the ggplot() function to create a new plot object, and then use the geom_point() function to create a scatter plot layer. In Python, we use the scatter() function from the matplotlib.pyplot module to create a scatter plot directly.","title":"Data Plots"},{"location":"additional-resources/bilingualism_md/#linear-regression","text":"R Code: # Load the \"ggplot2\" package for plotting library(ggplot2) # Generate some sample data x <- seq(1, 10, 1) y <- x + rnorm(10) # Perform linear regression model_r <- lm(y ~ x) # Print the model summary summary(model_r) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.69344 -0.42336 0.08961 0.34778 1.56728 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) -0.1676 0.6781 -0.247 0.811 ## x 0.9750 0.1093 8.921 1.98e-05 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 0.9926 on 8 degrees of freedom ## Multiple R-squared: 0.9087, Adjusted R-squared: 0.8972 ## F-statistic: 79.59 on 1 and 8 DF, p-value: 1.976e-05 # Plot the data and regression line ggplot(data.frame(x, y), aes(x = x, y = y)) + geom_point() + geom_smooth(method = \"lm\", se = FALSE) ## `geom_smooth()` using formula = 'y ~ x' Python code: # Load the \"matplotlib\" and \"scikit-learn\" libraries import matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression # Generate some sample data import numpy as np x = np.arange(1, 11) y = x + np.random.normal(0, 1, 10) # Perform linear regression model_py = LinearRegression().fit(x.reshape(-1, 1), y) # Print the model coefficients print(\"Coefficients: \", model_py.coef_) ## Coefficients: [1.15539692] print(\"Intercept: \", model_py.intercept_) #clear last plot ## Intercept: -1.1291396173221218 plt.clf() # Plot the data and regression line plt.scatter(x, y) plt.plot(x, model_py.predict(x.reshape(-1, 1)), color='red') plt.show() In both cases, we generate some sample data with a linear relationship between x and y, and then perform a simple linear regression to estimate the slope and intercept of the line. We then plot the data and regression line to visualize the fit. There are a few differences in the syntax and functionality between the two approaches: Library and package names: In R, we use the lm() function from the base package to perform linear regression, while in Python, we use the LinearRegression() class from the scikit-learn library. Additionally, we use the ggplot2 package in R for plotting, while we use the matplotlib library in Python. Data format: In R, we can specify the dependent and independent variables in the formula used for regression. In Python, we need to reshape the input data to a two-dimensional array before fitting the model. Model summary: In R, we can use the summary() function to print a summary of the model, including the estimated coefficients, standard errors, and p-values. In Python, we need to print the coefficients and intercept separately.","title":"Linear regression"},{"location":"additional-resources/bilingualism_md/#random-forest","text":"R Code: # Load the \"randomForest\" package library(randomForest) # Load the \"iris\" dataset data(iris) # Split the data into training and testing sets set.seed(123) train_idx <- sample(1:nrow(iris), nrow(iris) * 0.7, replace = FALSE) train_data <- iris[train_idx, ] test_data <- iris[-train_idx, ] # Build a random forest model rf_model <- randomForest(Species ~ ., data = train_data, ntree = 500) # Make predictions on the testing set predictions <- predict(rf_model, test_data) # Calculate accuracy of the model accuracy <- sum(predictions == test_data$Species) / nrow(test_data) print(paste(\"Accuracy:\", accuracy)) ## [1] \"Accuracy: 0.977777777777778\" Python code: # Load the \"pandas\", \"numpy\", and \"sklearn\" libraries import pandas as pd import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split # Load the \"iris\" dataset iris = load_iris() # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=123) # Build a random forest model rf_model = RandomForestClassifier(n_estimators=500, random_state=123) rf_model.fit(X_train, y_train) # Make predictions on the testing set ## RandomForestClassifier(n_estimators=500, random_state=123) predictions = rf_model.predict(X_test) # Calculate accuracy of the model accuracy = sum(predictions == y_test) / len(y_test) print(\"Accuracy:\", accuracy) ## Accuracy: 0.9555555555555556 In both cases, we load the iris dataset and split it into training and testing sets. We then build a random forest model using the training data and evaluate its accuracy on the testing data. There are a few differences in the syntax and functionality between the two approaches: Library and package names: In R, we use the randomForest package to build random forest models, while in Python, we use the RandomForestClassifier class from the sklearn.ensemble module. We also use different libraries for loading and manipulating data (pandas and numpy in Python, and built-in datasets in R). Model parameters: The syntax for setting model parameters is slightly different in R and Python. For example, in R, we specify the number of trees using the ntree parameter, while in Python, we use the n_estimators parameter. Data format: In R, we use a data frame to store the input data, while in Python, we use numpy arrays.","title":"Random Forest"},{"location":"additional-resources/bilingualism_md/#basic-streetmap-from-open-street-map","text":"R Code: # Load the \"osmdata\" package for mapping library(osmdata) library(tmap) # Define the map location and zoom level bbox <- c(left = -0.16, bottom = 51.49, right = -0.13, top = 51.51) # Get the OpenStreetMap data osm_data <- opq(bbox) %>% add_osm_feature(key = \"highway\") %>% osmdata_sf() # Plot the map using tmap tm_shape(osm_data$osm_lines) + tm_lines() Python code: # Load the \"osmnx\" package for mapping import osmnx as ox # Define the map location and zoom level bbox = (51.49, -0.16, 51.51, -0.13) # Get the OpenStreetMap data osm_data = ox.graph_from_bbox(north=bbox[2], south=bbox[0], east=bbox[3], west=bbox[1], network_type='all') # Plot the map using osmnx ox.plot_graph(osm_data) ## (<Figure size 1600x1600 with 0 Axes>, <AxesSubplot:>) In both cases, we define the map location and zoom level, retrieve the OpenStreetMap data using the specified bounding box, and plot the map. The main differences between the two approaches are: Package names and syntax: In R, we use the osmdata package and its syntax to download and process the OpenStreetMap data, while in Python, we use the osmnx package and its syntax. Mapping libraries: In R, we use the tmap package to create a static map of the OpenStreetMap data, while in Python, we use the built-in ox.plot_graph function from the osmnx package to plot the map.","title":"Basic streetmap from Open Street Map"},{"location":"additional-resources/bilingualism_md/#cnn-on-raster-data","text":"R Code: # Load the \"keras\" package for building the CNN library(tensorflow) library(keras) # Load the \"raster\" package for working with raster data library(raster) # Load the \"magrittr\" package for pipe operator library(magrittr) # Load the data as a raster brick raster_data <- brick(\"raster_data.tif\") # Split the data into training and testing sets split_data <- sample(1:nlayers(raster_data), size = nlayers(raster_data)*0.8, replace = FALSE) train_data <- raster_data[[split_data]] test_data <- raster_data[[setdiff(1:nlayers(raster_data), split_data)]] # Define the CNN model model <- keras_model_sequential() %>% layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = \"relu\", input_shape = c(ncol(train_data), nrow(train_data), ncell(train_data))) %>% layer_max_pooling_2d(pool_size = c(2, 2)) %>% layer_dropout(rate = 0.25) %>% layer_flatten() %>% layer_dense(units = 128, activation = \"relu\") %>% layer_dropout(rate = 0.5) %>% layer_dense(units = nlayers(train_data), activation = \"softmax\") # Compile the model model %>% compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = \"accuracy\") # Train the model history <- model %>% fit(x = array(train_data), y = to_categorical(1:nlayers(train_data)), epochs = 10, validation_split = 0.2) # Evaluate the model model %>% evaluate(x = array(test_data), y = to_categorical(1:nlayers(test_data))) # Plot the model accuracy over time plot(history)","title":"CNN on Raster data"},{"location":"additional-resources/bilingualism_md/#piping","text":"Piping is a powerful feature in both R and Python that allows for a more streamlined and readable code. However, the syntax for piping is slightly different between the two languages. In R, piping is done using the %>% operator from the magrittr package, while in Python, it is done using the | operator from the pandas package. Let\u2019s compare and contrast piping in R and Python with some examples: Piping in R In R, we can use the %>% operator to pipe output from one function to another, which can make our code more readable and easier to follow. Here\u2019s an example: R code: library(dplyr) # create a data frame df <- data.frame(x = c(1,2,3), y = c(4,5,6)) # calculate the sum of column x and y df %>% mutate(z = x + y) %>% summarize(sum_z = sum(z)) ## sum_z ## 1 21 In this example, we first create a data frame df with two columns x and y. We then pipe the output of df to mutate, which adds a new column z to the data frame that is the sum of x and y. Finally, we pipe the output to summarize, which calculates the sum of z and returns the result. Piping in Python In Python, we can use the | operator to pipe output from one function to another. However, instead of piping output from one function to another, we pipe a DataFrame to a method of the DataFrame. Here\u2019s an example: Python code: import pandas as pd # create a DataFrame df = pd.DataFrame({'x': [1,2,3], 'y': [4,5,6]}) # calculate the sum of column x and y (df.assign(z = df['x'] + df['y']) .agg(sum_z = ('z', 'sum'))) ## z ## sum_z 21 In this example, we first create a DataFrame df with two columns x and y. We then use the assign() method to add a new column z to the DataFrame that is the sum of x and y. Finally, we use the agg() method to calculate the sum of z and return the result. As we can see, the syntax for piping is slightly different between R and Python, but the concept remains the same. Piping can make our code more readable and easier to follow, which is an important aspect of creating efficient and effective code. R code: library(dplyr) library(ggplot2) iris %>% filter(Species == \"setosa\") %>% group_by(Sepal.Width) %>% summarise(mean.Petal.Length = mean(Petal.Length)) %>% mutate(Sepal.Width = as.factor(Sepal.Width)) %>% ggplot(aes(x = Sepal.Width, y = mean.Petal.Length)) + geom_bar(stat = \"identity\", fill = \"dodgerblue\") + labs(title = \"Mean Petal Length of Setosa by Sepal Width\", x = \"Sepal Width\", y = \"Mean Petal Length\") In this example, we start with the iris dataset and filter it to only include rows where the Species column is \u201csetosa\u201d. We then group the remaining rows by the Sepal.Width column and calculate the mean Petal.Length for each group. Next, we convert Sepal.Width to a factor variable to ensure that it is treated as a categorical variable in the visualization. Finally, we create a bar plot using ggplot2, with Sepal.Width on the x-axis and mean.Petal.Length on the y-axis. The resulting plot shows the mean petal length of setosa flowers for each sepal width category. Python code: import pandas as pd # Load the iris dataset and pipe it into the next function ( pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", header=None, names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']) # Select columns and pivot the dataset .loc[:, ['sepal_length', 'sepal_width', 'petal_length']] .melt(var_name='variable', value_name='value') # Group by variable and calculate mean .groupby('variable', as_index=False) .mean() # Filter for mean greater than 3.5 and sort by descending mean .query('value > 3.5') .sort_values('value', ascending=False) ) ## variable value ## 1 sepal_length 5.843333 ## 0 petal_length 3.758667","title":"Piping"},{"location":"additional-resources/bilingualism_md/#for-loops","text":"Here is an example of a for loop in R: R code # Create a vector of numbers numbers <- c(1, 2, 3, 4, 5) # Use a for loop to print out each number in the vector for (i in numbers) { print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 In this example, the for loop iterates over each element in the numbers vector, assigning the current element to the variable i. The print(i) statement is then executed for each iteration, outputting the value of i. Here is the equivalent example in Python: Python code # Create a list of numbers numbers = [1, 2, 3, 4, 5] # Use a for loop to print out each number in the list for i in numbers: print(i) ## 1 ## 2 ## 3 ## 4 ## 5 In Python, the for loop iterates over each element in the numbers list, assigning the current element to the variable i. The print(i) statement is then executed for each iteration, outputting the value of i. Both languages also support nested for loops, which can be used to perform iterations over multiple dimensions, such as looping through a 2D array.","title":"for loops"},{"location":"additional-resources/bilingualism_md/#parallel","text":"Parallel computing is a technique used to execute multiple computational tasks simultaneously, which can significantly reduce the time required to complete a task. Both R and Python have built-in support for parallel computing, although the approaches are slightly different. In this answer, we will compare and contrast the parallel computing capabilities of R and Python, and provide working examples in code. Parallel computing in R In R, there are several packages that support parallel computing, such as parallel, foreach, and doParallel. The parallel package provides basic functionality for parallel computing, while foreach and doParallel provide higher-level abstractions that make it easier to write parallel code. Here is an example of using the foreach package to execute a loop in parallel: R code: library(foreach) library(doParallel) # Set up a parallel backend with 4 workers cl <- makeCluster(4) registerDoParallel(cl) # Define a function to apply in parallel myfunc <- function(x) { # some computation here return(x^2) } # Generate some data mydata <- 1:1000 # Apply the function to the data in parallel result <- foreach(i = mydata) %dopar% { myfunc(i) } # Stop the cluster stopCluster(cl) In this example, we use the makeCluster() function to set up a cluster with 4 workers, and the registerDoParallel() function to register the cluster as the parallel backend for foreach. We then define a function myfunc() that takes an input x and returns x^2. We generate some data mydata and use foreach to apply myfunc() to each element of mydata in parallel, using the %dopar% operator. R Tidyverse parallel In R Tidyverse, we can use the furrr package for parallel computing. Here\u2019s an example of using furrr to parallelize a map function: R Tidy code: library(tidyverse) library(furrr) # Generate a list of numbers numbers <- 1:10 # Use the future_map function from furrr to parallelize the map function plan(multisession) squares <- future_map(numbers, function(x) x^2) In this example, we first load the Tidyverse and furrr libraries. We then generate a list of numbers from 1 to 10. We then use the plan function to set the parallelization strategy to \u201cmultisession\u201d, which will use multiple CPU cores to execute the code. Finally, we use the future_map function from furrr to apply the function x^2 to each number in the list in parallel. Parallel computing in Python In Python, the standard library includes the multiprocessing module, which provides basic support for parallel computing. Additionally, there are several third-party packages that provide higher-level abstractions, such as joblib and dask. Here is an example of using the multiprocessing module to execute a loop in parallel: Python code: def square(x): return x**2 from multiprocessing import Pool # Generate a list of numbers numbers = list(range(1, 11)) # Use the map function and a pool of workers to parallelize the square function with Pool() as pool: squares = pool.map(square, numbers) print(squares) In this example, we define a function myfunc() that takes an input x and returns x^2. We generate some data mydata and use the Pool class from the multiprocessing module to set up a pool of 4 workers. We then use the map() method of the Pool class to apply myfunc() to each element of mydata in parallel. Comparison and contrast Both R and Python have built-in support for parallel computing, with similar basic functionality for creating and managing parallel processes. However, the higher-level abstractions differ between the two languages. In R, the foreach package provides a high-level interface that makes it easy to write parallel code, while in Python, the multiprocessing module provides a basic interface that can be extended using third-party packages like joblib and dask. Additionally, Python has better support for distributed computing using frameworks like Apache Spark, while R has better support for shared-memory parallelism using tools like data.table and ff.","title":"Parallel"},{"location":"additional-resources/bilingualism_md/#data-wrangling","text":"Data wrangling is an important part of any data analysis project, and both R and Python provide tools and libraries for performing this task. In this answer, we will compare and contrast data wrangling in R\u2019s tidyverse and Python\u2019s pandas library, with working examples in code. Data Wrangling in R Tidyverse The tidyverse is a collection of R packages designed for data science, and it includes several packages that are useful for data wrangling. One of the most popular packages is dplyr, which provides a grammar of data manipulation for data frames. Here is an example of using dplyr to filter, mutate, and summarize a data frame: R code library(dplyr) # Load data data(mtcars) # Filter for cars with more than 100 horsepower mtcars %>% filter(hp > 100) %>% # Add a new column with fuel efficiency in km per liter mutate(kmpl = 0.425 * mpg) %>% # Group by number of cylinders and summarize group_by(cyl) %>% summarize(mean_hp = mean(hp), mean_kmpl = mean(kmpl)) ## # A tibble: 3 \u00d7 3 ## cyl mean_hp mean_kmpl ## <dbl> <dbl> <dbl> ## 1 4 111 11.0 ## 2 6 122. 8.39 ## 3 8 209. 6.42 In this example, we first filter the mtcars data frame to only include cars with more than 100 horsepower. We then use mutate to create a new column with fuel efficiency in kilometers per liter. Finally, we group the data by the number of cylinders and calculate the mean horsepower and fuel efficiency. Data Wrangling in Python Pandas Pandas is a popular library for data manipulation in Python. It provides a data frame object similar to R\u2019s data frames, along with a wide range of functions for data wrangling. Here is an example of using pandas to filter, transform, and group a data frame: Python code: import pandas as pd # Load data mtcars = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/mtcars.csv') # Filter for cars with more than 100 horsepower filtered_mtcars = mtcars[mtcars['hp'] > 100] # Add a new column with fuel efficiency in km per liter filtered_mtcars['kmpl'] = 0.425 * filtered_mtcars['mpg'] # Group by number of cylinders and calculate mean horsepower and fuel efficiency grouped_mtcars = filtered_mtcars.groupby('cyl').agg({'hp': 'mean', 'kmpl': 'mean'}) In this example, we first load the mtcars data from a CSV file. We then filter the data to only include cars with more than 100 horsepower, using boolean indexing. We use the assign function to create a new column with fuel efficiency in kilometers per liter. Finally, we group the data by the number of cylinders and calculate the mean horsepower and fuel efficiency. Comparison Overall, both R\u2019s tidyverse and Python\u2019s pandas provide similar functionality for data wrangling. Both allow for filtering, transforming, and aggregating data frames. The syntax for performing these operations is slightly different between the two languages, with R using the %>% operator for chaining operations and Python using method chaining or the apply family of functions. One key difference between the two languages is that R\u2019s tidyverse provides a consistent grammar for data manipulation across its various packages, making it easier to learn and use. However, Python\u2019s pandas library has a larger developer community and is more versatile for use in other applications, such as web development or machine learning. In conclusion, both R and Python provide powerful tools for data wrangling, and the choice between the two ultimately depends on the specific needs of the user and their familiarity","title":"Data wrangling"},{"location":"additional-resources/bilingualism_md/#data-from-api","text":"Retrieving data from an API is a common task in both R and Python. Here are examples of how to retrieve data from an API in both languages: Python To retrieve data from an API in Python, we can use the requests library. Here\u2019s an example of how to retrieve weather data from the OpenWeatherMap API: Python code: import requests url = 'https://api.openweathermap.org/data/2.5/weather?q=London,uk&appid=API_KEY' response = requests.get(url) data = response.json() print(data) This code retrieves the current weather data for London from the OpenWeatherMap API. We first construct the API URL with the location and API key, then use the requests.get() function to make a request to the API. We then extract the JSON data from the response using the .json() method and print the resulting data. R In R, we can use the httr package to retrieve data from an API. Here\u2019s an example of how to retrieve weather data from the OpenWeatherMap API in R: R code: library(httr) url <- 'https://api.openweathermap.org/data/2.5/weather?q=London,uk&appid=API_KEY' response <- GET(url) data <- content(response, 'text') print(data) This code is similar to the Python code above. We first load the httr library, then construct the API URL and use the GET() function to make a request to the API. We then extract the data from the response using the content() function and print the resulting data. Retrieving Data from an API in R Tidyverse In R Tidyverse, we can use the httr and jsonlite packages to retrieve and process data from an API. R code: # Load required packages library(httr) library(jsonlite) # Define API endpoint endpoint <- \"https://jsonplaceholder.typicode.com/posts\" # Retrieve data from API response <- GET(endpoint) # Extract content from response content <- content(response, \"text\") # Convert content to JSON json <- fromJSON(content) # Convert JSON to a data frame df <- as.data.frame(json) In the above example, we use the GET() function from the httr package to retrieve data from an API endpoint, and the content() function to extract the content of the response. We then use the fromJSON() function from the jsonlite package to convert the JSON content to a list, and the as.data.frame() function to convert the list to a data frame. Retrieving Data from an API in Python In Python, we can use the requests library to retrieve data from an API, and the json library to process the JSON data. Python code: # Load required libraries import requests import json # Define API endpoint endpoint = \"https://jsonplaceholder.typicode.com/posts\" # Retrieve data from API response = requests.get(endpoint) # Extract content from response content = response.content # Convert content to JSON json_data = json.loads(content) # Convert JSON to a list of dictionaries data = [dict(row) for row in json_data] In the above example, we use the get() function from the requests library to retrieve data from an API endpoint, and the content attribute to extract the content of the response. We then use the loads() function from the json library to convert the JSON content to a list of dictionaries. Comparison Both R Tidyverse and Python provide powerful tools for retrieving and processing data from an API. In terms of syntax, the two languages are somewhat similar. In both cases, we use a library to retrieve data from the API, extract the content of the response, and then process the JSON data. However, there are some differences in the specific functions and methods used. For example, in R Tidyverse, we use the content() function to extract the content of the response, whereas in Python, we use the content attribute. Additionally, in R Tidyverse, we use the fromJSON() function to convert the JSON data to a list, whereas in Python, we use the loads() function.","title":"Data from API"},{"location":"additional-resources/bilingualism_md/#census-data","text":"Retrieving USA census data in R, R Tidy, and Python can be done using different packages and libraries. Here are some working examples in code for each language: R: To retrieve census data in R, we can use the tidycensus package. Here\u2019s an example of how to retrieve the total population for the state of California: R code: library(tidycensus) library(tidyverse) # Set your Census API key census_api_key(\"your_api_key\") # Get the total population for the state of California ca_pop <- get_acs( geography = \"state\", variables = \"B01003_001\", state = \"CA\" ) %>% rename(total_population = estimate) %>% select(total_population) # View the result ca_pop R Tidy: To retrieve census data in R Tidy, we can also use the tidycensus package. Here\u2019s an example of how to retrieve the total population for the state of California using pipes and dplyr functions: R tidy code: library(tidycensus) library(tidyverse) # Set your Census API key census_api_key(\"your_api_key\") # Get the total population for the state of California ca_pop <- get_acs( geography = \"state\", variables = \"B01003_001\", state = \"CA\" ) %>% rename(total_population = estimate) %>% select(total_population) # View the result ca_pop Python: To retrieve census data in Python, we can use the census library. Here\u2019s an example of how to retrieve the total population for the state of California: Python code: from census import Census from us import states import pandas as pd # Set your Census API key c = Census(\"your_api_key\") # Get the total population for the state of California ca_pop = c.acs5.state((\"B01003_001\"), states.CA.fips, year=2019) # Convert the result to a Pandas DataFrame ca_pop_df = pd.DataFrame(ca_pop) # Rename the column ca_pop_df = ca_pop_df.rename(columns={\"B01003_001E\": \"total_population\"}) # Select only the total population column ca_pop_df = ca_pop_df[[\"total_population\"]] # View the result ca_pop_df","title":"Census data"},{"location":"additional-resources/bilingualism_md/#lidar-data","text":"To find Lidar data in R and Python, you typically need to start by identifying sources of Lidar data and then accessing them using appropriate packages and functions. Here are some examples of how to find Lidar data in R and Python: R: Identify sources of Lidar data: The USGS National Map Viewer provides access to Lidar data for the United States. You can also find Lidar data on state and local government websites, as well as on commercial data providers\u2019 websites. Access the data: You can use the lidR package in R to download and read Lidar data in the LAS format. For example, the following code downloads and reads Lidar data for a specific area: R code: library(lidR) # Download Lidar data LASfile <- system.file(\"extdata\", \"Megaplot.laz\", package=\"lidR\") lidar <- readLAS(LASfile) # Visualize the data plot(lidar) Python: Identify sources of Lidar data: The USGS 3DEP program provides access to Lidar data for the United States. You can also find Lidar data on state and local government websites, as well as on commercial data providers\u2019 websites. Access the data: You can use the pylastools package in Python to download and read Lidar data in the LAS format. For example, the following code downloads and reads Lidar data for a specific area: Python code: py_install(\"requests\") py_install(\"pylas\") py_install(\"laspy\") import requests from pylas import read import laspy import numpy as np # Download Lidar data url = \"https://s3-us-west-2.amazonaws.com/usgs-lidar-public/USGS_LPC_CA_SanFrancisco_2016_LAS_2018.zip\" lasfile = \"USGS_LPC_CA_SanFrancisco_2016_LAS_2018.las\" r = requests.get(url, allow_redirects=True) open(lasfile, 'wb').write(r.content) # Read the data lidar = read(lasfile) # Visualize the data laspy.plot.plot(lidar)","title":"Lidar data"},{"location":"additional-resources/bilingualism_md/#data-for-black-lives","text":"Data for Black Lives ( https://d4bl.org/ ) is a movement that uses data science to create measurable change in the lives of Black people. While the Data for Black Lives website provides resources, reports, articles, and datasets related to racial equity, it doesn\u2019t provide a direct API for downloading data. Instead, you can access the Data for Black Lives GitHub repository ( https://github.com/Data4BlackLives ) to find datasets and resources to work with. In this example, we\u2019ll use a sample dataset available at https://github.com/Data4BlackLives/covid-19/tree/master/data . The dataset \u201cCOVID19_race_data.csv\u201d contains COVID-19 race-related data. R: In R, we\u2019ll use the \u2018readr\u2019 and \u2018dplyr\u2019 packages to read, process, and analyze the dataset. R code: # Install and load necessary libraries library(readr) library(dplyr) # Read the CSV file url <- \"https://raw.githubusercontent.com/Data4BlackLives/covid-19/master/data/COVID19_race_data.csv\" data <- read_csv(url) # Basic information about the dataset print(dim(data)) print(head(data)) # Example analysis: calculate the mean of 'cases_total' by 'state' data %>% group_by(state) %>% summarize(mean_cases_total = mean(cases_total, na.rm = TRUE)) %>% arrange(desc(mean_cases_total)) Python: In Python, we\u2019ll use the \u2018pandas\u2019 library to read, process, and analyze the dataset. Python code: import pandas as pd # Read the CSV file url = \"https://raw.githubusercontent.com/Data4BlackLives/covid-19/master/data/COVID19_race_data.csv\" data = pd.read_csv(url) # Basic information about the dataset print(data.shape) print(data.head()) # Example analysis: calculate the mean of 'cases_total' by 'state' mean_cases_total = data.groupby(\"state\")[\"cases_total\"].mean().sort_values(ascending=False) print(mean_cases_total) In conclusion, both R and Python provide powerful libraries and tools for downloading, processing, and analyzing datasets, such as those found in the Data for Black Lives repository. The \u2018readr\u2019 and \u2018dplyr\u2019 libraries in R offer a simple and intuitive way to read and manipulate data, while the \u2018pandas\u2019 library in Python offers similar functionality with a different syntax. Depending on your preferred programming language and environment, both options can be effective in working with social justice datasets.","title":"Data for black lives"},{"location":"additional-resources/bilingualism_md/#propublica-congress-api","text":"The ProPublica Congress API provides information about the U.S. Congress members and their voting records. In this example, we\u2019ll fetch data about the current Senate members and calculate the number of members in each party. R: In R, we\u2019ll use the \u2018httr\u2019 and \u2018jsonlite\u2019 packages to fetch and process data from the ProPublica Congress API. R code: # load necessary libraries library(httr) library(jsonlite) # Replace 'your_api_key' with your ProPublica API key # # Fetch data about the current Senate members url <- \"https://api.propublica.org/congress/v1/117/senate/members.json\" response <- GET(url, add_headers(`X-API-Key` = api_key)) # Check if the request was successful if (http_status(response)$category == \"Success\") { data <- content(response, \"parsed\") members <- data$results[[1]]$members # Calculate the number of members in each party party_counts <- table(sapply(members, function(x) x$party)) print(party_counts) } else { print(http_status(response)$message) } ## ## D I ID R ## 49 1 2 51 Python: In Python, we\u2019ll use the \u2018requests\u2019 library to fetch data from the ProPublica Congress API and \u2018pandas\u2019 library to process the data. python code: # Install necessary libraries import requests import pandas as pd # Replace 'your_api_key' with your ProPublica API key api_key = \"your_api_key\" headers = {\"X-API-Key\": api_key} # Fetch data about the current Senate members url = \"https://api.propublica.org/congress/v1/117/senate/members.json\" response = requests.get(url, headers=headers) # Check if the request was successful if response.status_code == 200: data = response.json() members = data[\"results\"][0][\"members\"] # Calculate the number of members in each party party_counts = pd.DataFrame(members)[\"party\"].value_counts() print(party_counts) else: print(f\"Error: {response.status_code}\") In conclusion, both R and Python offer efficient ways to fetch and process data from APIs like the ProPublica Congress API. The \u2018httr\u2019 and \u2018jsonlite\u2019 libraries in R provide a straightforward way to make HTTP requests and parse JSON data, while the \u2018requests\u2019 library in Python offers similar functionality. The \u2018pandas\u2019 library in Python can be used for data manipulation and analysis, and R provides built-in functions like table() for aggregating data. Depending on your preferred programming language and environment, both options can be effective for working with the ProPublica Congress API.","title":"Propublica Congress API"},{"location":"additional-resources/bilingualism_md/#nonprofit-explorer-api-by-propublica","text":"The Nonprofit Explorer API by ProPublica provides data on tax-exempt organizations in the United States. In this example, we\u2019ll search for organizations with the keyword \u201ceducation\u201d and analyze the results. R: In R, we\u2019ll use the \u2018httr\u2019 and \u2018jsonlite\u2019 packages to fetch and process data from the Nonprofit Explorer API. R code: # Install and load necessary libraries library(httr) library(jsonlite) # Fetch data for organizations with the keyword \"education\" url <- \"https://projects.propublica.org/nonprofits/api/v2/search.json?q=education\" response <- GET(url) # Check if the request was successful if (http_status(response)$category == \"Success\") { data <- content(response, \"parsed\") organizations <- data$organizations # Count the number of organizations per state state_counts <- table(sapply(organizations, function(x) x$state)) print(state_counts) } else { print(http_status(response)$message) } ## ## AZ CA CO DC FL GA HI IL Indiana LA ## 3 22 6 5 3 2 1 2 1 1 ## MD MI MN MO MP MS NC NE NJ NM ## 1 2 5 3 1 1 2 2 2 1 ## NY OH OK Oregon PA TX UT VA WA WV ## 1 5 1 2 2 12 1 4 3 1 ## ZZ ## 2 Python: In Python, we\u2019ll use the \u2018requests\u2019 library to fetch data from the Nonprofit Explorer API and \u2018pandas\u2019 library to process the data. Python code: # Install necessary libraries import requests import pandas as pd # Fetch data for organizations with the keyword \"education\" url = \"https://projects.propublica.org/nonprofits/api/v2/search.json?q=education\" response = requests.get(url) # Check if the request was successful if response.status_code == 200: data = response.json() organizations = data[\"organizations\"] # Count the number of organizations per state state_counts = pd.DataFrame(organizations)[\"state\"].value_counts() print(state_counts) else: print(f\"Error: {response.status_code}\") ## CA 22 ## TX 12 ## CO 6 ## MN 5 ## OH 5 ## DC 5 ## VA 4 ## AZ 3 ## WA 3 ## MO 3 ## FL 3 ## IL 2 ## GA 2 ## NC 2 ## MI 2 ## Oregon 2 ## NE 2 ## ZZ 2 ## PA 2 ## NJ 2 ## HI 1 ## MS 1 ## NY 1 ## Indiana 1 ## NM 1 ## LA 1 ## UT 1 ## MD 1 ## MP 1 ## WV 1 ## OK 1 ## Name: state, dtype: int64 In conclusion, both R and Python offer efficient ways to fetch and process data from APIs like the Nonprofit Explorer API. The \u2018httr\u2019 and \u2018jsonlite\u2019 libraries in R provide a straightforward way to make HTTP requests and parse JSON data, while the \u2018requests\u2019 library in Python offers similar functionality. The \u2018pandas\u2019 library in Python can be used for data manipulation and analysis, and R provides built-in functions like table() for aggregating data. Depending on your preferred programming language and environment, both options can be effective for working with the Nonprofit Explorer API.","title":"Nonprofit Explorer API by ProPublica"},{"location":"additional-resources/bilingualism_md/#campaign-finance-api-by-propublica","text":"The Campaign Finance API by the Federal Election Commission (FEC) provides data on campaign finance in U.S. federal elections. In this example, we\u2019ll fetch data about individual contributions for the 2020 election cycle and analyze the results. R: In R, we\u2019ll use the \u2018httr\u2019 and \u2018jsonlite\u2019 packages to fetch and process data from the Campaign Finance API. R code: # Install and load necessary libraries library(httr) library(jsonlite) # Fetch data about individual contributions for the 2020 election cycle url <- \"https://api.open.fec.gov/v1/schedules/schedule_a/?api_key='OGwpkX7tH5Jihs1qQcisKfVAMddJzmzouWKtKoby'&two_year_transaction_period=2020&sort_hide_null=false&sort_null_only=false&per_page=20&page=1\" response <- GET(url) # Check if the request was successful if (http_status(response)$category == \"Success\") { data <- content(response, \"parsed\") contributions <- data$results # Calculate the total contributions per state state_totals <- aggregate(contributions$contributor_state, by = list(contributions$contributor_state), FUN = sum) colnames(state_totals) <- c(\"State\", \"Total_Contributions\") print(state_totals) } else { print(http_status(response)$message) } ## [1] \"Client error: (403) Forbidden\" Python: In Python, we\u2019ll use the \u2018requests\u2019 library to fetch data from the Campaign Finance API and \u2018pandas\u2019 library to process the data. Python code: # Install necessary libraries import requests import pandas as pd # Fetch data about individual contributions for the 2020 election cycle url = \"https://api.open.fec.gov/v1/schedules/schedule_a/?api_key=your_api_key&two_year_transaction_period=2020&sort_hide_null=false&sort_null_only=false&per_page=20&page=1\" response = requests.get(url) # Check if the request was successful if response.status_code == 200: data = response.json() contributions = data[\"results\"] # Calculate the total contributions per state df = pd.DataFrame(contributions) state_totals = df.groupby(\"contributor_state\")[\"contribution_receipt_amount\"].sum() print(state_totals) else: print(f\"Error: {response.status_code}\") ## Error: 403 In conclusion, both R and Python offer efficient ways to fetch and process data from APIs like the Campaign Finance API. The \u2018httr\u2019 and \u2018jsonlite\u2019 libraries in R provide a straightforward way to make HTTP requests and parse JSON data, while the \u2018requests\u2019 library in Python offers similar functionality. The \u2018pandas\u2019 library in Python can be used for data manipulation and analysis, and R provides built-in functions like aggregate() for aggregating data. Depending on your preferred programming language and environment, both options can be effective for working with the Campaign Finance API. Note: Remember to replace your_api_key with your actual FEC API key in the code examples above.","title":"Campaign Finance API by ProPublica"},{"location":"additional-resources/bilingualism_md/#historic-redlining","text":"Historic redlining data refers to data from the Home Owners\u2019 Loan Corporation (HOLC) that created residential security maps in the 1930s, which contributed to racial segregation and disinvestment in minority neighborhoods. One popular source for this data is the Mapping Inequality project ( https://dsl.richmond.edu/panorama/redlining/ ). In this example, we\u2019ll download historic redlining data for Philadelphia in the form of a GeoJSON file and analyze the data in R and Python. R: In R, we\u2019ll use the \u2018sf\u2019 and \u2018dplyr\u2019 packages to read and process the GeoJSON data. R code: # Install and load necessary libraries library(sf) library(dplyr) # Download historic redlining data for Philadelphia url <- \"https://dsl.richmond.edu/panorama/redlining/static/downloads/geojson/PAPhiladelphia1937.geojson\" philly_geojson <- read_sf(url) # Count the number of areas per HOLC grade grade_counts <- philly_geojson %>% group_by(holc_grade) %>% summarize(count = n()) plot(grade_counts) Python: In Python, we\u2019ll use the \u2018geopandas\u2019 library to read and process the GeoJSON data. Python code: # Install necessary libraries import geopandas as gpd # Download historic redlining data for Philadelphia url = \"https://dsl.richmond.edu/panorama/redlining/static/downloads/geojson/PAPhiladelphia1937.geojson\" philly_geojson = gpd.read_file(url) # Count the number of areas per HOLC grade grade_counts = philly_geojson[\"holc_grade\"].value_counts() print(grade_counts) ## B 28 ## D 26 ## C 18 ## A 10 ## Name: holc_grade, dtype: int64 In conclusion, both R and Python offer efficient ways to download and process historic redlining data in the form of GeoJSON files. The \u2018sf\u2019 package in R provides a simple way to read and manipulate spatial data, while the \u2018geopandas\u2019 library in Python offers similar functionality. The \u2018dplyr\u2019 package in R can be used for data manipulation and analysis, and Python\u2019s built-in functions like value_counts() can be used for aggregating data. Depending on your preferred programming language and environment, both options can be effective for working with historic redlining data.","title":"Historic Redlining"},{"location":"additional-resources/bilingualism_md/#american-indian-and-alaska-native-areas-aiannh","text":"In this example, we\u2019ll download and analyze the American Indian and Alaska Native Areas (AIANNH) TIGER/Line Shapefile from the U.S. Census Bureau. We\u2019ll download the data for the year 2020, and analyze the number of AIANNH per congressional district R: In R, we\u2019ll use the \u2018sf\u2019 and \u2018dplyr\u2019 packages to read and process the Shapefile data. R code: # Install and load necessary libraries library(sf) library(dplyr) # Download historic redlining data for Philadelphia url <- \"https://www2.census.gov/geo/tiger/TIGER2020/AIANNH/tl_2020_us_aiannh.zip\" temp_file <- tempfile(fileext = \".zip\") download.file(url, temp_file, mode = \"wb\") unzip(temp_file, exdir = tempdir()) # Read the Shapefile shapefile_path <- file.path(tempdir(), \"tl_2020_us_aiannh.shp\") aiannh <- read_sf(shapefile_path) # Count the number of AIANNH per congressional district state_counts <- aiannh %>% group_by(LSAD) %>% summarize(count = n()) print(state_counts[order(-state_counts$count),]) ## Simple feature collection with 26 features and 2 fields ## Geometry type: GEOMETRY ## Dimension: XY ## Bounding box: xmin: -174.236 ymin: 18.91069 xmax: -67.03552 ymax: 71.34019 ## Geodetic CRS: NAD83 ## # A tibble: 26 \u00d7 3 ## LSAD count geometry ## <chr> <int> <MULTIPOLYGON [\u00b0]> ## 1 79 221 (((-166.5331 65.33918, -166.5331 65.33906, -166.533 65.33699, -1\u2026 ## 2 86 206 (((-83.38811 35.46645, -83.38342 35.46596, -83.38316 35.46593, -\u2026 ## 3 OT 155 (((-92.32972 47.81374, -92.3297 47.81305, -92.32967 47.81196, -9\u2026 ## 4 78 75 (((-155.729 20.02457, -155.7288 20.02428, -155.7288 20.02427, -1\u2026 ## 5 85 46 (((-122.3355 37.95215, -122.3354 37.95206, -122.3352 37.95199, -\u2026 ## 6 92 35 (((-93.01356 31.56287, -93.01354 31.56251, -93.01316 31.56019, -\u2026 ## 7 88 25 (((-97.35299 36.908, -97.35291 36.90801, -97.35287 36.908, -97.3\u2026 ## 8 96 19 (((-116.48 32.63814, -116.48 32.63718, -116.4794 32.63716, -116.\u2026 ## 9 84 16 (((-105.5937 36.40379, -105.5937 36.40324, -105.5937 36.40251, -\u2026 ## 10 89 11 (((-95.91705 41.28037, -95.91653 41.28036, -95.91653 41.28125, -\u2026 ## # \u2139 16 more rows Python: In Python, we\u2019ll use the \u2018geopandas\u2019 library to read and process the Shapefile data. Python code: import geopandas as gpd import pandas as pd import requests import zipfile import os from io import BytesIO # Download historic redlining data for Philadelphia url = \"https://www2.census.gov/geo/tiger/TIGER2020/AIANNH/tl_2020_us_aiannh.zip\" response = requests.get(url) zip_file = zipfile.ZipFile(BytesIO(response.content)) # Extract Shapefile temp_dir = \"temp\" if not os.path.exists(temp_dir): os.makedirs(temp_dir) zip_file.extractall(path=temp_dir) shapefile_path = os.path.join(temp_dir, \"tl_2020_us_aiannh.shp\") # Read the Shapefile aiannh = gpd.read_file(shapefile_path) # Count the number of AIANNH per congressional district state_counts = aiannh.groupby(\"LSAD\").size().reset_index(name=\"count\") # Sort by descending count state_counts_sorted = state_counts.sort_values(by=\"count\", ascending=False) print(state_counts_sorted) ## LSAD count ## 2 79 221 ## 9 86 206 ## 25 OT 155 ## 1 78 75 ## 8 85 46 ## 15 92 35 ## 11 88 25 ## 19 96 19 ## 7 84 16 ## 12 89 11 ## 5 82 8 ## 3 80 7 ## 4 81 6 ## 21 98 5 ## 20 97 5 ## 13 90 4 ## 18 95 3 ## 6 83 3 ## 17 94 2 ## 16 93 1 ## 14 91 1 ## 10 87 1 ## 22 99 1 ## 23 9C 1 ## 24 9D 1 ## 0 00 1 In conclusion, both R and Python offer efficient ways to download and process AIANNH TIGER/Line Shapefile data from the U.S. Census Bureau. The \u2018sf\u2019 package in R provides a simple way to read and manipulate spatial data, while the \u2018geopandas\u2019 library in Python offers similar functionality. The \u2018dplyr\u2019 package in R can be used for data manipulation and analysis, and Python\u2019s built-in functions like value_counts() can be used for aggregating data. Depending on your preferred programming language and environment, both options can be effective for working with AIANNH data.","title":"American Indian and Alaska Native Areas (AIANNH)"},{"location":"additional-resources/bilingualism_md/#indian-entities-recognized-and-eligible-to-receive-services-by-bia","text":"The Bureau of Indian Affairs (BIA) provides a PDF document containing a list of Indian Entities Recognized and Eligible To Receive Services. To analyze the data, we\u2019ll first need to extract the information from the PDF. In this example, we\u2019ll extract the names of the recognized tribes and count the number of tribes per state. R: In R, we\u2019ll use the \u2018pdftools\u2019 package to extract text from the PDF and the \u2018stringr\u2019 package to process the text data. R code: # Install and load necessary libraries library(pdftools) library(stringr) library(dplyr) # Download the BIA PDF url <- \"https://www.govinfo.gov/content/pkg/FR-2022-01-28/pdf/2022-01789.pdf\" temp_file <- tempfile(fileext = \".pdf\") download.file(url, temp_file, mode = \"wb\") # Extract text from the PDF pdf_text <- pdf_text(temp_file) tribe_text <- pdf_text[4:length(pdf_text)] # Define helper functions tribe_state_extractor <- function(text_line) { regex_pattern <- \"(.*),\\\\s+([A-Z]{2})$\" tribe_state <- str_match(text_line, regex_pattern) return(tribe_state) } is_valid_tribe_line <- function(text_line) { regex_pattern <- \"^\\\\d+\\\\s+\" return(!is.na(str_match(text_line, regex_pattern))) } # Process text data to extract tribes and states tribe_states <- sapply(tribe_text, tribe_state_extractor) valid_lines <- sapply(tribe_text, is_valid_tribe_line) tribe_states <- tribe_states[valid_lines, 2:3] # Count the number of tribes per state tribe_data <- as.data.frame(tribe_states) colnames(tribe_data) <- c(\"Tribe\", \"State\") state_counts <- tribe_data %>% group_by(State) %>% summarise(Count = n()) print(state_counts) ## # A tibble: 0 \u00d7 2 ## # \u2139 2 variables: State <chr>, Count <int> Python: In Python, we\u2019ll use the \u2018PyPDF2\u2019 library to extract text from the PDF and the \u2018re\u2019 module to process the text data. Python code: # Install necessary libraries import requests import PyPDF2 import io import re from collections import Counter # Download the BIA PDF url = \"https://www.bia.gov/sites/bia.gov/files/assets/public/raca/online-tribal-leaders-directory/tribal_leaders_2021-12-27.pdf\" response = requests.get(url) # Extract text from the PDF pdf_reader = PyPDF2.PdfFileReader(io.BytesIO(response.content)) tribe_text = [pdf_reader.getPage(i).extractText() for i in range(3, pdf_reader.numPages)] # Process text data to extract tribes and states tribes = [re.findall(r'^\\d+\\s+(.+),\\s+([A-Z]{2})', line) for text in tribe_text for line in text.split('\\n') if line] tribe_states = [state for tribe, state in tribes] # Count the number of tribes per state state_counts = Counter(tribe_states) print(state_counts) In conclusion, both R and Python offer efficient ways to download and process the list of Indian Entities Recognized and Eligible To Receive Services from the BIA. The \u2018pdftools\u2019 package in R provides a simple way to extract text from PDF files, while the \u2018PyPDF2\u2019 library in Python offers similar functionality. The \u2018stringr\u2019 package in R and the \u2018re\u2019 module in Python can be used to process and analyze text data. Depending on your preferred programming language and environment, both options can be effective for working with BIA data.","title":"Indian Entities Recognized and Eligible To Receive Services by BIA"},{"location":"additional-resources/bilingualism_md/#national-atlas-indian-lands-of-the-united-states-dataset","text":"In this example, we will download and analyze the National Atlas - Indian Lands of the United States dataset in both R and Python. We will read the dataset and count the number of Indian lands per state. R: In R, we\u2019ll use the \u2018sf\u2019 package to read the Shapefile and the \u2018dplyr\u2019 package to process the data. R code: # Install and load necessary libraries library(sf) library(dplyr) # Download the Indian Lands dataset url <- \"https://prd-tnm.s3.amazonaws.com/StagedProducts/Small-scale/data/Boundaries/indlanp010g.shp_nt00968.tar.gz\" temp_file <- tempfile(fileext = \".tar.gz\") download.file(url, temp_file, mode = \"wb\") untar(temp_file, exdir = tempdir()) # Read the Shapefile shapefile_path <- file.path(tempdir(), \"indlanp010g.shp\") indian_lands <- read_sf(shapefile_path) # Count the number of Indian lands per state # state_counts <- indian_lands %>% # group_by(STATE) %>% # summarize(count = n()) plot(indian_lands) ## Warning: plotting the first 9 out of 23 attributes; use max.plot = 23 to plot ## all Python: In Python, we\u2019ll use the \u2018geopandas\u2019 and \u2018pandas\u2019 libraries to read the Shapefile and process the data. Python code: import geopandas as gpd import pandas as pd import requests import tarfile import os from io import BytesIO # Download the Indian Lands dataset url = \"https://prd-tnm.s3.amazonaws.com/StagedProducts/Small-scale/data/Boundaries/indlanp010g.shp_nt00966.tar.gz\" response = requests.get(url) tar_file = tarfile.open(fileobj=BytesIO(response.content), mode='r:gz') # Extract Shapefile temp_dir = \"temp\" if not os.path.exists(temp_dir): os.makedirs(temp_dir) tar_file.extractall(path=temp_dir) shapefile_path = os.path.join(temp_dir, \"indlanp010g.shp\") # Read the Shapefile indian_lands = gpd.read_file(shapefile_path) # Count the number of Indian lands per state state_counts = indian_lands.groupby(\"STATE\").size().reset_index(name=\"count\") print(state_counts) Both R and Python codes download the dataset and read the Shapefile using the respective packages. They then group the data by the \u2018STATE\u2019 attribute and calculate the count of Indian lands per state.","title":"National Atlas - Indian Lands of the United States dataset"},{"location":"additional-resources/code-of-conduct/","text":"Code of Conduct and Respectful Inclusive Collaboration Guidelines CIRES Earth Lab and Environmental Data Science Innovation & Inclusion Lab (ESIIL) are committed to building, maintaining, and fostering an inclusive, kind, collaborative, and diverse transdisciplinary environmental data science community, whose members feel welcome, supported, and safe to contribute ideas and knowledge. The Forest Carbon Codefest will follow all aspects of the ESIIL Code of Conduct (below). All community members are responsible for creating this culture, embodying our values, welcoming diverse perspectives and ways of knowing, creating safe inclusive spaces, and conducting ethical science as guided by FAIR (Findable, Accessible, Interoperable, Reusable) and CARE (Collective Benefit, Authority to Control, Responsibility, and Ethics) principles for scientific and Indigenous data management, governance, and stewardship. Our values ESIIL\u2019s vision is grounded in the conviction that innovation and breakthroughs in environmental data science will be precipitated by a diverse, collaborative, curious, and inclusive research community empowered by open data and infrastructure, cross-sector and community partnerships, team science, and engaged learning. As such, our core values center people through inclusion, kindness, respect, collaboration, and genuine relationships. They also center innovation, driven by collaborative, cross-sector science and synthesis, open, accessible data and tools, and fun, diverse teams. Finally, they center learning, propelled by curiosity and accessible, inclusive training, and education opportunities. When and how to use these guidelines These guidelines outline behavior expectations for ESIIL community members. Your participation in the ESIIL network is contingent upon following these guidelines in all ESIIL activities, including, but not limited to, participating in meetings, webinars, hackathons, working groups, hosted or funded by ESIIL, as well as email lists and online forums such as GutHub, Slack, and Twitter. These guidelines have been adapted from those of the International Arctic Research Policy Committee, the Geological Society of America, the American Geophysical Union, the University Corporation for Atmospheric Research, The Carpentries, and others. We encourage other organizations to adapt these guidelines for use in their own meetings. Note: Working groups and hackathon/codefest teams are encouraged to discuss these guidelines and what they mean to them, and will have the opportunity to add to them to specifically support and empower their team. Collaborative and behavior commitments complement data use, management, authorship, and access plans that commit to CARE and FAIR principles. Behavior Agreements ESIIL community members are expected to act professionally and respectfully in all activities, such that each person, regardless of gender, gender identity or expression, sexual orientation, disability, physical appearance, age, body size, race, religion, national origin, ethnicity, level of experience, language fluency, political affiliation, veteran status, pregnancy, country of origin, and any other characteristic protected under state or federal law, feels safe and welcome in our activities and community. We gain strength from diversity and actively seek participation from those who enhance it. In order to garner the benefits of a diverse community and to reach the full potential of our mission and charge, ESIIL participants must be allowed to develop a sense of belonging and trust within a respectful, inclusive, and collaborative culture. Guiding behaviors that contribute to this culture include, but are not limited to: Showing Respect Listen carefully \u2013 we each bring our own styles of communication, language, and ideas, and we must do our best to accept and accommodate differences. Do not interrupt when someone is speaking and maintain an open mind when others have different ideas than yours. Be present \u2013 when engaging with others, give them your full attention. If you need to respond to outside needs, please step away from the group quietly. Be kind \u2013 offer positive, supportive comments and constructive feedback. Critique ideas, not people. Harassment, discrimination, bullying, aggression, including offensive comments, jokes, and imagery, are unacceptable, regardless of intent, and will not be tolerated. Be punctual - adhere to the schedule provided by the organizers and avoid disruptive behavior during presentations, trainings, or working sessions. Respect privacy - be mindful of the confidentiality of others. Always obtain explicit consent before recording, sharing, or using someone else\u2019s personal information, photos, or recordings. Practice good digital etiquette (netiquette) when communicating online, whether in emails, messages, or social media - think before posting online and consider the potential impact on others. Do not share or distribute content generated by or involving others without their explicit consent. Being Inclusive Create space for everyone to participate \u2013 be thoughtful about who is at the table; openly address accessibility needs, and provide multiple ways to contribute. Be welcoming \u2013 ESIIL participants come from a wide range of skill levels and career stages, backgrounds, and cultures. Demonstrate that you value these different perspectives and identities through your words and actions, including through correct use of names, titles, and pronouns. Be self-aware \u2013 recognize that positionality, identity, unconscious biases, and upbringing can all affect how words and behaviors are perceived. Ensure that your words and behavior make others feel welcome. Commit to ongoing learning \u2013 the move toward inclusive, equitable, and just environmental data science is a collective journey. Continue to learn about and apply practices of inclusion, anti-racism, bystander intervention, and cultural sensitivity. None of us is perfect; all of us will, from time to time, fail to live up to our own high standards. Being perfect is not what matters; owning our mistakes and committing to clear and persistent efforts to grow and improve is. Being Curious Check your presumptions \u2013 we each bring our own ideas and assumptions about how the world should and does work \u2013 what are yours, and how do they affect how you interact with others? How do they shape your perception of new ideas? Ask questions \u2013 one of the strengths of interdisciplinary and diverse teams is that we all bring different knowledge and viewpoints; no one person is expected to know everything. So don\u2019t be afraid to ask, to learn, and to share. Be bold \u2013 significant innovations don\u2019t come from incremental efforts. Be brave in proposing and testing new ideas. When things don\u2019t work, learn from the experience. Invite feedback \u2013 new ideas and improvements can emerge from many places when we\u2019re open to hearing them. Check your defensiveness and listen; accept feedback as a gift toward improving our work and ourselves. Being Collaborative Recognize that everyone is bringing something different to the table \u2013 take the time to get to know each other. Keep an open mind, encourage ideas that are different from yours, and learn from each other\u2019s expertise and experience. Be accountable - great team science depends on trust, communication, respect, and delivering on your commitments. Be clear about your needs, as both a requester and a responder, realistic about your time and capacity commitments, and communicate timelines and standards in advance. Make assumptions explicit and provide context wherever possible - misunderstandings are common on transdisciplinary and cross-cultural teams and can best be managed with intentionality. Check in about assumptions, and be willing to share and correct misunderstandings or mistakes when they happen. Make use of collaboration agreements, communicate clearly and avoid jargon wherever possible. Respect intellectual property and Indigenous data sovereignty \u2013 ESIIL recognizes the extractive and abusive history of scientific engagement with Native peoples, and is committed to doing better. Indigenous knowledge holders are under no obligation to share their data, stories or knowledge. Their work should always be credited, and only shared with permission. Follow guidelines for authorship, Indigenous data sovereignty, and CARE principles. Acknowledge and credit the ideas and work of others. Use the resources that we provide - take advantage of the cyberinfrastructure and data cube at your disposal, but do not use them for unrelated tasks, as it could disrupt the event, introduce security risks, undermine the spirit of collaboration and fair play, and erode trust within the event community. Be safe - never share sensitive personal information; use strong passwords for your Cyverse and GitHub accounts and do not share them with other participants; be cautious of unsolicited emails, messages, or links; and verify online contacts. If you encounter any illegal or harmful activities online related to this event, report them to Tyler McIntosh or Susan Sullivan. Finally, speak up if you experience or notice a dangerous situation, or someone in distress! Code of Conduct: Unacceptable behaviors We adopt the full Code of Conduct of our home institution, the University of Colorado, details of which are found here . To summarize, examples of unacceptable and reportable behaviors include, but are not limited to: Harassment, intimidation, or discrimination in any form Physical or verbal abuse by anyone to anyone, including but not limited to a participant, member of the public, guest, member of any institution or sponsor Unwelcome sexual attention or advances Personal attacks directed at other guests, members, participants, etc. Alarming, intimidating, threatening, or hostile comments or conduct Inappropriate use of nudity and/or sexual images in public spaces or in presentations Threatening or stalking anyone Unauthorized use or sharing of personal or confidential information or private communication Continuing interactions, including but not limited to conversations, photographies, recordings, instant messages, and emails, after being asked to stop Ethical and scientific misconduct, including failing to credit contributions or respect intellectual property Engaging in any illegal activities, including hacking, cheating, or unauthorized access to systems or data Using the cyberinfrastructure provided by the organizers for activities unrelated to this event. Other conduct which could reasonably be considered inappropriate in a professional setting. The University of Colorado recognizes all Federal and State protected classes, which include the following: race, color, national origin, sex, pregnancy, age, marital status, disability, creed, religion, sexual orientation, gender identity, gender expression, veteran status, political affiliation or political philosophy. Mistreatment or harassment not related to protected class also has a negative impact and will be addressed by the ESIIL team. Anyone requested to stop unacceptable behavior is expected to comply immediately. If there is a clear violation of the code of conduct during an ESIIL event\u2014for example, a meeting is Zoom bombed or a team member is verbally abusing another participant during a workshop\u2014 ESIIL leaders, facilitators (or their designee) or campus/local police may take any action deemed necessary and appropriate, including expelling the violator, or immediate removal of the violator from any online or in-person event or platform without warning or refund. If such actions are necessary, there will be follow up with the ESIIL Diversity Equity and Inclusion (DEI) team to determine what further action is needed (see Reporting Process and Consequences below). Addressing Behavior Directly For smaller incidents that might be settled with a brief conversation, you may choose to contact the person in question or set up a (video) conversation to discuss how the behavior affected you. Please use this approach only if you feel comfortable; you do not have to carry the weight of addressing these issues yourself. If you are interested in this option but unsure how to go about it, please contact the ESIIL DEI lead, Susan Sullivan, first\u2014she will have advice on how to make the conversation happen and is available to join you in a conversation as requested. Reporting Process and Consequences We take any reports of Code of Conduct violations seriously, and aim to support those who are impacted and ensure that problematic behavior doesn\u2019t happen again. Making a Report If you believe you\u2019re experiencing or have experienced unacceptable behavior that is counter to this code of conduct, or you are witness to this behavior happening to someone else, we encourage you to contact our DEI lead: Susan Sullivan, CIRES Email: susan.sullivan@colorado.edu You may also choose to anonymously report behavior to ESIIL using this form . The DEI team will keep reports as confidential as possible. However, as mandatory reporters, we have an obligation to report alleged protected class violations to our home institution or to law enforcement. Specifically: Cases of potential protected-class harassment will be reported to the CU Office of Institutional Equity and Compliance. If the violation is made by a member of another institution, that information may also be shared with that member\u2019s home institution by the CU Office of Institutional Equity and Compliance under Title IX. In some instances, harassment information may be shared with the National Science Foundation, who are the funding organization of ESIIL. When we discuss incidents with people who are accused of misconduct (the respondent), we will anonymize details as much as possible to protect the privacy of the reporter and the person who was impacted (the complainant). In some cases, even when the details are anonymized, the respondent may guess at the identities of the reporter and complainants. If you have concerns about retaliation or your personal safety, please let us know (or note that in your report). We encourage you to report in any case, so that we can support you while keeping ESIIL members safe. In some cases, we are able to compile several anonymized reports into a pattern of behavior, and take action based on that pattern. If you prefer to speak with someone who is not on the ESIIL leadership team, or who can maintain confidentiality, you may contact: CU Ombuds Phone: 303-492-5077 (for guidance and support navigating difficult conversations) CU Office of Victim Assistance Phone: 303-492-8855 If you want more information about when to report, or how to help someone who needs to report, please review the resources at Don\u2019t Ignore It . Note: The reporting party does not need to be directly involved in a code of conduct violation incident. Please make a bystander report if you observe a potentially dangerous situation, someone in distress, or violations of these guidelines, even if the situation is not happening to you. What Happens After a Report Is Filed After a member of the ESIIL DEI team takes your report, they will (if necessary) consult with the appropriate support people at CU. The ESIIL DEI team will respond with a status update within 5 business days. During this time, they, or members of the CU Office of Institutional Equity and Compliance, will: Meet with you or review report documentation to determine what happened Consult documentation of past incidents for patterns of behavior Discuss appropriate response(s) to the incident Connect with the appropriate offices and/or make those response(s) Determine the follow-up actions for any impacted people and/or the reporter Follow up with the impacted people, including connecting them with support and resources. As a result of this process, in minor cases ESIIL DEI may communicate with the respondent to: Explain what happened and the impact of their behavior Offer concrete examples of how to improve their behavior Explain consequences of their behavior, or future consequences if the behavior is repeated. For significant infractions, follow up to the report may be turned over to the CU Office of Institutional Equity and Compliance and/or campus police. Possible Consequences to Code of Conduct Violations What follows are examples of possible responses to an incident report. This list is not inclusive, and ESIIL reserves the right to take any action it deems necessary. Generally speaking, the strongest response ESIIL may take is to completely ban a user from further engagement with ESIIL activities and, as is required, report a person to the CU Office of Institutional Equity and Compliance and/or their home institution and NSF. If law enforcement should be involved, they will recommend that the complainant make that contact. Employees of CU Boulder may also be subject to consequences as determined by the institution. In addition to the responses above, ESIIL responses may include but are not limited to the following: A verbal discussion in person or via phone/Zoom followed by documentation of the conversation via email Not publishing the video or slides of a talk that violated the code of conduct Not allowing a speaker who violated the code of conduct to give (further) talks Immediately ending any team leadership, membership, or other responsibilities and privileges that a person holds Temporarily banning a person from ESIIL activities Permanently banning a person from ESIIL activities Nothing, if the behavior is determined to not be a code of conduct violation Do you need more resources? Please don\u2019t hesitate to contact the ESIIL DEI lead, Susan Sullivan, if you have questions or concerns. The CU Office of Institutional Equity and Compliance is a resource for all of us in navigating this space. They also offer resource materials that can assist you in exploring various topics and skills here. If you have questions about what, when or how to report, or how to help someone else with concerns, Don\u2019t Ignore It. CU Ombud\u2019s Office: Confidential support to navigate university situations. (Most universities have these resources) The CU Office of Victims Assistance (counseling limited to CU students/staff/faculty, though advocacy is open to everyone engaged with a CU-sponsored activity. Please look for a similar resource on your campus if you are from another institution). National Crisis Hotlines How are we doing? Despite our best intentions, in some cases we may not be living up to our ideals of a positive, supportive, inclusive, respectful and collaborative community. If you feel we could do better, we welcome your feedback. Comments, suggestions and praise are also very welcome! Acknowledgment By participating in this event, you agree to abide by this code of conduct and understand the consequences of violating it. We believe that a respectful and inclusive environment benefits all participants and leads to more creative and successful outcomes. Thank you for your cooperation in making the this event a welcoming event for all. Have fun!","title":"Code of Conduct"},{"location":"additional-resources/code-of-conduct/#code-of-conduct-and-respectful-inclusive-collaboration-guidelines","text":"CIRES Earth Lab and Environmental Data Science Innovation & Inclusion Lab (ESIIL) are committed to building, maintaining, and fostering an inclusive, kind, collaborative, and diverse transdisciplinary environmental data science community, whose members feel welcome, supported, and safe to contribute ideas and knowledge. The Forest Carbon Codefest will follow all aspects of the ESIIL Code of Conduct (below). All community members are responsible for creating this culture, embodying our values, welcoming diverse perspectives and ways of knowing, creating safe inclusive spaces, and conducting ethical science as guided by FAIR (Findable, Accessible, Interoperable, Reusable) and CARE (Collective Benefit, Authority to Control, Responsibility, and Ethics) principles for scientific and Indigenous data management, governance, and stewardship.","title":"Code of Conduct and Respectful Inclusive Collaboration Guidelines"},{"location":"additional-resources/code-of-conduct/#our-values","text":"ESIIL\u2019s vision is grounded in the conviction that innovation and breakthroughs in environmental data science will be precipitated by a diverse, collaborative, curious, and inclusive research community empowered by open data and infrastructure, cross-sector and community partnerships, team science, and engaged learning. As such, our core values center people through inclusion, kindness, respect, collaboration, and genuine relationships. They also center innovation, driven by collaborative, cross-sector science and synthesis, open, accessible data and tools, and fun, diverse teams. Finally, they center learning, propelled by curiosity and accessible, inclusive training, and education opportunities.","title":"Our values"},{"location":"additional-resources/code-of-conduct/#when-and-how-to-use-these-guidelines","text":"These guidelines outline behavior expectations for ESIIL community members. Your participation in the ESIIL network is contingent upon following these guidelines in all ESIIL activities, including, but not limited to, participating in meetings, webinars, hackathons, working groups, hosted or funded by ESIIL, as well as email lists and online forums such as GutHub, Slack, and Twitter. These guidelines have been adapted from those of the International Arctic Research Policy Committee, the Geological Society of America, the American Geophysical Union, the University Corporation for Atmospheric Research, The Carpentries, and others. We encourage other organizations to adapt these guidelines for use in their own meetings. Note: Working groups and hackathon/codefest teams are encouraged to discuss these guidelines and what they mean to them, and will have the opportunity to add to them to specifically support and empower their team. Collaborative and behavior commitments complement data use, management, authorship, and access plans that commit to CARE and FAIR principles.","title":"When and how to use these guidelines"},{"location":"additional-resources/code-of-conduct/#behavior-agreements","text":"ESIIL community members are expected to act professionally and respectfully in all activities, such that each person, regardless of gender, gender identity or expression, sexual orientation, disability, physical appearance, age, body size, race, religion, national origin, ethnicity, level of experience, language fluency, political affiliation, veteran status, pregnancy, country of origin, and any other characteristic protected under state or federal law, feels safe and welcome in our activities and community. We gain strength from diversity and actively seek participation from those who enhance it. In order to garner the benefits of a diverse community and to reach the full potential of our mission and charge, ESIIL participants must be allowed to develop a sense of belonging and trust within a respectful, inclusive, and collaborative culture. Guiding behaviors that contribute to this culture include, but are not limited to:","title":"Behavior Agreements"},{"location":"additional-resources/code-of-conduct/#showing-respect","text":"Listen carefully \u2013 we each bring our own styles of communication, language, and ideas, and we must do our best to accept and accommodate differences. Do not interrupt when someone is speaking and maintain an open mind when others have different ideas than yours. Be present \u2013 when engaging with others, give them your full attention. If you need to respond to outside needs, please step away from the group quietly. Be kind \u2013 offer positive, supportive comments and constructive feedback. Critique ideas, not people. Harassment, discrimination, bullying, aggression, including offensive comments, jokes, and imagery, are unacceptable, regardless of intent, and will not be tolerated. Be punctual - adhere to the schedule provided by the organizers and avoid disruptive behavior during presentations, trainings, or working sessions. Respect privacy - be mindful of the confidentiality of others. Always obtain explicit consent before recording, sharing, or using someone else\u2019s personal information, photos, or recordings. Practice good digital etiquette (netiquette) when communicating online, whether in emails, messages, or social media - think before posting online and consider the potential impact on others. Do not share or distribute content generated by or involving others without their explicit consent.","title":"Showing Respect"},{"location":"additional-resources/code-of-conduct/#being-inclusive","text":"Create space for everyone to participate \u2013 be thoughtful about who is at the table; openly address accessibility needs, and provide multiple ways to contribute. Be welcoming \u2013 ESIIL participants come from a wide range of skill levels and career stages, backgrounds, and cultures. Demonstrate that you value these different perspectives and identities through your words and actions, including through correct use of names, titles, and pronouns. Be self-aware \u2013 recognize that positionality, identity, unconscious biases, and upbringing can all affect how words and behaviors are perceived. Ensure that your words and behavior make others feel welcome. Commit to ongoing learning \u2013 the move toward inclusive, equitable, and just environmental data science is a collective journey. Continue to learn about and apply practices of inclusion, anti-racism, bystander intervention, and cultural sensitivity. None of us is perfect; all of us will, from time to time, fail to live up to our own high standards. Being perfect is not what matters; owning our mistakes and committing to clear and persistent efforts to grow and improve is.","title":"Being Inclusive"},{"location":"additional-resources/code-of-conduct/#being-curious","text":"Check your presumptions \u2013 we each bring our own ideas and assumptions about how the world should and does work \u2013 what are yours, and how do they affect how you interact with others? How do they shape your perception of new ideas? Ask questions \u2013 one of the strengths of interdisciplinary and diverse teams is that we all bring different knowledge and viewpoints; no one person is expected to know everything. So don\u2019t be afraid to ask, to learn, and to share. Be bold \u2013 significant innovations don\u2019t come from incremental efforts. Be brave in proposing and testing new ideas. When things don\u2019t work, learn from the experience. Invite feedback \u2013 new ideas and improvements can emerge from many places when we\u2019re open to hearing them. Check your defensiveness and listen; accept feedback as a gift toward improving our work and ourselves.","title":"Being Curious"},{"location":"additional-resources/code-of-conduct/#being-collaborative","text":"Recognize that everyone is bringing something different to the table \u2013 take the time to get to know each other. Keep an open mind, encourage ideas that are different from yours, and learn from each other\u2019s expertise and experience. Be accountable - great team science depends on trust, communication, respect, and delivering on your commitments. Be clear about your needs, as both a requester and a responder, realistic about your time and capacity commitments, and communicate timelines and standards in advance. Make assumptions explicit and provide context wherever possible - misunderstandings are common on transdisciplinary and cross-cultural teams and can best be managed with intentionality. Check in about assumptions, and be willing to share and correct misunderstandings or mistakes when they happen. Make use of collaboration agreements, communicate clearly and avoid jargon wherever possible. Respect intellectual property and Indigenous data sovereignty \u2013 ESIIL recognizes the extractive and abusive history of scientific engagement with Native peoples, and is committed to doing better. Indigenous knowledge holders are under no obligation to share their data, stories or knowledge. Their work should always be credited, and only shared with permission. Follow guidelines for authorship, Indigenous data sovereignty, and CARE principles. Acknowledge and credit the ideas and work of others. Use the resources that we provide - take advantage of the cyberinfrastructure and data cube at your disposal, but do not use them for unrelated tasks, as it could disrupt the event, introduce security risks, undermine the spirit of collaboration and fair play, and erode trust within the event community. Be safe - never share sensitive personal information; use strong passwords for your Cyverse and GitHub accounts and do not share them with other participants; be cautious of unsolicited emails, messages, or links; and verify online contacts. If you encounter any illegal or harmful activities online related to this event, report them to Tyler McIntosh or Susan Sullivan. Finally, speak up if you experience or notice a dangerous situation, or someone in distress!","title":"Being Collaborative"},{"location":"additional-resources/code-of-conduct/#code-of-conduct-unacceptable-behaviors","text":"We adopt the full Code of Conduct of our home institution, the University of Colorado, details of which are found here . To summarize, examples of unacceptable and reportable behaviors include, but are not limited to: Harassment, intimidation, or discrimination in any form Physical or verbal abuse by anyone to anyone, including but not limited to a participant, member of the public, guest, member of any institution or sponsor Unwelcome sexual attention or advances Personal attacks directed at other guests, members, participants, etc. Alarming, intimidating, threatening, or hostile comments or conduct Inappropriate use of nudity and/or sexual images in public spaces or in presentations Threatening or stalking anyone Unauthorized use or sharing of personal or confidential information or private communication Continuing interactions, including but not limited to conversations, photographies, recordings, instant messages, and emails, after being asked to stop Ethical and scientific misconduct, including failing to credit contributions or respect intellectual property Engaging in any illegal activities, including hacking, cheating, or unauthorized access to systems or data Using the cyberinfrastructure provided by the organizers for activities unrelated to this event. Other conduct which could reasonably be considered inappropriate in a professional setting. The University of Colorado recognizes all Federal and State protected classes, which include the following: race, color, national origin, sex, pregnancy, age, marital status, disability, creed, religion, sexual orientation, gender identity, gender expression, veteran status, political affiliation or political philosophy. Mistreatment or harassment not related to protected class also has a negative impact and will be addressed by the ESIIL team. Anyone requested to stop unacceptable behavior is expected to comply immediately. If there is a clear violation of the code of conduct during an ESIIL event\u2014for example, a meeting is Zoom bombed or a team member is verbally abusing another participant during a workshop\u2014 ESIIL leaders, facilitators (or their designee) or campus/local police may take any action deemed necessary and appropriate, including expelling the violator, or immediate removal of the violator from any online or in-person event or platform without warning or refund. If such actions are necessary, there will be follow up with the ESIIL Diversity Equity and Inclusion (DEI) team to determine what further action is needed (see Reporting Process and Consequences below).","title":"Code of Conduct: Unacceptable behaviors"},{"location":"additional-resources/code-of-conduct/#addressing-behavior-directly","text":"For smaller incidents that might be settled with a brief conversation, you may choose to contact the person in question or set up a (video) conversation to discuss how the behavior affected you. Please use this approach only if you feel comfortable; you do not have to carry the weight of addressing these issues yourself. If you are interested in this option but unsure how to go about it, please contact the ESIIL DEI lead, Susan Sullivan, first\u2014she will have advice on how to make the conversation happen and is available to join you in a conversation as requested.","title":"Addressing Behavior Directly"},{"location":"additional-resources/code-of-conduct/#reporting-process-and-consequences","text":"We take any reports of Code of Conduct violations seriously, and aim to support those who are impacted and ensure that problematic behavior doesn\u2019t happen again.","title":"Reporting Process and Consequences"},{"location":"additional-resources/code-of-conduct/#making-a-report","text":"If you believe you\u2019re experiencing or have experienced unacceptable behavior that is counter to this code of conduct, or you are witness to this behavior happening to someone else, we encourage you to contact our DEI lead: Susan Sullivan, CIRES Email: susan.sullivan@colorado.edu You may also choose to anonymously report behavior to ESIIL using this form . The DEI team will keep reports as confidential as possible. However, as mandatory reporters, we have an obligation to report alleged protected class violations to our home institution or to law enforcement.","title":"Making a Report"},{"location":"additional-resources/code-of-conduct/#specifically","text":"Cases of potential protected-class harassment will be reported to the CU Office of Institutional Equity and Compliance. If the violation is made by a member of another institution, that information may also be shared with that member\u2019s home institution by the CU Office of Institutional Equity and Compliance under Title IX. In some instances, harassment information may be shared with the National Science Foundation, who are the funding organization of ESIIL. When we discuss incidents with people who are accused of misconduct (the respondent), we will anonymize details as much as possible to protect the privacy of the reporter and the person who was impacted (the complainant). In some cases, even when the details are anonymized, the respondent may guess at the identities of the reporter and complainants. If you have concerns about retaliation or your personal safety, please let us know (or note that in your report). We encourage you to report in any case, so that we can support you while keeping ESIIL members safe. In some cases, we are able to compile several anonymized reports into a pattern of behavior, and take action based on that pattern. If you prefer to speak with someone who is not on the ESIIL leadership team, or who can maintain confidentiality, you may contact: CU Ombuds Phone: 303-492-5077 (for guidance and support navigating difficult conversations) CU Office of Victim Assistance Phone: 303-492-8855 If you want more information about when to report, or how to help someone who needs to report, please review the resources at Don\u2019t Ignore It . Note: The reporting party does not need to be directly involved in a code of conduct violation incident. Please make a bystander report if you observe a potentially dangerous situation, someone in distress, or violations of these guidelines, even if the situation is not happening to you.","title":"Specifically:"},{"location":"additional-resources/code-of-conduct/#what-happens-after-a-report-is-filed","text":"After a member of the ESIIL DEI team takes your report, they will (if necessary) consult with the appropriate support people at CU. The ESIIL DEI team will respond with a status update within 5 business days. During this time, they, or members of the CU Office of Institutional Equity and Compliance, will: Meet with you or review report documentation to determine what happened Consult documentation of past incidents for patterns of behavior Discuss appropriate response(s) to the incident Connect with the appropriate offices and/or make those response(s) Determine the follow-up actions for any impacted people and/or the reporter Follow up with the impacted people, including connecting them with support and resources.","title":"What Happens After a Report Is Filed"},{"location":"additional-resources/code-of-conduct/#as-a-result-of-this-process-in-minor-cases-esiil-dei-may-communicate-with-the-respondent-to","text":"Explain what happened and the impact of their behavior Offer concrete examples of how to improve their behavior Explain consequences of their behavior, or future consequences if the behavior is repeated. For significant infractions, follow up to the report may be turned over to the CU Office of Institutional Equity and Compliance and/or campus police.","title":"As a result of this process, in minor cases ESIIL DEI may communicate with the respondent to:"},{"location":"additional-resources/code-of-conduct/#possible-consequences-to-code-of-conduct-violations","text":"What follows are examples of possible responses to an incident report. This list is not inclusive, and ESIIL reserves the right to take any action it deems necessary. Generally speaking, the strongest response ESIIL may take is to completely ban a user from further engagement with ESIIL activities and, as is required, report a person to the CU Office of Institutional Equity and Compliance and/or their home institution and NSF. If law enforcement should be involved, they will recommend that the complainant make that contact. Employees of CU Boulder may also be subject to consequences as determined by the institution. In addition to the responses above, ESIIL responses may include but are not limited to the following: A verbal discussion in person or via phone/Zoom followed by documentation of the conversation via email Not publishing the video or slides of a talk that violated the code of conduct Not allowing a speaker who violated the code of conduct to give (further) talks Immediately ending any team leadership, membership, or other responsibilities and privileges that a person holds Temporarily banning a person from ESIIL activities Permanently banning a person from ESIIL activities Nothing, if the behavior is determined to not be a code of conduct violation Do you need more resources? Please don\u2019t hesitate to contact the ESIIL DEI lead, Susan Sullivan, if you have questions or concerns. The CU Office of Institutional Equity and Compliance is a resource for all of us in navigating this space. They also offer resource materials that can assist you in exploring various topics and skills here. If you have questions about what, when or how to report, or how to help someone else with concerns, Don\u2019t Ignore It. CU Ombud\u2019s Office: Confidential support to navigate university situations. (Most universities have these resources) The CU Office of Victims Assistance (counseling limited to CU students/staff/faculty, though advocacy is open to everyone engaged with a CU-sponsored activity. Please look for a similar resource on your campus if you are from another institution). National Crisis Hotlines How are we doing? Despite our best intentions, in some cases we may not be living up to our ideals of a positive, supportive, inclusive, respectful and collaborative community. If you feel we could do better, we welcome your feedback. Comments, suggestions and praise are also very welcome! Acknowledgment By participating in this event, you agree to abide by this code of conduct and understand the consequences of violating it. We believe that a respectful and inclusive environment benefits all participants and leads to more creative and successful outcomes. Thank you for your cooperation in making the this event a welcoming event for all. Have fun!","title":"Possible Consequences to Code of Conduct Violations"},{"location":"additional-resources/cyverse_hacks/","text":"Transitioning Workflows to CyVerse: Tips & Tricks Forest Carbon Codefest Data Storage Path: ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/ Your team has a subdirectory within the Team_outputs directory. Setup CyVerse Account: Create an account if not already owned. Contact Tyson for account upgrades after maximizing current limits. GitHub Connection Follow the guide for connecting GitHub to CyVerse Select \u201cJupyterLab ESIIL\u201d and choose \u201cmacrosystems\u201d in the version dropdown. Clone into /home/jovyan/data-store . Clone innovation-summit-utils for SSH connection to GitHub. Run conda install -c conda-forge openssh in the terminal if encountering errors. GitHub authentication is session-specific. RStudio in Discovery Environment Copy your instance ID. It can be found in your analyis URL in form https:// .cyverse.run/lab. Use your ID in these links and run them each, in sequence, in the same browser window: https://<id>.cyverse.run/rstudio/auth-sign-in https://<id>.cyverse.run/rstudio/ Data Transfer to CyVerse Use GoCommands for HPC/CyVerse transfers. Installation: Linux: GOCMD_VER=$(curl -L -s https://raw.githubusercontent.com/cyverse/gocommands/main/VERSION.txt); \\ curl -L -s https://github.com/cyverse/gocommands/releases/download/${GOCMD_VER}/gocmd-${GOCMD_VER}-linux-amd64.tar.gz | tar zxvf - Windows Powershell: curl -o gocmdv.txt https://raw.githubusercontent.com/cyverse/gocommands/main/VERSION.txt ; $env:GOCMD_VER = (Get-Content gocmdv.txt) curl -o gocmd.zip https://github.com/cyverse/gocommands/releases/download/$env:GOCMD_VER/gocmd-$env:GOCMD_VER-windows-amd64.zip ; tar zxvf gocmd.zip ; del gocmd.zip ; del gocmdv.txt Usage: ./gocmd init Hit enter until you are asked for your iRODS Username (which is your cyverse username) Use put for upload and get for download. Ensure correct CyVerse directory path. Note that the CyVerse directory path should start from \u201c/iplant/home/\u2026\u201d (i.e. if you start from \u2018/home/jovyan/\u2026\u2019 GoCommands will not find the directory and throw an error)","title":"Cyverse hacks"},{"location":"additional-resources/cyverse_hacks/#transitioning-workflows-to-cyverse-tips-tricks","text":"","title":"Transitioning Workflows to CyVerse: Tips &amp; Tricks"},{"location":"additional-resources/cyverse_hacks/#forest-carbon-codefest-data-storage","text":"Path: ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/ Your team has a subdirectory within the Team_outputs directory.","title":"Forest Carbon Codefest Data Storage"},{"location":"additional-resources/cyverse_hacks/#setup","text":"CyVerse Account: Create an account if not already owned. Contact Tyson for account upgrades after maximizing current limits.","title":"Setup"},{"location":"additional-resources/cyverse_hacks/#github-connection","text":"Follow the guide for connecting GitHub to CyVerse Select \u201cJupyterLab ESIIL\u201d and choose \u201cmacrosystems\u201d in the version dropdown. Clone into /home/jovyan/data-store . Clone innovation-summit-utils for SSH connection to GitHub. Run conda install -c conda-forge openssh in the terminal if encountering errors. GitHub authentication is session-specific.","title":"GitHub Connection"},{"location":"additional-resources/cyverse_hacks/#rstudio-in-discovery-environment","text":"Copy your instance ID. It can be found in your analyis URL in form https:// .cyverse.run/lab. Use your ID in these links and run them each, in sequence, in the same browser window: https://<id>.cyverse.run/rstudio/auth-sign-in https://<id>.cyverse.run/rstudio/","title":"RStudio in Discovery Environment"},{"location":"additional-resources/cyverse_hacks/#data-transfer-to-cyverse","text":"Use GoCommands for HPC/CyVerse transfers. Installation: Linux: GOCMD_VER=$(curl -L -s https://raw.githubusercontent.com/cyverse/gocommands/main/VERSION.txt); \\ curl -L -s https://github.com/cyverse/gocommands/releases/download/${GOCMD_VER}/gocmd-${GOCMD_VER}-linux-amd64.tar.gz | tar zxvf - Windows Powershell: curl -o gocmdv.txt https://raw.githubusercontent.com/cyverse/gocommands/main/VERSION.txt ; $env:GOCMD_VER = (Get-Content gocmdv.txt) curl -o gocmd.zip https://github.com/cyverse/gocommands/releases/download/$env:GOCMD_VER/gocmd-$env:GOCMD_VER-windows-amd64.zip ; tar zxvf gocmd.zip ; del gocmd.zip ; del gocmdv.txt Usage: ./gocmd init Hit enter until you are asked for your iRODS Username (which is your cyverse username) Use put for upload and get for download. Ensure correct CyVerse directory path. Note that the CyVerse directory path should start from \u201c/iplant/home/\u2026\u201d (i.e. if you start from \u2018/home/jovyan/\u2026\u2019 GoCommands will not find the directory and throw an error)","title":"Data Transfer to CyVerse"},{"location":"additional-resources/participant_agreement/","text":"Participant Agreement This Participant Agreement (\u201cAgreement\u201d) is a contract between you (\u201cYou/Your\u201d or \u201cParticipant\u201d) and THE REGENTS OF THE UNIVERSITY OF COLORADO, a body corporate, acting on behalf of the University of Colorado Boulder, a public institution of higher education created under the Constitution and the Law of the State of Colorado (the \u201cUniversity\u201d), having offices located at 3100 Marine Street, Boulder, CO 80309. In consideration of Your participation in the Forest Carbon Codefest (the \u201cCodefest\u201d), the sufficiency of which is hereby acknowledged, You agree as follows: Environmental Data Science Innovation & Inclusion Lab (\u201cESIIL\u201d) is a National Science Foundation (\u201cNSF\u201d) funded data synthesis center led by the University. Earth Lab is part of the Cooperative Institute for Research in Environmental Sciences (CIRES) specializing in data-intensive open, reproducible environmental science. Earth Lab and ESIIL will co-host the Codefest in person from March 12 through March 14, 2023. Codefest Description The Codefest aims to use big data to understand aboveground forest carbon dynamics. It brings together environmental scientists, data analysts, and forest experts from academia, non-profits, and the private sector under one roof to collaborate and provide solutions to on-the-ground needs in a respectful and inclusive environment. The Codefest provides resources to address environmental challenges, such as participation in training on Environmental Data Science and cyberinfrastructure. You will work on a cloud-based, highly secure platform that allows for the seamless integration of data sets and analytical tools. The Codefest encourages individuals participating to create a solution in the form of a research project, computer code, and models (individually and/or collectively, a \u201cSolution\u201d). How to Participate You will join a team of environmental scientists, data experts, and coders to explore curated data, consider the objectivity of the data, propose a scientific question that can be addressed with all or some of the data sets, and analyze the data in an attempt to answer your scientific question by the end of the event. You will present your Solution to the event community. Earth Lab and ESIIL will provide environmental data, cyberinfrastructure, cyberinfrastructure and data analytics training, and technical support. The team members of the winning Solution will receive a small monetary award. Details regarding how the winning Solution will be chosen and how the prize will be managed are outlined in the Official Rules for the Codefest (\u201cOfficial Rules\u201d). Representations and Warranties By and through Your participation in the Codefest, You represent and warrant the following: You have read, understand, and agree to abide by the Code of Conduct and Respectful Inclusive Collaboration Guidelines for the Forest Carbon Codefest (\u201cCode of Conduct\u201d) and the Official Rules. Any decisions concerning the Code of Conduct, Official Rules, or any other matter relating to this Codefest by the University is final and binding on all Participants. Codefest Assets 5.1 Access and Use By participating in the Codefest, You may receive access to certain datasets, webinars, and/or other copyrighted materials (collectively, the \u201cCodefest Assets\u201d). You agree to follow all licenses, restrictions, and other instructions provided to You with the Codefest Assets. 5.2 Disclaimer The Codefest Assets are provided \u201cas is\u201d without warranty of any kind, either express or implied, including, without limitation, any implied warranties of merchantability and fitness for a particular purpose. Without limiting the foregoing, the University does not warrant that the Materials will be suitable for Your Solution or that the operation or supply of the Codefest Assets will be uninterrupted or error free. 5.3 Restrictions You agree not to access or use the Codefest Assets in a manner that may interfere with any other participants\u2019 or users\u2019 use of such assets, unless provided with express written consent by the University. Your access to and use of the Codefest Assets may be limited, throttled, or terminated at any time at the sole discretion of the University. Intellectual Property For purposes of this Agreement, \u201cIntellectual Property\u201d includes all copyright, patent, trademark, trade secret and other intellectual property rights. 6.1 Solution Publication You agree to make Your Solution publicly available in GitHub under the MIT open-source license within five (5) months from the end of the Codefest. 6.2 Government Rights In consideration of the funding provided by NSF for the Codefest, You hereby grant and agree to grant to the U.S. Government (i) a nonexclusive, non-transferable, irrevocable, royalty-free license to exercise or have exercised for or on behalf of the U.S. throughout the world all the exclusive rights provided by copyright in any copyrightable intellectual property that You create during the Codefest and (ii) a nonexclusive, non-transferable, irrevocable, paid-up license to practice, or have practiced for or on its behalf, any invention created by You during the Codefest throughout the world. 6.3 NSF Agreement and Invention Ownership You acknowledge that the Codefest is subject to NSF\u2019s Cooperative Agreement Terms and Conditions effective May 12, 2023. As such, the U.S. Government may be entitled to own any invention that You create during the Codefest as further outlined within 37 CFR 401 and the NSF Proposal & Award Policies and Procedures Guide. In such event, You agree to assign any invention created by You during the Codefest to the U.S. Government and to reasonably cooperate with the University and NSF to accomplish the same. 6.4 Representations and Warranties You represent and warrant that You are legally entitled to grant the above licenses and other intellectual property rights as outlined in Sections 6.1, 6.2, and 6.3. If You are an individual, and if Your employer(s) and/or another third party has rights to Intellectual Property that You create that includes Your Solution, You represent that You have received permission to participate in the Codefest on behalf of that employer or third party and that Your employer or the third party has waived such rights for Your contributions. 6.5 Originality and Third-Party Materials You represent that Your Solution is Your original creation. If you obtain permission under Section 6.5 to include third-party materials, You represent that Your Solution includes complete details of any third-party license or other restriction (including, but not limited to, related patents and trademarks) of which You are aware and which are associated with any part of Your Solution. You represent and warrant that You will not submit any materials to the University that You know or believe to have components that are malicious or harmful. You represent that You will perform a reasonable amount of due diligence in order to be properly informed of third-party licenses, infringing materials, or harmful content associated with any part of Your Solution. 6.6 Submission of External Work Should You wish to submit work that is not Your original creation during the Codefest (and that is not a Codefest Asset), You may ask for permission from the University to incorporate such work into Your Solution. To request such permission, identify the complete details of the materials you wish to use, including any licenses or other restrictions (including, but not limited to, related patents, trademarks, and license agreements) of which You are personally aware. Request for permission to add materials created outside of the Codefest must be submitted in advance of the Codefest to: Tyler McIntosh; tyler.l.mcintosh@colorado.edu . 6.7 Notification of Changes You agree to notify the University of any facts or circumstances of which You become aware that would make these representations inaccurate in any respect. You agree to notify the University of any administrative or court proceeding which involve Your Solution. Limitation of Liability TO THE EXTENT ALLOWED BY LAW, IN NO EVENT SHALL THE UNIVERSITY, ITS PARTNERS, LICENSORS, SERVICE PROVIDERS, OR ANY OF THEIR RESPECTIVE OFFICERS, DIRECTORS, AGENTS, EMPLOYEES OR REPRESENTATIVES, BE LIABLE FOR DIRECT, INCIDENTAL, CONSEQUENTIAL, EXEMPLARY OR PUNITIVE DAMAGES ARISING OUT OF OR IN CONNECTION WITH THE HACKATHON OR THIS AGREEMENT (HOWEVER ARISING, INCLUDING NEGLIGENCE). IF YOU HAVE A DISPUTE WITH ANY PARTICIPANT OR ANY OTHER THIRD PARTY, YOU RELEASE THE UNIVERSITY, ITS, PARTNERS, LICENSORS, AND SERVICE PROVIDERS, AND EACH OF THEIR RESPECTIVE OFFICERS, DIRECTORS, AGENTS, EMPLOYEES AND REPRESENTATIVES FROM ANY AND ALL CLAIMS, DEMANDS AND DAMAGES (ACTUAL AND CONSEQUENTIAL) OF EVERY KIND AND NATURE ARISING OUT OF OR IN ANY WAY CONNECTED WITH SUCH DISPUTES. YOU AGREE THAT ANY CLAIMS AGAINST UNIVERSITY ARISING OUT OF THE HACKATHON OR THIS AGREEMENT MUST BE FILED WITHIN ONE YEAR AFTER SUCH CLAIM AROSE; OTHERWISE, YOUR CLAIM IS PERMANENTLY BARRED. Not an Offer or Contract of Employment Under no circumstances will Your participation in the Codefest or anything in this Agreement be construed as an offer or contract of employment with the University. Additional Terms You must be at least eighteen (18) years of age to participate in the Codefest. The Codefest is subject to applicable federal, state, and local laws. The University reserves the right to permanently disqualify any person from the Codefest that it reasonably believes has violated this Agreement, the Code of Conduct, and/or the Official Rules. Any attempt to deliberately damage the Codefest or the operation thereof is unlawful and subject to legal action by the University, which may seek damages to the fullest extent permitted by law. The University assumes no responsibility for any injury or damage to Your or any other person\u2019s computer relating to or resulting from entering or downloading materials or software in connection with the Codefest. The University is not responsible for telecommunications, network, electronic, technical, or computer failures of any kind; for inaccurate transcription of entry information; for any human or electronic error; or for Solutions that are stolen, misdirected, garbled, delayed, lost, late, damaged, or returned. The University reserves the right to cancel, modify, or suspend the Codefest or any element thereof (including, without limitation, this Agreement) without notice in any manner and for any reason (including, without limitation, in the event of any unanticipated occurrence that is not fully addressed in this Agreement). The University may prohibit any person from participating in the Codefest, if such person shows a disregard for this Agreement; acts with an intent to annoy, abuse, threaten, or harass any other entrant or any agents or representatives of the University (or any associated, partners, licensors, or service providers for the University); or behaves in any other disruptive manner (as determined by the University in its sole discretion). Nothing contained in this Agreement shall be construed as an express or implied waiver by University of its governmental immunity or of the governmental immunity of the State of Colorado. Your Solutions shall not contain any item(s) that are either export-controlled under the International Traffic in Arms Regulations, or that appear on the Commerce Control List (except as EAR99) of the Export Administration Regulations. Dispute Resolution This Agreement and the Codefest shall be governed and construed in accordance with and governed by the laws of the state of Colorado without giving effect to conflict of law provisions. Entire Agreement This Agreement, along with the Official Rules, and the Event Code of Conduct, constitutes the entire agreement between the University and You with respect to the Codefest and supersedes all previous or contemporaneous oral or written agreements concerning the Codefest. In the event of a conflict between this Agreement, the Official Rules and/or the Event Code of Conduct, the conflict shall be resolved with the following order of precedence: This Agreement The Official Rules The Event Code of Conduct Severability The invalidity, illegality, or unenforceability of any one or more phrases, sentences, clauses, or sections in this Agreement does not affect the remaining portions of this Agreement. If you have questions about the Codefest, please contact Tyler McIntosh at tyler.l.mcintosh@colorado.edu .","title":"Participant agreement"},{"location":"additional-resources/participant_agreement/#participant-agreement","text":"This Participant Agreement (\u201cAgreement\u201d) is a contract between you (\u201cYou/Your\u201d or \u201cParticipant\u201d) and THE REGENTS OF THE UNIVERSITY OF COLORADO, a body corporate, acting on behalf of the University of Colorado Boulder, a public institution of higher education created under the Constitution and the Law of the State of Colorado (the \u201cUniversity\u201d), having offices located at 3100 Marine Street, Boulder, CO 80309. In consideration of Your participation in the Forest Carbon Codefest (the \u201cCodefest\u201d), the sufficiency of which is hereby acknowledged, You agree as follows: Environmental Data Science Innovation & Inclusion Lab (\u201cESIIL\u201d) is a National Science Foundation (\u201cNSF\u201d) funded data synthesis center led by the University. Earth Lab is part of the Cooperative Institute for Research in Environmental Sciences (CIRES) specializing in data-intensive open, reproducible environmental science. Earth Lab and ESIIL will co-host the Codefest in person from March 12 through March 14, 2023.","title":"Participant Agreement"},{"location":"additional-resources/participant_agreement/#codefest-description","text":"The Codefest aims to use big data to understand aboveground forest carbon dynamics. It brings together environmental scientists, data analysts, and forest experts from academia, non-profits, and the private sector under one roof to collaborate and provide solutions to on-the-ground needs in a respectful and inclusive environment. The Codefest provides resources to address environmental challenges, such as participation in training on Environmental Data Science and cyberinfrastructure. You will work on a cloud-based, highly secure platform that allows for the seamless integration of data sets and analytical tools. The Codefest encourages individuals participating to create a solution in the form of a research project, computer code, and models (individually and/or collectively, a \u201cSolution\u201d).","title":"Codefest Description"},{"location":"additional-resources/participant_agreement/#how-to-participate","text":"You will join a team of environmental scientists, data experts, and coders to explore curated data, consider the objectivity of the data, propose a scientific question that can be addressed with all or some of the data sets, and analyze the data in an attempt to answer your scientific question by the end of the event. You will present your Solution to the event community. Earth Lab and ESIIL will provide environmental data, cyberinfrastructure, cyberinfrastructure and data analytics training, and technical support. The team members of the winning Solution will receive a small monetary award. Details regarding how the winning Solution will be chosen and how the prize will be managed are outlined in the Official Rules for the Codefest (\u201cOfficial Rules\u201d).","title":"How to Participate"},{"location":"additional-resources/participant_agreement/#representations-and-warranties","text":"By and through Your participation in the Codefest, You represent and warrant the following: You have read, understand, and agree to abide by the Code of Conduct and Respectful Inclusive Collaboration Guidelines for the Forest Carbon Codefest (\u201cCode of Conduct\u201d) and the Official Rules. Any decisions concerning the Code of Conduct, Official Rules, or any other matter relating to this Codefest by the University is final and binding on all Participants.","title":"Representations and Warranties"},{"location":"additional-resources/participant_agreement/#codefest-assets","text":"","title":"Codefest Assets"},{"location":"additional-resources/participant_agreement/#51-access-and-use","text":"By participating in the Codefest, You may receive access to certain datasets, webinars, and/or other copyrighted materials (collectively, the \u201cCodefest Assets\u201d). You agree to follow all licenses, restrictions, and other instructions provided to You with the Codefest Assets.","title":"5.1 Access and Use"},{"location":"additional-resources/participant_agreement/#52-disclaimer","text":"The Codefest Assets are provided \u201cas is\u201d without warranty of any kind, either express or implied, including, without limitation, any implied warranties of merchantability and fitness for a particular purpose. Without limiting the foregoing, the University does not warrant that the Materials will be suitable for Your Solution or that the operation or supply of the Codefest Assets will be uninterrupted or error free.","title":"5.2 Disclaimer"},{"location":"additional-resources/participant_agreement/#53-restrictions","text":"You agree not to access or use the Codefest Assets in a manner that may interfere with any other participants\u2019 or users\u2019 use of such assets, unless provided with express written consent by the University. Your access to and use of the Codefest Assets may be limited, throttled, or terminated at any time at the sole discretion of the University.","title":"5.3 Restrictions"},{"location":"additional-resources/participant_agreement/#intellectual-property","text":"For purposes of this Agreement, \u201cIntellectual Property\u201d includes all copyright, patent, trademark, trade secret and other intellectual property rights.","title":"Intellectual Property"},{"location":"additional-resources/participant_agreement/#61-solution-publication","text":"You agree to make Your Solution publicly available in GitHub under the MIT open-source license within five (5) months from the end of the Codefest.","title":"6.1 Solution Publication"},{"location":"additional-resources/participant_agreement/#62-government-rights","text":"In consideration of the funding provided by NSF for the Codefest, You hereby grant and agree to grant to the U.S. Government (i) a nonexclusive, non-transferable, irrevocable, royalty-free license to exercise or have exercised for or on behalf of the U.S. throughout the world all the exclusive rights provided by copyright in any copyrightable intellectual property that You create during the Codefest and (ii) a nonexclusive, non-transferable, irrevocable, paid-up license to practice, or have practiced for or on its behalf, any invention created by You during the Codefest throughout the world.","title":"6.2 Government Rights"},{"location":"additional-resources/participant_agreement/#63-nsf-agreement-and-invention-ownership","text":"You acknowledge that the Codefest is subject to NSF\u2019s Cooperative Agreement Terms and Conditions effective May 12, 2023. As such, the U.S. Government may be entitled to own any invention that You create during the Codefest as further outlined within 37 CFR 401 and the NSF Proposal & Award Policies and Procedures Guide. In such event, You agree to assign any invention created by You during the Codefest to the U.S. Government and to reasonably cooperate with the University and NSF to accomplish the same.","title":"6.3 NSF Agreement and Invention Ownership"},{"location":"additional-resources/participant_agreement/#64-representations-and-warranties","text":"You represent and warrant that You are legally entitled to grant the above licenses and other intellectual property rights as outlined in Sections 6.1, 6.2, and 6.3. If You are an individual, and if Your employer(s) and/or another third party has rights to Intellectual Property that You create that includes Your Solution, You represent that You have received permission to participate in the Codefest on behalf of that employer or third party and that Your employer or the third party has waived such rights for Your contributions.","title":"6.4 Representations and Warranties"},{"location":"additional-resources/participant_agreement/#65-originality-and-third-party-materials","text":"You represent that Your Solution is Your original creation. If you obtain permission under Section 6.5 to include third-party materials, You represent that Your Solution includes complete details of any third-party license or other restriction (including, but not limited to, related patents and trademarks) of which You are aware and which are associated with any part of Your Solution. You represent and warrant that You will not submit any materials to the University that You know or believe to have components that are malicious or harmful. You represent that You will perform a reasonable amount of due diligence in order to be properly informed of third-party licenses, infringing materials, or harmful content associated with any part of Your Solution.","title":"6.5 Originality and Third-Party Materials"},{"location":"additional-resources/participant_agreement/#66-submission-of-external-work","text":"Should You wish to submit work that is not Your original creation during the Codefest (and that is not a Codefest Asset), You may ask for permission from the University to incorporate such work into Your Solution. To request such permission, identify the complete details of the materials you wish to use, including any licenses or other restrictions (including, but not limited to, related patents, trademarks, and license agreements) of which You are personally aware. Request for permission to add materials created outside of the Codefest must be submitted in advance of the Codefest to: Tyler McIntosh; tyler.l.mcintosh@colorado.edu .","title":"6.6 Submission of External Work"},{"location":"additional-resources/participant_agreement/#67-notification-of-changes","text":"You agree to notify the University of any facts or circumstances of which You become aware that would make these representations inaccurate in any respect. You agree to notify the University of any administrative or court proceeding which involve Your Solution.","title":"6.7 Notification of Changes"},{"location":"additional-resources/participant_agreement/#limitation-of-liability","text":"TO THE EXTENT ALLOWED BY LAW, IN NO EVENT SHALL THE UNIVERSITY, ITS PARTNERS, LICENSORS, SERVICE PROVIDERS, OR ANY OF THEIR RESPECTIVE OFFICERS, DIRECTORS, AGENTS, EMPLOYEES OR REPRESENTATIVES, BE LIABLE FOR DIRECT, INCIDENTAL, CONSEQUENTIAL, EXEMPLARY OR PUNITIVE DAMAGES ARISING OUT OF OR IN CONNECTION WITH THE HACKATHON OR THIS AGREEMENT (HOWEVER ARISING, INCLUDING NEGLIGENCE). IF YOU HAVE A DISPUTE WITH ANY PARTICIPANT OR ANY OTHER THIRD PARTY, YOU RELEASE THE UNIVERSITY, ITS, PARTNERS, LICENSORS, AND SERVICE PROVIDERS, AND EACH OF THEIR RESPECTIVE OFFICERS, DIRECTORS, AGENTS, EMPLOYEES AND REPRESENTATIVES FROM ANY AND ALL CLAIMS, DEMANDS AND DAMAGES (ACTUAL AND CONSEQUENTIAL) OF EVERY KIND AND NATURE ARISING OUT OF OR IN ANY WAY CONNECTED WITH SUCH DISPUTES. YOU AGREE THAT ANY CLAIMS AGAINST UNIVERSITY ARISING OUT OF THE HACKATHON OR THIS AGREEMENT MUST BE FILED WITHIN ONE YEAR AFTER SUCH CLAIM AROSE; OTHERWISE, YOUR CLAIM IS PERMANENTLY BARRED.","title":"Limitation of Liability"},{"location":"additional-resources/participant_agreement/#not-an-offer-or-contract-of-employment","text":"Under no circumstances will Your participation in the Codefest or anything in this Agreement be construed as an offer or contract of employment with the University.","title":"Not an Offer or Contract of Employment"},{"location":"additional-resources/participant_agreement/#additional-terms","text":"You must be at least eighteen (18) years of age to participate in the Codefest. The Codefest is subject to applicable federal, state, and local laws. The University reserves the right to permanently disqualify any person from the Codefest that it reasonably believes has violated this Agreement, the Code of Conduct, and/or the Official Rules. Any attempt to deliberately damage the Codefest or the operation thereof is unlawful and subject to legal action by the University, which may seek damages to the fullest extent permitted by law. The University assumes no responsibility for any injury or damage to Your or any other person\u2019s computer relating to or resulting from entering or downloading materials or software in connection with the Codefest. The University is not responsible for telecommunications, network, electronic, technical, or computer failures of any kind; for inaccurate transcription of entry information; for any human or electronic error; or for Solutions that are stolen, misdirected, garbled, delayed, lost, late, damaged, or returned. The University reserves the right to cancel, modify, or suspend the Codefest or any element thereof (including, without limitation, this Agreement) without notice in any manner and for any reason (including, without limitation, in the event of any unanticipated occurrence that is not fully addressed in this Agreement). The University may prohibit any person from participating in the Codefest, if such person shows a disregard for this Agreement; acts with an intent to annoy, abuse, threaten, or harass any other entrant or any agents or representatives of the University (or any associated, partners, licensors, or service providers for the University); or behaves in any other disruptive manner (as determined by the University in its sole discretion). Nothing contained in this Agreement shall be construed as an express or implied waiver by University of its governmental immunity or of the governmental immunity of the State of Colorado. Your Solutions shall not contain any item(s) that are either export-controlled under the International Traffic in Arms Regulations, or that appear on the Commerce Control List (except as EAR99) of the Export Administration Regulations.","title":"Additional Terms"},{"location":"additional-resources/participant_agreement/#dispute-resolution","text":"This Agreement and the Codefest shall be governed and construed in accordance with and governed by the laws of the state of Colorado without giving effect to conflict of law provisions.","title":"Dispute Resolution"},{"location":"additional-resources/participant_agreement/#entire-agreement","text":"This Agreement, along with the Official Rules, and the Event Code of Conduct, constitutes the entire agreement between the University and You with respect to the Codefest and supersedes all previous or contemporaneous oral or written agreements concerning the Codefest. In the event of a conflict between this Agreement, the Official Rules and/or the Event Code of Conduct, the conflict shall be resolved with the following order of precedence: This Agreement The Official Rules The Event Code of Conduct","title":"Entire Agreement"},{"location":"additional-resources/participant_agreement/#severability","text":"The invalidity, illegality, or unenforceability of any one or more phrases, sentences, clauses, or sections in this Agreement does not affect the remaining portions of this Agreement. If you have questions about the Codefest, please contact Tyler McIntosh at tyler.l.mcintosh@colorado.edu .","title":"Severability"},{"location":"additional-resources/useful_links/","text":"Useful links for the Forest Carbon Codefest GitHub Usernames Spreadsheet CyVerse User Portal FCC GitHub repo GitHub ESIIL Website CyVerse Utils Repo","title":"Useful links"},{"location":"additional-resources/useful_links/#useful-links-for-the-forest-carbon-codefest","text":"GitHub Usernames Spreadsheet CyVerse User Portal FCC GitHub repo GitHub ESIIL Website CyVerse Utils Repo","title":"Useful links for the Forest Carbon Codefest"},{"location":"collaborating-on-the-cloud/cyverse-instructions/","text":"Connecting to Cyverse and GitHub Log in to Cyverse Go to the Cyverse user account website https://user.cyverse.org/ Click Sign up (if you do not already have an account). When you make this account, please use the email that you have been using to communicate with our team regarding the event. That email is attached to our CyVerse workshop. Log in to Cyverse https://user.cyverse.org/ with your new account. From your account, go to the navigation bar at left and select 'Workshops' From the workshop page, find the workshop titled \"Forest Carbon Codefest\". It should look like this: Click on the tile, and then on the page for the workshop, click, \"Enroll\" at upper right. You should be enrolled automatically if you are using the email you have given our team. Head over to the Cyverse Discovery Environment by clicking on 'Services' at the upper right and then 'Discovery Environment' under 'My Services'. You should now see the Discovery Environment: Open up an analysis with the hackathon environment (Jupyter Lab) From the Cyverse Discovery Environment, click on Apps in the left menu Select JupyterLab ESIIL Configure and launch your analysis - the defaults are fine for now: Click Go to analysis : Now you should see Jupyter Lab! Set up your GitHub credentials If you would prefer to follow a video instead of a written outline, we have prepared a video here: From Jupyter Lab, click on the GitHub icon on the left menu: Click Clone a Repository : Paste the link to the innovation-summit-utils https://github.com/CU-ESIIL/innovation-summit-utils.git and click Clone : You should now see the innovation-summit-utils folder in your directory tree (provided you haven't changed directories from the default /home/jovyan/data-store Go into the innovation-summit-utils folder: open up the create_github_keypair.ipynb notebook by double-clicking: Select the default kernel Now you should see the notebook open. Click the play button at the top. You will be prompted to enter your GitHub username and email: You should now see your Public Key. Copy the WHOLE LINE including ssh-ed25519 at the beginning and the jovyan@... at the end Go to your GitHub settings page (you may need to log in to GitHub first): Select SSH and GPG keys Select New SSH key Give your key a descriptive name, paste your ENTIRE public key in the Key input box, and click Add SSH Key . You may need to re-authenticate with your password or two-factor authentication.: You should now see your new SSH key in your Authentication Keys list! Now you will be able to clone private repositories and push changes to GitHub from your Cyverse analysis! NOTE! Your GitHub authentication is ONLY for the analysis you're working with right now. You will be able to use it as long as you want there, but once you start a new analysis you will need to go through this process again. Feel free to delete keys from old analyses that have been shut down.","title":"Cyverse basics"},{"location":"collaborating-on-the-cloud/cyverse-instructions/#connecting-to-cyverse-and-github","text":"","title":"Connecting to Cyverse and GitHub"},{"location":"collaborating-on-the-cloud/cyverse-instructions/#log-in-to-cyverse","text":"Go to the Cyverse user account website https://user.cyverse.org/ Click Sign up (if you do not already have an account). When you make this account, please use the email that you have been using to communicate with our team regarding the event. That email is attached to our CyVerse workshop. Log in to Cyverse https://user.cyverse.org/ with your new account. From your account, go to the navigation bar at left and select 'Workshops' From the workshop page, find the workshop titled \"Forest Carbon Codefest\". It should look like this: Click on the tile, and then on the page for the workshop, click, \"Enroll\" at upper right. You should be enrolled automatically if you are using the email you have given our team. Head over to the Cyverse Discovery Environment by clicking on 'Services' at the upper right and then 'Discovery Environment' under 'My Services'. You should now see the Discovery Environment:","title":"Log in to Cyverse"},{"location":"collaborating-on-the-cloud/cyverse-instructions/#open-up-an-analysis-with-the-hackathon-environment-jupyter-lab","text":"From the Cyverse Discovery Environment, click on Apps in the left menu Select JupyterLab ESIIL Configure and launch your analysis - the defaults are fine for now: Click Go to analysis : Now you should see Jupyter Lab!","title":"Open up an analysis with the hackathon environment (Jupyter Lab)"},{"location":"collaborating-on-the-cloud/cyverse-instructions/#set-up-your-github-credentials","text":"","title":"Set up your GitHub credentials"},{"location":"collaborating-on-the-cloud/cyverse-instructions/#if-you-would-prefer-to-follow-a-video-instead-of-a-written-outline-we-have-prepared-a-video-here","text":"From Jupyter Lab, click on the GitHub icon on the left menu: Click Clone a Repository : Paste the link to the innovation-summit-utils https://github.com/CU-ESIIL/innovation-summit-utils.git and click Clone : You should now see the innovation-summit-utils folder in your directory tree (provided you haven't changed directories from the default /home/jovyan/data-store Go into the innovation-summit-utils folder: open up the create_github_keypair.ipynb notebook by double-clicking: Select the default kernel Now you should see the notebook open. Click the play button at the top. You will be prompted to enter your GitHub username and email: You should now see your Public Key. Copy the WHOLE LINE including ssh-ed25519 at the beginning and the jovyan@... at the end Go to your GitHub settings page (you may need to log in to GitHub first): Select SSH and GPG keys Select New SSH key Give your key a descriptive name, paste your ENTIRE public key in the Key input box, and click Add SSH Key . You may need to re-authenticate with your password or two-factor authentication.: You should now see your new SSH key in your Authentication Keys list! Now you will be able to clone private repositories and push changes to GitHub from your Cyverse analysis! NOTE! Your GitHub authentication is ONLY for the analysis you're working with right now. You will be able to use it as long as you want there, but once you start a new analysis you will need to go through this process again. Feel free to delete keys from old analyses that have been shut down.","title":"If you would prefer to follow a video instead of a written outline, we have prepared a video here:"},{"location":"collaborating-on-the-cloud/cyverse_data_management/","text":"Cyverse data management Cloud-to-instance data access The best and most efficient way to access most data from within your Cyverse instance is via APIs, VSI, or STAC. Examples of such data access can be found throughout the data library. This is the preferred method of data access since it keeps data on the cloud, puts it directly on your instance, and then the data is removed upon instance termination. Note that any data you want to keep must be moved off the instance and to the Cyverse data store prior to instance termination (see below, \"Saving data from your instance to the data store\"). Pre-downloaded data on Cyverse data store Some data can be time consuming or frustrating to access. Or, you or one of your teammates may just be much more comfortable working with data that has effectively been 'downloaded locally'. In an attempt to streamline your projects, the ESIIL and Earth Lab teams have loaded a set of data onto the Cyverse data store, which can be read from your Cyverse instance. Pre-downloaded data for the Forest Carbon Codefest can be found in the Cyverse data store at this link. The path directory to this location from within a Cyverse instance is: ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest Note that, while data CAN be read on your instance directly from the data store, it is usually best to move the data to your instance prior to reading and processing the data. Having the data directly on your instance will dramatically improve processing time and performance. (see below, \"Moving data from the data store to your instance\") Moving data from the data store to your instance Use the terminal command line interface on your instance to move data from the data store to your instance (whether that is pre-downloaded data or data that you have saved to your team folder). The home directory of your instance is: /home/jovyan To do so, open the Terminal from your launcher Then, use the 'cp' command to copy data from the data store to your instance. Use the flag -r if you are moving an entire directory or directory structure. The command is in the form: cp -r data-store-location new-location-on-instance For example, the below command will move the entire LCMAP_SR_1985-2021 directory to a new data directory on your instance: cp -r ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/LCMAP_SR_1985_2021 /home/jovyan/data/ Saving data from your instance to the data store Any data or outputs that you want to keep, such as newly derived datasets or figures, must be moved off the instance and to the Cyverse data store prior to instance termination. To do so, you will follow the same steps as in \"Moving data from the data store to your instance\" (see above), but with the directories in the command reversed. All team outputs should be stored in the subdirectories named TeamX in this directory: ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/Team_outputs Each team has their own directory; make sure you are saving to the correct one! For example, if you were on Team1 and wanted to save a figures directory, you could use the below command: cp -r /home/jovyan/figures ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/Team_outputs/Team1/","title":"Cyverse data management"},{"location":"collaborating-on-the-cloud/cyverse_data_management/#cyverse-data-management","text":"","title":"Cyverse data management"},{"location":"collaborating-on-the-cloud/cyverse_data_management/#cloud-to-instance-data-access","text":"The best and most efficient way to access most data from within your Cyverse instance is via APIs, VSI, or STAC. Examples of such data access can be found throughout the data library. This is the preferred method of data access since it keeps data on the cloud, puts it directly on your instance, and then the data is removed upon instance termination. Note that any data you want to keep must be moved off the instance and to the Cyverse data store prior to instance termination (see below, \"Saving data from your instance to the data store\").","title":"Cloud-to-instance data access"},{"location":"collaborating-on-the-cloud/cyverse_data_management/#pre-downloaded-data-on-cyverse-data-store","text":"Some data can be time consuming or frustrating to access. Or, you or one of your teammates may just be much more comfortable working with data that has effectively been 'downloaded locally'. In an attempt to streamline your projects, the ESIIL and Earth Lab teams have loaded a set of data onto the Cyverse data store, which can be read from your Cyverse instance. Pre-downloaded data for the Forest Carbon Codefest can be found in the Cyverse data store at this link. The path directory to this location from within a Cyverse instance is: ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest Note that, while data CAN be read on your instance directly from the data store, it is usually best to move the data to your instance prior to reading and processing the data. Having the data directly on your instance will dramatically improve processing time and performance. (see below, \"Moving data from the data store to your instance\")","title":"Pre-downloaded data on Cyverse data store"},{"location":"collaborating-on-the-cloud/cyverse_data_management/#moving-data-from-the-data-store-to-your-instance","text":"Use the terminal command line interface on your instance to move data from the data store to your instance (whether that is pre-downloaded data or data that you have saved to your team folder). The home directory of your instance is: /home/jovyan To do so, open the Terminal from your launcher Then, use the 'cp' command to copy data from the data store to your instance. Use the flag -r if you are moving an entire directory or directory structure. The command is in the form: cp -r data-store-location new-location-on-instance For example, the below command will move the entire LCMAP_SR_1985-2021 directory to a new data directory on your instance: cp -r ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/LCMAP_SR_1985_2021 /home/jovyan/data/","title":"Moving data from the data store to your instance"},{"location":"collaborating-on-the-cloud/cyverse_data_management/#saving-data-from-your-instance-to-the-data-store","text":"Any data or outputs that you want to keep, such as newly derived datasets or figures, must be moved off the instance and to the Cyverse data store prior to instance termination. To do so, you will follow the same steps as in \"Moving data from the data store to your instance\" (see above), but with the directories in the command reversed. All team outputs should be stored in the subdirectories named TeamX in this directory: ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/Team_outputs Each team has their own directory; make sure you are saving to the correct one! For example, if you were on Team1 and wanted to save a figures directory, you could use the below command: cp -r /home/jovyan/figures ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/Team_outputs/Team1/","title":"Saving data from your instance to the data store"},{"location":"collaborating-on-the-cloud/github-basics/","text":"Github essentials I. Introduction (2 minutes) A. Brief overview of GitHub: GitHub is a web-based platform that provides version control and collaboration features using Git, a distributed version control system. It enables developers to work together on projects, track changes to code, and efficiently manage different versions of the project. GitHub is widely used in the software development industry and is an essential tool for collaborative projects and maintaining code quality. B. Introduce GitHub Desktop and JupyterHub GitHub widget: GitHub Desktop is a graphical user interface (GUI) application that simplifies working with Git and GitHub by providing a more visual and intuitive way to manage repositories, branches, commits, and other Git features. JupyterHub GitHub widget, on the other hand, is a built-in widget that integrates Git and GitHub functionality directly into Jupyter notebooks, allowing users to perform version control and collaboration tasks within the Jupyter environment. Both tools help streamline the process of working with GitHub and make it more accessible to users with varying levels of experience with Git and version control. 1. Download GitHub Desktop Step 1: Download GitHub Desktop Go to the GitHub Desktop download page: https://desktop.github.com/ Click on the \u201cDownload for Windows\u201d or \u201cDownload for macOS\u201d button, depending on your operating system. The download should start automatically. Step 2: Install GitHub Desktop For Windows: Locate the downloaded installer file (usually in the Downloads folder) and double-click on it to run the installer. Follow the installation instructions that appear on the screen, accepting the default settings or customizing them as desired. Once the installation is complete, GitHub Desktop will launch automatically. For macOS: Locate the downloaded .zip file (usually in the Downloads folder) and double-click on it to extract the GitHub Desktop application. Drag the extracted \u201cGitHub Desktop\u201d application into the \u201cApplications\u201d folder. Open the \u201cApplications\u201d folder and double-click on \u201cGitHub Desktop\u201d to launch the application. Step 3: Set up GitHub Desktop When GitHub Desktop launches for the first time, you will be prompted to sign in with your GitHub account. If you don\u2019t have one, you can create one at https://github.com/join. Enter your GitHub username (or email) and password, and click on \u201cSign in.\u201d You will then be prompted to configure Git. Enter your name and email address, which will be used for your commit messages. Click \u201cContinue\u201d when you\u2019re done. Choose whether you want to submit usage data to help improve GitHub Desktop. Click \u201cFinish\u201d to complete the setup. Now, you have successfully installed and set up GitHub Desktop. You can start using it to clone repositories, make changes, commit, and sync with the remote repositories on GitHub. 1. Download GitHub for JupyterHub cloud service Step 1: Accessing JupyterHub on the cloud Visit the JupyterHub cloud service you want to use (e.g., Binder, Google Colab, or a custom JupyterHub deployment provided by your organization). Sign in with your credentials or authenticate using a third-party service if required. Step 2: Launch a new Jupyter Notebook or open an existing one Click on the \u201cNew\u201d button (usually located in the top right corner) and select \u201cPython\u201d to create a new Jupyter Notebook or open an existing one from the file browser. Once the notebook is open, you will see the Jupyter Notebook interface with the familiar cells for writing and executing code. Step 3: Install and enable the JupyterLab Git extension In your Jupyter Notebook, create a new code cell and run the following command to install the JupyterLab Git extension: !pip install jupyterlab-git Restart the Jupyter Notebook server for the changes to take effect. Step 4: Using the JupyterHub GitHub widget In the Jupyter Notebook interface, you should now see a Git icon on the left sidebar. Click on it to open the GitHub widget. To clone a repository, click on the \u201c+\u201d icon in the GitHub widget and enter the repository URL. This will clone the repository into your JupyterHub workspace. You can now navigate through the cloned repository, make changes, and use the GitHub widget to stage, commit, and push your changes back to the remote repository. To create and manage branches, use the branch icon in the GitHub widget. You can create new branches, switch between branches, and merge branches using this interface. To sync your local repository with the remote repository, use the \u201cPull\u201d and \u201cPush\u201d buttons in the GitHub widget. Now, you know how to access and use the JupyterHub GitHub widget running on the cloud. This allows you to work with Git and GitHub directly from your Jupyter Notebook interface, streamlining your workflow and making collaboration easier. C. GitHub in Rstudio: Integrating GitHub with RStudio allows users to manage their Git repositories and collaborate on projects directly within the RStudio environment. It offers similar functionality to GitHub Desktop but caters specifically to R users working within RStudio. By configuring RStudio to work with Git, creating or opening RStudio projects, and linking projects to GitHub repositories, users can enjoy a seamless workflow for version control and collaboration. RStudio\u2019s Git pane enables users to stage, commit, and push changes to remote repositories, as well as manage branches and sync local repositories with remote ones, providing a comprehensive solution for R developers working with GitHub. Step 1: Install Git Before integrating GitHub with RStudio, you need to have Git installed on your computer. Visit the official Git website (https://git-scm.com/) to download and install the latest version of Git for your operating system. Step 2: Configure RStudio to work with Git Open RStudio. Go to \u201cTools\u201d > \u201cGlobal Options\u201d in the top menu. In the \u201cGlobal Options\u201d window, click on the \u201cGit/SVN\u201d tab. Check that the \u201cGit executable\u201d field is pointing to the correct location of the installed Git. If not, click \u201cBrowse\u201d and navigate to the location of the Git executable file (usually found in the \u201cbin\u201d folder of the Git installation directory). Click \u201cOK\u201d to save the changes. Step 3: Create or open an RStudio project To create a new RStudio project, go to \u201cFile\u201d > \u201cNew Project\u201d in the top menu. You can either create a new directory or choose an existing one for your project. To open an existing RStudio project, go to \u201cFile\u201d > \u201cOpen Project\u201d and navigate to the project\u2019s \u201c.Rproj\u201d file. Step 4: Link your RStudio project to a GitHub repository In the RStudio project, go to the \u201cTools\u201d menu and select \u201cVersion Control\u201d > \u201cProject Setup.\u201d In the \u201cProject Setup\u201d window, select \u201cGit\u201d as the version control system and click \u201cOK.\u201d A new \u201c.git\u201d folder will be created in your project directory, initializing it as a Git repository. Commit any changes you have made so far by clicking on the \u201cCommit\u201d button in the \u201cGit\u201d pane in RStudio. To link your local repository to a remote GitHub repository, go to your GitHub account and create a new repository. Copy the remote repository\u2019s URL (e.g., \u201chttps://github.com/username/repository.git\u201d). In RStudio, open the \u201cShell\u201d by going to \u201cTools\u201d > \u201cShell.\u201d In the shell, run the following command to add the remote repository: git remote add origin https://github.com/username/repository.git Replace the URL with the one you copied from your GitHub repository. Push your changes to the remote repository by running the following command in the shell: git push -u origin master Now, your RStudio project is linked to a GitHub repository. You can use the \u201cGit\u201d pane in RStudio to stage, commit, and push changes to the remote repository, as well as manage branches and sync your local repository with the remote one. By integrating GitHub with RStudio, you can streamline your workflow, collaborate more effectively with your team, and manage your Git repositories directly from the RStudio interface. II. GitHub Basics (4 minutes) A. Repository: A repository, often abbreviated as \u201crepo,\u201d is the fundamental building block of GitHub. It is a storage space for your project files, including the code, documentation, and other related resources. Each repository also contains the complete history of all changes made to the project files, which is crucial for effective version control. Repositories can be public, allowing anyone to access and contribute, or private, restricting access to specific collaborators. B. Fork and Clone: Forking and cloning are two essential operations for working with repositories on GitHub. Forking creates a personal copy of someone else\u2019s repository under your GitHub account, enabling you to make changes to the project without affecting the original repo. Cloning, on the other hand, is the process of downloading a remote repository to your local machine for offline development. In GitHub Desktop, you can clone a repository by selecting \u201cClone a repository from the Internet\u201d and entering the repository URL. In JupyterHub GitHub widget, you can clone a repository by entering the repo URL in the \u201cClone Repository\u201d section of the widget. C. Branches: Branches are a critical aspect of Git version control, as they allow you to create multiple parallel versions of your project within a single repository. This is particularly useful when working on new features or bug fixes, as it prevents changes from interfering with the main (or \u201cmaster\u201d) branch until they are ready to be merged. Creating a new branch in GitHub Desktop can be done by clicking the \u201cCurrent Branch\u201d dropdown and selecting \u201cNew Branch.\u201d In JupyterHub GitHub widget, you can create a new branch by clicking the \u201cNew Branch\u201d button in the \u201cBranches\u201d section of the widget. D. Replace \u2018master\u2019 with \u2018main\u2019: In recent years, there has been a growing awareness of the importance of inclusive language in technology. One such example is the use of the term \u201cmaster\u201d in the context of the default branch in a GitHub repository. The term \u201cmaster\u201d has historical connections to the \u201cmaster/slave\u201d file structure, which evokes an unsavory colonial past associated with slavery. In light of this, many developers and organizations have begun to replace the term \u201cmaster\u201d with more neutral terms, such as \u201cmain.\u201d We encourage you to follow this practice and change the default branch name in your repositories from \u201cmaster\u201d to \u201cmain\u201d or another suitable alternative. This small change can help promote a more inclusive and welcoming environment within the technology community. III. Collaboration and Version Control (5 minutes) A. Commits: Commits are snapshots of your project\u2019s changes at a specific point in time, serving as the fundamental building blocks of Git\u2019s version control system. Commits make it possible to track changes, revert to previous versions, and collaborate with others. In GitHub Desktop, you can make a commit by staging the changes you want to include, adding a descriptive commit message, and clicking \u201cCommit to [branch_name].\u201d In JupyterHub GitHub widget, you can create a commit by selecting the files with changes, entering a commit message, and clicking the \u201cCommit\u201d button. B. Push: In GitHub, \u201cpush\u201d is a fundamental operation in the version control process that transfers commits from your local repository to a remote repository, such as the one hosted on GitHub. When you push changes, you synchronize the remote repository with the latest updates made to your local repository, making those changes accessible to other collaborators working on the same project. This operation ensures that the remote repository reflects the most recent state of your work and allows your team members to stay up to date with your changes. Pushing is an essential step in distributed version control systems like Git, as it promotes efficient collaboration among multiple contributors and provides a centralized location for tracking the project\u2019s history and progress. In GitHub, the concepts of \u201ccommit\u201d and \u201cpush\u201d represent two distinct steps in the version control process. A \u201ccommit\u201d is the action of saving changes to your local repository. When you commit changes, you create a snapshot of your work, accompanied by a unique identifier and an optional descriptive message. Commits allow you to track the progress of your work over time and make it easy to revert to a previous state if necessary. On the other hand, \u201cpush\u201d is the action of transferring your local commits to a remote repository, such as the one hosted on GitHub. Pushing makes your changes accessible to others collaborating on the same project and ensures that the remote repository stays up to date with your local repository. In summary, committing saves changes locally, while pushing synchronizes those changes with a remote repository, allowing for seamless collaboration among multiple contributors. C. Pull Requests: Pull requests are a collaboration feature on GitHub that enables developers to propose changes to a repository, discuss those changes, and ultimately merge them into the main branch. To create a pull request, you must first push your changes to a branch on your fork of the repository. Then, using either GitHub Desktop or JupyterHub GitHub widget, you can navigate to the original repository, click the \u201cPull Request\u201d tab, and create a new pull request. After the pull request is reviewed and approved, it can be merged into the main branch. D. Merging and Resolving Conflicts: Merging is the process of combining changes from one branch into another. This is typically done when a feature or bugfix has been completed and is ready to be integrated into the main branch. Conflicts can arise during the merging process if the same lines of code have been modified in both branches. To resolve conflicts, you must manually review the changes and decide which version to keep. In GitHub Desktop, you can merge branches by selecting the target branch and choosing \u201cMerge into Current Branch.\u201d Conflicts will be highlighted, and you can edit the files to resolve them before committing the changes. In JupyterHub GitHub widget, you can merge branches by selecting the target branch in the \u201cBranches\u201d section and clicking the \u201cMerge\u201d button. If conflicts occur, the widget will prompt you to resolve them before completing the merge. IV. Additional Features (2 minutes) A. Issues and Project Management: Issues are a powerful feature in GitHub that allows developers to track and manage bugs, enhancements, and other tasks within a project. Issues can be assigned to collaborators, labeled for easy organization, and linked to specific commits or pull requests. They provide a centralized location for discussing and addressing project-related concerns, fostering collaboration and transparent communication among team members. Using issues effectively can significantly improve the overall management and organization of your projects. B. GitHub Pages: GitHub Pages is a service offered by GitHub that allows you to host static websites directly from a repository. By creating a new branch named \u201cgh-pages\u201d in your repository and adding the necessary files (HTML, CSS, JavaScript, etc.), GitHub will automatically build and deploy your website to a publicly accessible URL. This is particularly useful for showcasing project documentation, creating personal portfolios, or hosting project demos. With GitHub Pages, you can take advantage of the version control and collaboration features of GitHub while easily sharing your work with others. V. Conclusion (2 minutes) A. Recap of the essentials of GitHub: In this brief introduction, we have covered the essentials of GitHub, including the basics of repositories, forking, cloning, branching, commits, pull requests, merging, and resolving conflicts. We have also discussed additional features like issues for project management and GitHub Pages for hosting websites directly from a repository. B. Encourage further exploration and learning: While this introduction provides a solid foundation for understanding and using GitHub, there is still much more to learn and explore. As you continue to use GitHub in your projects, you will discover new features and workflows that can enhance your productivity and collaboration. We encourage you to dive deeper into the platform and experiment with different tools and techniques. C. Share resources for learning more about GitHub: There are many resources available for learning more about GitHub and expanding your skills. Some popular resources include GitHub Guides (https://guides.github.com/), which offers a collection of tutorials and best practices, the official GitHub documentation (https://docs.github.com/), and various online tutorials and courses. By engaging with these resources and participating in the GitHub community, you can further develop your understanding of the platform and become a more proficient user. V. Conclusion (2 minutes) A. Recap of the essentials of GitHub: In this brief introduction, we have covered the essentials of GitHub, including the basics of repositories, forking, cloning, branching, commits, pull requests, merging, and resolving conflicts. We have also discussed additional features like issues for project management and GitHub Pages for hosting websites directly from a repository. B. Encourage further exploration and learning: While this introduction provides a solid foundation for understanding and using GitHub, there is still much more to learn and explore. As you continue to use GitHub in your projects, you will discover new features and workflows that can enhance your productivity and collaboration. We encourage you to dive deeper into the platform and experiment with different tools and techniques. C. Share resources for learning more about GitHub: There are many resources available for learning more about GitHub and expanding your skills. Some popular resources include GitHub Guides (https://guides.github.com/), which offers a collection of tutorials and best practices, the official GitHub documentation (https://docs.github.com/), and various online tutorials and courses. By engaging with these resources and participating in the GitHub community, you can further develop your understanding of the platform and become a more proficient user. By Ty Tuff, ESIIL","title":"Github basics"},{"location":"collaborating-on-the-cloud/github-basics/#github-essentials","text":"","title":"Github essentials"},{"location":"collaborating-on-the-cloud/github-basics/#i-introduction-2-minutes","text":"","title":"I. Introduction (2 minutes)"},{"location":"collaborating-on-the-cloud/github-basics/#a-brief-overview-of-github","text":"GitHub is a web-based platform that provides version control and collaboration features using Git, a distributed version control system. It enables developers to work together on projects, track changes to code, and efficiently manage different versions of the project. GitHub is widely used in the software development industry and is an essential tool for collaborative projects and maintaining code quality.","title":"A. Brief overview of GitHub:"},{"location":"collaborating-on-the-cloud/github-basics/#b-introduce-github-desktop-and-jupyterhub-github-widget","text":"GitHub Desktop is a graphical user interface (GUI) application that simplifies working with Git and GitHub by providing a more visual and intuitive way to manage repositories, branches, commits, and other Git features. JupyterHub GitHub widget, on the other hand, is a built-in widget that integrates Git and GitHub functionality directly into Jupyter notebooks, allowing users to perform version control and collaboration tasks within the Jupyter environment. Both tools help streamline the process of working with GitHub and make it more accessible to users with varying levels of experience with Git and version control.","title":"B. Introduce GitHub Desktop and JupyterHub GitHub widget:"},{"location":"collaborating-on-the-cloud/github-basics/#1-download-github-desktop","text":"","title":"1. Download GitHub Desktop"},{"location":"collaborating-on-the-cloud/github-basics/#step-1-download-github-desktop","text":"Go to the GitHub Desktop download page: https://desktop.github.com/ Click on the \u201cDownload for Windows\u201d or \u201cDownload for macOS\u201d button, depending on your operating system. The download should start automatically.","title":"Step 1: Download GitHub Desktop"},{"location":"collaborating-on-the-cloud/github-basics/#step-2-install-github-desktop","text":"For Windows: Locate the downloaded installer file (usually in the Downloads folder) and double-click on it to run the installer. Follow the installation instructions that appear on the screen, accepting the default settings or customizing them as desired. Once the installation is complete, GitHub Desktop will launch automatically. For macOS: Locate the downloaded .zip file (usually in the Downloads folder) and double-click on it to extract the GitHub Desktop application. Drag the extracted \u201cGitHub Desktop\u201d application into the \u201cApplications\u201d folder. Open the \u201cApplications\u201d folder and double-click on \u201cGitHub Desktop\u201d to launch the application.","title":"Step 2: Install GitHub Desktop"},{"location":"collaborating-on-the-cloud/github-basics/#step-3-set-up-github-desktop","text":"When GitHub Desktop launches for the first time, you will be prompted to sign in with your GitHub account. If you don\u2019t have one, you can create one at https://github.com/join. Enter your GitHub username (or email) and password, and click on \u201cSign in.\u201d You will then be prompted to configure Git. Enter your name and email address, which will be used for your commit messages. Click \u201cContinue\u201d when you\u2019re done. Choose whether you want to submit usage data to help improve GitHub Desktop. Click \u201cFinish\u201d to complete the setup. Now, you have successfully installed and set up GitHub Desktop. You can start using it to clone repositories, make changes, commit, and sync with the remote repositories on GitHub.","title":"Step 3: Set up GitHub Desktop"},{"location":"collaborating-on-the-cloud/github-basics/#1-download-github-for-jupyterhub-cloud-service","text":"","title":"1. Download GitHub for JupyterHub cloud service"},{"location":"collaborating-on-the-cloud/github-basics/#step-1-accessing-jupyterhub-on-the-cloud","text":"Visit the JupyterHub cloud service you want to use (e.g., Binder, Google Colab, or a custom JupyterHub deployment provided by your organization). Sign in with your credentials or authenticate using a third-party service if required.","title":"Step 1: Accessing JupyterHub on the cloud"},{"location":"collaborating-on-the-cloud/github-basics/#step-2-launch-a-new-jupyter-notebook-or-open-an-existing-one","text":"Click on the \u201cNew\u201d button (usually located in the top right corner) and select \u201cPython\u201d to create a new Jupyter Notebook or open an existing one from the file browser. Once the notebook is open, you will see the Jupyter Notebook interface with the familiar cells for writing and executing code.","title":"Step 2: Launch a new Jupyter Notebook or open an existing one"},{"location":"collaborating-on-the-cloud/github-basics/#step-3-install-and-enable-the-jupyterlab-git-extension","text":"In your Jupyter Notebook, create a new code cell and run the following command to install the JupyterLab Git extension: !pip install jupyterlab-git Restart the Jupyter Notebook server for the changes to take effect.","title":"Step 3: Install and enable the JupyterLab Git extension"},{"location":"collaborating-on-the-cloud/github-basics/#step-4-using-the-jupyterhub-github-widget","text":"In the Jupyter Notebook interface, you should now see a Git icon on the left sidebar. Click on it to open the GitHub widget. To clone a repository, click on the \u201c+\u201d icon in the GitHub widget and enter the repository URL. This will clone the repository into your JupyterHub workspace. You can now navigate through the cloned repository, make changes, and use the GitHub widget to stage, commit, and push your changes back to the remote repository. To create and manage branches, use the branch icon in the GitHub widget. You can create new branches, switch between branches, and merge branches using this interface. To sync your local repository with the remote repository, use the \u201cPull\u201d and \u201cPush\u201d buttons in the GitHub widget. Now, you know how to access and use the JupyterHub GitHub widget running on the cloud. This allows you to work with Git and GitHub directly from your Jupyter Notebook interface, streamlining your workflow and making collaboration easier.","title":"Step 4: Using the JupyterHub GitHub widget"},{"location":"collaborating-on-the-cloud/github-basics/#c-github-in-rstudio","text":"Integrating GitHub with RStudio allows users to manage their Git repositories and collaborate on projects directly within the RStudio environment. It offers similar functionality to GitHub Desktop but caters specifically to R users working within RStudio. By configuring RStudio to work with Git, creating or opening RStudio projects, and linking projects to GitHub repositories, users can enjoy a seamless workflow for version control and collaboration. RStudio\u2019s Git pane enables users to stage, commit, and push changes to remote repositories, as well as manage branches and sync local repositories with remote ones, providing a comprehensive solution for R developers working with GitHub.","title":"C. GitHub in Rstudio:"},{"location":"collaborating-on-the-cloud/github-basics/#step-1-install-git","text":"Before integrating GitHub with RStudio, you need to have Git installed on your computer. Visit the official Git website (https://git-scm.com/) to download and install the latest version of Git for your operating system.","title":"Step 1: Install Git"},{"location":"collaborating-on-the-cloud/github-basics/#step-2-configure-rstudio-to-work-with-git","text":"Open RStudio. Go to \u201cTools\u201d > \u201cGlobal Options\u201d in the top menu. In the \u201cGlobal Options\u201d window, click on the \u201cGit/SVN\u201d tab. Check that the \u201cGit executable\u201d field is pointing to the correct location of the installed Git. If not, click \u201cBrowse\u201d and navigate to the location of the Git executable file (usually found in the \u201cbin\u201d folder of the Git installation directory). Click \u201cOK\u201d to save the changes.","title":"Step 2: Configure RStudio to work with Git"},{"location":"collaborating-on-the-cloud/github-basics/#step-3-create-or-open-an-rstudio-project","text":"To create a new RStudio project, go to \u201cFile\u201d > \u201cNew Project\u201d in the top menu. You can either create a new directory or choose an existing one for your project. To open an existing RStudio project, go to \u201cFile\u201d > \u201cOpen Project\u201d and navigate to the project\u2019s \u201c.Rproj\u201d file.","title":"Step 3: Create or open an RStudio project"},{"location":"collaborating-on-the-cloud/github-basics/#step-4-link-your-rstudio-project-to-a-github-repository","text":"In the RStudio project, go to the \u201cTools\u201d menu and select \u201cVersion Control\u201d > \u201cProject Setup.\u201d In the \u201cProject Setup\u201d window, select \u201cGit\u201d as the version control system and click \u201cOK.\u201d A new \u201c.git\u201d folder will be created in your project directory, initializing it as a Git repository. Commit any changes you have made so far by clicking on the \u201cCommit\u201d button in the \u201cGit\u201d pane in RStudio. To link your local repository to a remote GitHub repository, go to your GitHub account and create a new repository. Copy the remote repository\u2019s URL (e.g., \u201chttps://github.com/username/repository.git\u201d). In RStudio, open the \u201cShell\u201d by going to \u201cTools\u201d > \u201cShell.\u201d In the shell, run the following command to add the remote repository: git remote add origin https://github.com/username/repository.git Replace the URL with the one you copied from your GitHub repository. Push your changes to the remote repository by running the following command in the shell: git push -u origin master Now, your RStudio project is linked to a GitHub repository. You can use the \u201cGit\u201d pane in RStudio to stage, commit, and push changes to the remote repository, as well as manage branches and sync your local repository with the remote one. By integrating GitHub with RStudio, you can streamline your workflow, collaborate more effectively with your team, and manage your Git repositories directly from the RStudio interface.","title":"Step 4: Link your RStudio project to a GitHub repository"},{"location":"collaborating-on-the-cloud/github-basics/#ii-github-basics-4-minutes","text":"","title":"II. GitHub Basics (4 minutes)"},{"location":"collaborating-on-the-cloud/github-basics/#a-repository","text":"A repository, often abbreviated as \u201crepo,\u201d is the fundamental building block of GitHub. It is a storage space for your project files, including the code, documentation, and other related resources. Each repository also contains the complete history of all changes made to the project files, which is crucial for effective version control. Repositories can be public, allowing anyone to access and contribute, or private, restricting access to specific collaborators.","title":"A. Repository:"},{"location":"collaborating-on-the-cloud/github-basics/#b-fork-and-clone","text":"Forking and cloning are two essential operations for working with repositories on GitHub. Forking creates a personal copy of someone else\u2019s repository under your GitHub account, enabling you to make changes to the project without affecting the original repo. Cloning, on the other hand, is the process of downloading a remote repository to your local machine for offline development. In GitHub Desktop, you can clone a repository by selecting \u201cClone a repository from the Internet\u201d and entering the repository URL. In JupyterHub GitHub widget, you can clone a repository by entering the repo URL in the \u201cClone Repository\u201d section of the widget.","title":"B. Fork and Clone:"},{"location":"collaborating-on-the-cloud/github-basics/#c-branches","text":"Branches are a critical aspect of Git version control, as they allow you to create multiple parallel versions of your project within a single repository. This is particularly useful when working on new features or bug fixes, as it prevents changes from interfering with the main (or \u201cmaster\u201d) branch until they are ready to be merged. Creating a new branch in GitHub Desktop can be done by clicking the \u201cCurrent Branch\u201d dropdown and selecting \u201cNew Branch.\u201d In JupyterHub GitHub widget, you can create a new branch by clicking the \u201cNew Branch\u201d button in the \u201cBranches\u201d section of the widget.","title":"C. Branches:"},{"location":"collaborating-on-the-cloud/github-basics/#d-replace-master-with-main","text":"In recent years, there has been a growing awareness of the importance of inclusive language in technology. One such example is the use of the term \u201cmaster\u201d in the context of the default branch in a GitHub repository. The term \u201cmaster\u201d has historical connections to the \u201cmaster/slave\u201d file structure, which evokes an unsavory colonial past associated with slavery. In light of this, many developers and organizations have begun to replace the term \u201cmaster\u201d with more neutral terms, such as \u201cmain.\u201d We encourage you to follow this practice and change the default branch name in your repositories from \u201cmaster\u201d to \u201cmain\u201d or another suitable alternative. This small change can help promote a more inclusive and welcoming environment within the technology community.","title":"D. Replace \u2018master\u2019 with \u2018main\u2019:"},{"location":"collaborating-on-the-cloud/github-basics/#iii-collaboration-and-version-control-5-minutes","text":"","title":"III. Collaboration and Version Control (5 minutes)"},{"location":"collaborating-on-the-cloud/github-basics/#a-commits","text":"Commits are snapshots of your project\u2019s changes at a specific point in time, serving as the fundamental building blocks of Git\u2019s version control system. Commits make it possible to track changes, revert to previous versions, and collaborate with others. In GitHub Desktop, you can make a commit by staging the changes you want to include, adding a descriptive commit message, and clicking \u201cCommit to [branch_name].\u201d In JupyterHub GitHub widget, you can create a commit by selecting the files with changes, entering a commit message, and clicking the \u201cCommit\u201d button.","title":"A. Commits:"},{"location":"collaborating-on-the-cloud/github-basics/#b-push","text":"In GitHub, \u201cpush\u201d is a fundamental operation in the version control process that transfers commits from your local repository to a remote repository, such as the one hosted on GitHub. When you push changes, you synchronize the remote repository with the latest updates made to your local repository, making those changes accessible to other collaborators working on the same project. This operation ensures that the remote repository reflects the most recent state of your work and allows your team members to stay up to date with your changes. Pushing is an essential step in distributed version control systems like Git, as it promotes efficient collaboration among multiple contributors and provides a centralized location for tracking the project\u2019s history and progress. In GitHub, the concepts of \u201ccommit\u201d and \u201cpush\u201d represent two distinct steps in the version control process. A \u201ccommit\u201d is the action of saving changes to your local repository. When you commit changes, you create a snapshot of your work, accompanied by a unique identifier and an optional descriptive message. Commits allow you to track the progress of your work over time and make it easy to revert to a previous state if necessary. On the other hand, \u201cpush\u201d is the action of transferring your local commits to a remote repository, such as the one hosted on GitHub. Pushing makes your changes accessible to others collaborating on the same project and ensures that the remote repository stays up to date with your local repository. In summary, committing saves changes locally, while pushing synchronizes those changes with a remote repository, allowing for seamless collaboration among multiple contributors.","title":"B. Push:"},{"location":"collaborating-on-the-cloud/github-basics/#c-pull-requests","text":"Pull requests are a collaboration feature on GitHub that enables developers to propose changes to a repository, discuss those changes, and ultimately merge them into the main branch. To create a pull request, you must first push your changes to a branch on your fork of the repository. Then, using either GitHub Desktop or JupyterHub GitHub widget, you can navigate to the original repository, click the \u201cPull Request\u201d tab, and create a new pull request. After the pull request is reviewed and approved, it can be merged into the main branch.","title":"C. Pull Requests:"},{"location":"collaborating-on-the-cloud/github-basics/#d-merging-and-resolving-conflicts","text":"Merging is the process of combining changes from one branch into another. This is typically done when a feature or bugfix has been completed and is ready to be integrated into the main branch. Conflicts can arise during the merging process if the same lines of code have been modified in both branches. To resolve conflicts, you must manually review the changes and decide which version to keep. In GitHub Desktop, you can merge branches by selecting the target branch and choosing \u201cMerge into Current Branch.\u201d Conflicts will be highlighted, and you can edit the files to resolve them before committing the changes. In JupyterHub GitHub widget, you can merge branches by selecting the target branch in the \u201cBranches\u201d section and clicking the \u201cMerge\u201d button. If conflicts occur, the widget will prompt you to resolve them before completing the merge.","title":"D. Merging and Resolving Conflicts:"},{"location":"collaborating-on-the-cloud/github-basics/#iv-additional-features-2-minutes","text":"","title":"IV. Additional Features (2 minutes)"},{"location":"collaborating-on-the-cloud/github-basics/#a-issues-and-project-management","text":"Issues are a powerful feature in GitHub that allows developers to track and manage bugs, enhancements, and other tasks within a project. Issues can be assigned to collaborators, labeled for easy organization, and linked to specific commits or pull requests. They provide a centralized location for discussing and addressing project-related concerns, fostering collaboration and transparent communication among team members. Using issues effectively can significantly improve the overall management and organization of your projects.","title":"A. Issues and Project Management:"},{"location":"collaborating-on-the-cloud/github-basics/#b-github-pages","text":"GitHub Pages is a service offered by GitHub that allows you to host static websites directly from a repository. By creating a new branch named \u201cgh-pages\u201d in your repository and adding the necessary files (HTML, CSS, JavaScript, etc.), GitHub will automatically build and deploy your website to a publicly accessible URL. This is particularly useful for showcasing project documentation, creating personal portfolios, or hosting project demos. With GitHub Pages, you can take advantage of the version control and collaboration features of GitHub while easily sharing your work with others.","title":"B. GitHub Pages:"},{"location":"collaborating-on-the-cloud/github-basics/#v-conclusion-2-minutes","text":"","title":"V. Conclusion (2 minutes)"},{"location":"collaborating-on-the-cloud/github-basics/#a-recap-of-the-essentials-of-github","text":"In this brief introduction, we have covered the essentials of GitHub, including the basics of repositories, forking, cloning, branching, commits, pull requests, merging, and resolving conflicts. We have also discussed additional features like issues for project management and GitHub Pages for hosting websites directly from a repository.","title":"A. Recap of the essentials of GitHub:"},{"location":"collaborating-on-the-cloud/github-basics/#b-encourage-further-exploration-and-learning","text":"While this introduction provides a solid foundation for understanding and using GitHub, there is still much more to learn and explore. As you continue to use GitHub in your projects, you will discover new features and workflows that can enhance your productivity and collaboration. We encourage you to dive deeper into the platform and experiment with different tools and techniques.","title":"B. Encourage further exploration and learning:"},{"location":"collaborating-on-the-cloud/github-basics/#c-share-resources-for-learning-more-about-github","text":"There are many resources available for learning more about GitHub and expanding your skills. Some popular resources include GitHub Guides (https://guides.github.com/), which offers a collection of tutorials and best practices, the official GitHub documentation (https://docs.github.com/), and various online tutorials and courses. By engaging with these resources and participating in the GitHub community, you can further develop your understanding of the platform and become a more proficient user.","title":"C. Share resources for learning more about GitHub:"},{"location":"collaborating-on-the-cloud/github-basics/#v-conclusion-2-minutes_1","text":"","title":"V. Conclusion (2 minutes)"},{"location":"collaborating-on-the-cloud/github-basics/#a-recap-of-the-essentials-of-github_1","text":"In this brief introduction, we have covered the essentials of GitHub, including the basics of repositories, forking, cloning, branching, commits, pull requests, merging, and resolving conflicts. We have also discussed additional features like issues for project management and GitHub Pages for hosting websites directly from a repository.","title":"A. Recap of the essentials of GitHub:"},{"location":"collaborating-on-the-cloud/github-basics/#b-encourage-further-exploration-and-learning_1","text":"While this introduction provides a solid foundation for understanding and using GitHub, there is still much more to learn and explore. As you continue to use GitHub in your projects, you will discover new features and workflows that can enhance your productivity and collaboration. We encourage you to dive deeper into the platform and experiment with different tools and techniques.","title":"B. Encourage further exploration and learning:"},{"location":"collaborating-on-the-cloud/github-basics/#c-share-resources-for-learning-more-about-github_1","text":"There are many resources available for learning more about GitHub and expanding your skills. Some popular resources include GitHub Guides (https://guides.github.com/), which offers a collection of tutorials and best practices, the official GitHub documentation (https://docs.github.com/), and various online tutorials and courses. By engaging with these resources and participating in the GitHub community, you can further develop your understanding of the platform and become a more proficient user. By Ty Tuff, ESIIL","title":"C. Share resources for learning more about GitHub:"},{"location":"collaborating-on-the-cloud/markdown_basics/","text":"Markdown for the Modern Researcher at ESIIL Introduction Overview of Markdown's relevance and utility in modern research. How Markdown streamlines documentation in diverse scientific and coding environments. Section 1: Mastering Markdown Syntax Objective: Equip researchers with a thorough understanding of Markdown syntax and its diverse applications. Topics Covered: Fundamentals of Text Formatting (headings, lists, bold, italics) Advanced Structures (tables, blockquotes) Integrating Multimedia (image and video links) Diagrams with Mermaid (creating flowcharts, mind maps, timelines) Interactive Elements (hyperlinks, embedding interactive content) Activities: Crafting a Markdown document with various formatting elements. Developing diagrams using Mermaid for research presentations. Embedding multimedia elements in a Markdown document for enhanced communication. Section 2: Markdown in Research Tools Objective: Showcase the integration of Markdown in RStudio and Jupyter Notebooks for scientific documentation. Topics Covered: Implementing Markdown in RStudio (R Markdown, knitting to HTML/PDF) Utilizing Markdown in Jupyter Notebooks (code and Markdown cells) Best practices for documenting research code Including code outputs and visualizations in documentation Activities: Creating and sharing an R Markdown document with annotated research data. Building a comprehensive Jupyter Notebook with integrated Markdown annotations. Section 3: Disseminating Research with Markdown and GitHub Pages Objective: Teach researchers how to publish and manage Markdown-based documentation as web pages. Topics Covered: Setting up a GitHub repository for hosting documentation Transforming Markdown files into web-friendly formats Customizing web page layouts and themes Advanced features using Jekyll Version control and content management for documentation Activities: Publishing a research project documentation on GitHub Pages. Applying custom themes and layouts to enhance online documentation. Conclusion Review of Markdown's role in enhancing research efficiency and clarity. Encouraging the integration of Markdown into daily research activities for improved documentation and dissemination. Additional Resources Curated list of advanced Markdown tutorials, guides for GitHub Pages, and Jekyll resources for researchers. Section 1: Mastering Markdown Syntax 1. Fundamentals of Text Formatting Headings : Use # for different levels of headings. Heading Level 1 Heading Level 2 Heading Level 3 Lists : Bulleted lists use asterisks, numbers for ordered lists. Item 1 Item 2 Subitem 2.1 Subitem 2.2 First item Second item Bold and Italics : Use asterisks or underscores. Bold Text Italic Text 2. Advanced Structures Tables : Create tables using dashes and pipes. Header 1 Header 2 Header 3 Row 1 Data Data Row 2 Data Data Add a \":\"\" to change text justification. Here the : is added on the left for left justification. | Header 1 | Header 2 | Header 3 | |---------:|--------- |----------| | Row 1 | Data | Data | | Row 2 | Data | Data | A N A L Y T I C S E N R E I N V I R O N M E N T V E L O P M O C O M U N E G A G E L L A H C N E R A T A D E V E L O P W E I T S I T N E I C S R S O I G O L O I B H T L A H T L A E W E G N E L T I T S I T N E I C S N I E E S R E H T O E N I C S L L A H C E G L A N E G A L L E H C N E I C If you hit the boundaries of Markdown's capabilities, you can start to add html directly. Remember, this entire exercisse is to translate to html. Sudoku Puzzle Fill in the blank cells with numbers from 1 to 9, such that each row, column, and 3x3 subgrid contains all the numbers from 1 to 9 without repetition. 5 3 7 6 1 9 5 9 8 6 8 6 3 4 8 3 1 7 2 6 6 2 8 4 1 9 5 8 7 9 5 3 4 6 7 8 9 1 2 6 7 2 1 9 5 3 4 8 1 9 8 3 4 2 5 6 7 8 5 9 7 6 1 4 2 3 4 2 6 8 5 3 7 9 1 7 1 3 9 2 4 8 5 6 9 6 1 5 3 7 2 8 4 2 8 7 4 1 9 6 3 5 3 4 5 2 8 6 1 7 9 Blockquotes : Use > for blockquotes. This is a blockquote. It can span multiple lines. 3. Integrating Multimedia Images : Add images using the format ![alt text](image_url) . Videos : Embed videos using HTML in Markdown. <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/dQw4w9WgXcQ\" frameborder=\"0\" allowfullscreen></iframe> 4. Diagrams with Mermaid Flowcharts : graph TD A[Start] --> B[Analyze Data] B --> C{Is Data Large?} C -->|Yes| D[Apply Big Data Solutions] C -->|No| E[Use Traditional Methods] D --> F[Machine Learning] E --> G[Statistical Analysis] F --> H{Model Accurate?} G --> I[Report Results] H -->|Yes| J[Deploy Model] H -->|No| K[Refine Model] J --> L[Monitor Performance] K --> F L --> M[End: Success] I --> N[End: Report Generated] style A fill:#f9f,stroke:#333,stroke-width:2px style M fill:#9f9,stroke:#333,stroke-width:2px style N fill:#9f9,stroke:#333,stroke-width:2px Mind Maps : mindmap root((ESIIL)) section Data Sources Satellite Imagery ::icon(fa fa-satellite) Remote Sensing Data Drones Aircraft On-ground Sensors Weather Stations IoT Devices Open Environmental Data Public Datasets ::icon(fa fa-database) section Research Focus Climate Change Analysis Ice Melt Patterns Sea Level Rise Biodiversity Monitoring Species Distribution Habitat Fragmentation Geospatial Analysis Techniques Machine Learning Models Predictive Analytics section Applications Conservation Strategies ::icon(fa fa-leaf) Urban Planning Green Spaces Disaster Response Flood Mapping Wildfire Tracking section Tools and Technologies GIS Software QGIS ArcGIS Programming Languages Python R Cloud Computing Platforms AWS Google Earth Engine Data Visualization D3.js Tableau Timelines : gantt title ESIIL Year 2 Project Schedule dateFormat YYYY-MM-DD section CI Sovereign OASIS via private jupiterhubs :2024-08-01, 2024-10-30 OASIS documentation :2024-09-15, 70d Data cube OASIS via cyverse account :2024-09-15, 100d Integrate with ESIIL User Management system :2024-08-01, 2024-11-30 Build badges to deploy DE from mkdoc :2024-09-01, 2024-12-15 Streamline Github ssh key management :2024-10-01, 2024-12-31 Cyverse support (R proxy link) :2024-11-01, 2024-12-31 Cyverse use summary and statistics :2024-08-01, 2024-12-15 section CI Consultation and Education Conferences/Invited talks :2024-08-01, 2024-12-31 Office hours :2024-08-15, 2024-12-15 Proposals :2024-09-01, 2024-11-15 Private lessons :2024-09-15, 2024-11-30 Pre-event trainings :2024-10-01, 2024-12-15 Textbook development w/ education team :2024-08-01, 2024-12-15 Train the trainers / group lessons :2024-08-15, 2024-11-30 Tribal engagement :2024-09-01, 2024-12-15 Ethical Space training :2024-09-15, 2024-12-31 section CI Design and Build Data library (repository) :2024-08-01, 2024-10-30 Analytics library (repository) :2024-08-15, 2024-11-15 Containers (repository) :2024-09-01, 2024-11-30 Cloud infrastructure templates (repository) :2024-09-15, 2024-12-15 Tribal resilience Data Cube :2024-10-01, 2024-12-31 %%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': {'rotateCommitLabel': true}} }%% gitGraph commit id: \"Start from template\" branch c1 commit id: \"Set up SSH key pair\" commit id: \"Modify _config.yml for GitHub Pages\" commit id: \"Initial website structure\" commit id: \"Add new markdown pages\" commit id: \"Update navigation tree\" commit id: \"Edit existing pages\" commit id: \"Delete old markdown pages\" commit id: \"Finalize website updates\" commit id: \"Add new markdown pages\" commit id: \"Update navigation tree\" checkout c1 branch b1 commit commit checkout c1 merge b1 %%{init: {\"quadrantChart\": {\"chartWidth\": 400, \"chartHeight\": 400}, \"themeVariables\": {\"quadrant1TextFill\": \"#ff0000\"} }}%% quadrantChart x-axis Urgent --> Not Urgent y-axis Not Important --> \"Important \u2764\" quadrant-1 Plan quadrant-2 Do quadrant-3 Delegate quadrant-4 Delete timeline title Major Events in Environmental Science and Data Science section Environmental Science 19th century : Foundations in Ecology and Conservation 1962 : Publication of 'Silent Spring' by Rachel Carson 1970 : First Earth Day 1987 : Brundtland Report introduces Sustainable Development 1992 : Rio Earth Summit 2015 : Paris Agreement on Climate Change section Data Science 1960s-1970s : Development of Database Management Systems 1980s : Emergence of Data Warehousing 1990s : Growth of the World Wide Web and Data Mining 2000s : Big Data and Predictive Analytics 2010s : AI and Machine Learning Revolution 2020s : Integration of AI in Environmental Research erDiagram CAR ||--o{ NAMED-DRIVER : allows CAR { string registrationNumber string make string model } PERSON ||--o{ NAMED-DRIVER : is PERSON { string firstName string lastName int age } --- config: sankey: showValues: false --- sankey-beta NASA Data,Big Data Harmonization,100 Satellite Imagery,Big Data Harmonization,80 Open Environmental Data,Big Data Harmonization,70 Remote Sensing Data,Big Data Harmonization,90 Big Data Harmonization, Data Analysis and Integration,340 Data Analysis and Integration,Climate Change Research,100 Data Analysis and Integration,Biodiversity Monitoring,80 Data Analysis and Integration,Geospatial Mapping,60 Data Analysis and Integration,Urban Planning,50 Data Analysis and Integration,Disaster Response,50 5. Interactive Elements Hyperlinks : Use the format [link text](URL) . Google Play Tetris Embedding Interactive Content : Use HTML tags or specific platform embed codes. <iframe src=\"https://example.com/interactive-content\" width=\"600\" height=\"400\"></iframe> 6. Math Notation Markdown can be combined with LaTeX for mathematical notation, useful in environmental data science for expressing statistical distributions, coordinate systems, and more. This requires a Markdown renderer with LaTeX support (like MathJax or KaTeX). Inline Math : Use single dollar signs for inline math expressions. Representing the normal distribution. Example: The probability density function of the normal distribution is given by $f(x|\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}$.` Display Math : Use double dollar signs for standalone equations. Example: $$ f(x|\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} $$ Common LaTeX Elements for Environmental Data Science : Statistical Distributions : Normal Distribution: \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} for $\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}$ Poisson Distribution: P(k; \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!} for $P(k; \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}$ Coordinate Systems : Spherical Coordinates: (r, \\theta, \\phi) for $(r, \\theta, \\phi)$ Cartesian Coordinates: (x, y, z) for $(x, y, z)$ Geospatial Equations : Haversine Formula for Distance: a = \\sin^2\\left(\\frac{\\Delta\\phi}{2}\\right) + \\cos(\\phi_1)\\cos(\\phi_2)\\sin^2\\left(\\frac{\\Delta\\lambda}{2}\\right) for $a = \\sin^2\\left(\\frac{\\Delta\\phi}{2}\\right) + \\cos(\\phi_1)\\cos(\\phi_2)\\sin^2\\left(\\frac{\\Delta\\lambda}{2}\\right)$ Note: The rendering of these equations as formatted math will depend on your Markdown viewer's LaTeX capabilities. 7. Effective Citations in Markdown Inline Citations Objective: Learn how to use inline citations in Markdown. Example Usage: Inline citation of a single work: Some text with an inline citation. [@jones:envstudy:2020] Inline citation with specific page or section: More text with a specific section cited. [See @jones:envstudy:2020, \u00a74.2] Contrasting views: Discussion of a topic with a contrasting view. [Contra @smith:climatechange:2019, p. 78] Footnote Citations Objective: Understand how to use footnote citations in Markdown. Example Usage: Citing with a footnote: Some statement in the text.[^1] Multiple references to the same footnote: Another statement referring to the same source.[^1] A different citation: Additional comment with a new citation.[^2] Creating Footnotes Example Syntax: [^1]: First reference details. Example: Emma Jones, \"Environmental Study,\" Nature Journal, May 2020, https://nature-journal.com/envstudy2020. [^2]: Second reference details. Example: David Smith, \"Climate Change Controversies,\" Science Daily, August 2019, https://sciencedaily.com/climatechange2019.","title":"Markdown basics"},{"location":"collaborating-on-the-cloud/markdown_basics/#markdown-for-the-modern-researcher-at-esiil","text":"","title":"Markdown for the Modern Researcher at ESIIL"},{"location":"collaborating-on-the-cloud/markdown_basics/#introduction","text":"Overview of Markdown's relevance and utility in modern research. How Markdown streamlines documentation in diverse scientific and coding environments.","title":"Introduction"},{"location":"collaborating-on-the-cloud/markdown_basics/#section-1-mastering-markdown-syntax","text":"Objective: Equip researchers with a thorough understanding of Markdown syntax and its diverse applications. Topics Covered: Fundamentals of Text Formatting (headings, lists, bold, italics) Advanced Structures (tables, blockquotes) Integrating Multimedia (image and video links) Diagrams with Mermaid (creating flowcharts, mind maps, timelines) Interactive Elements (hyperlinks, embedding interactive content) Activities: Crafting a Markdown document with various formatting elements. Developing diagrams using Mermaid for research presentations. Embedding multimedia elements in a Markdown document for enhanced communication.","title":"Section 1: Mastering Markdown Syntax"},{"location":"collaborating-on-the-cloud/markdown_basics/#section-2-markdown-in-research-tools","text":"Objective: Showcase the integration of Markdown in RStudio and Jupyter Notebooks for scientific documentation. Topics Covered: Implementing Markdown in RStudio (R Markdown, knitting to HTML/PDF) Utilizing Markdown in Jupyter Notebooks (code and Markdown cells) Best practices for documenting research code Including code outputs and visualizations in documentation Activities: Creating and sharing an R Markdown document with annotated research data. Building a comprehensive Jupyter Notebook with integrated Markdown annotations.","title":"Section 2: Markdown in Research Tools"},{"location":"collaborating-on-the-cloud/markdown_basics/#section-3-disseminating-research-with-markdown-and-github-pages","text":"Objective: Teach researchers how to publish and manage Markdown-based documentation as web pages. Topics Covered: Setting up a GitHub repository for hosting documentation Transforming Markdown files into web-friendly formats Customizing web page layouts and themes Advanced features using Jekyll Version control and content management for documentation Activities: Publishing a research project documentation on GitHub Pages. Applying custom themes and layouts to enhance online documentation.","title":"Section 3: Disseminating Research with Markdown and GitHub Pages"},{"location":"collaborating-on-the-cloud/markdown_basics/#conclusion","text":"Review of Markdown's role in enhancing research efficiency and clarity. Encouraging the integration of Markdown into daily research activities for improved documentation and dissemination.","title":"Conclusion"},{"location":"collaborating-on-the-cloud/markdown_basics/#additional-resources","text":"Curated list of advanced Markdown tutorials, guides for GitHub Pages, and Jekyll resources for researchers.","title":"Additional Resources"},{"location":"collaborating-on-the-cloud/markdown_basics/#section-1-mastering-markdown-syntax_1","text":"","title":"Section 1: Mastering Markdown Syntax"},{"location":"collaborating-on-the-cloud/markdown_basics/#1-fundamentals-of-text-formatting","text":"Headings : Use # for different levels of headings.","title":"1. Fundamentals of Text Formatting"},{"location":"collaborating-on-the-cloud/markdown_basics/#heading-level-1","text":"","title":"Heading Level 1"},{"location":"collaborating-on-the-cloud/markdown_basics/#heading-level-2","text":"","title":"Heading Level 2"},{"location":"collaborating-on-the-cloud/markdown_basics/#heading-level-3","text":"Lists : Bulleted lists use asterisks, numbers for ordered lists. Item 1 Item 2 Subitem 2.1 Subitem 2.2 First item Second item Bold and Italics : Use asterisks or underscores. Bold Text Italic Text","title":"Heading Level 3"},{"location":"collaborating-on-the-cloud/markdown_basics/#2-advanced-structures","text":"Tables : Create tables using dashes and pipes. Header 1 Header 2 Header 3 Row 1 Data Data Row 2 Data Data Add a \":\"\" to change text justification. Here the : is added on the left for left justification. | Header 1 | Header 2 | Header 3 | |---------:|--------- |----------| | Row 1 | Data | Data | | Row 2 | Data | Data | A N A L Y T I C S E N R E I N V I R O N M E N T V E L O P M O C O M U N E G A G E L L A H C N E R A T A D E V E L O P W E I T S I T N E I C S R S O I G O L O I B H T L A H T L A E W E G N E L T I T S I T N E I C S N I E E S R E H T O E N I C S L L A H C E G L A N E G A L L E H C N E I C If you hit the boundaries of Markdown's capabilities, you can start to add html directly. Remember, this entire exercisse is to translate to html. Sudoku Puzzle Fill in the blank cells with numbers from 1 to 9, such that each row, column, and 3x3 subgrid contains all the numbers from 1 to 9 without repetition. 5 3 7 6 1 9 5 9 8 6 8 6 3 4 8 3 1 7 2 6 6 2 8 4 1 9 5 8 7 9 5 3 4 6 7 8 9 1 2 6 7 2 1 9 5 3 4 8 1 9 8 3 4 2 5 6 7 8 5 9 7 6 1 4 2 3 4 2 6 8 5 3 7 9 1 7 1 3 9 2 4 8 5 6 9 6 1 5 3 7 2 8 4 2 8 7 4 1 9 6 3 5 3 4 5 2 8 6 1 7 9 Blockquotes : Use > for blockquotes. This is a blockquote. It can span multiple lines.","title":"2. Advanced Structures"},{"location":"collaborating-on-the-cloud/markdown_basics/#3-integrating-multimedia","text":"Images : Add images using the format ![alt text](image_url) . Videos : Embed videos using HTML in Markdown. <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/dQw4w9WgXcQ\" frameborder=\"0\" allowfullscreen></iframe>","title":"3. Integrating Multimedia"},{"location":"collaborating-on-the-cloud/markdown_basics/#4-diagrams-with-mermaid","text":"Flowcharts : graph TD A[Start] --> B[Analyze Data] B --> C{Is Data Large?} C -->|Yes| D[Apply Big Data Solutions] C -->|No| E[Use Traditional Methods] D --> F[Machine Learning] E --> G[Statistical Analysis] F --> H{Model Accurate?} G --> I[Report Results] H -->|Yes| J[Deploy Model] H -->|No| K[Refine Model] J --> L[Monitor Performance] K --> F L --> M[End: Success] I --> N[End: Report Generated] style A fill:#f9f,stroke:#333,stroke-width:2px style M fill:#9f9,stroke:#333,stroke-width:2px style N fill:#9f9,stroke:#333,stroke-width:2px Mind Maps : mindmap root((ESIIL)) section Data Sources Satellite Imagery ::icon(fa fa-satellite) Remote Sensing Data Drones Aircraft On-ground Sensors Weather Stations IoT Devices Open Environmental Data Public Datasets ::icon(fa fa-database) section Research Focus Climate Change Analysis Ice Melt Patterns Sea Level Rise Biodiversity Monitoring Species Distribution Habitat Fragmentation Geospatial Analysis Techniques Machine Learning Models Predictive Analytics section Applications Conservation Strategies ::icon(fa fa-leaf) Urban Planning Green Spaces Disaster Response Flood Mapping Wildfire Tracking section Tools and Technologies GIS Software QGIS ArcGIS Programming Languages Python R Cloud Computing Platforms AWS Google Earth Engine Data Visualization D3.js Tableau Timelines : gantt title ESIIL Year 2 Project Schedule dateFormat YYYY-MM-DD section CI Sovereign OASIS via private jupiterhubs :2024-08-01, 2024-10-30 OASIS documentation :2024-09-15, 70d Data cube OASIS via cyverse account :2024-09-15, 100d Integrate with ESIIL User Management system :2024-08-01, 2024-11-30 Build badges to deploy DE from mkdoc :2024-09-01, 2024-12-15 Streamline Github ssh key management :2024-10-01, 2024-12-31 Cyverse support (R proxy link) :2024-11-01, 2024-12-31 Cyverse use summary and statistics :2024-08-01, 2024-12-15 section CI Consultation and Education Conferences/Invited talks :2024-08-01, 2024-12-31 Office hours :2024-08-15, 2024-12-15 Proposals :2024-09-01, 2024-11-15 Private lessons :2024-09-15, 2024-11-30 Pre-event trainings :2024-10-01, 2024-12-15 Textbook development w/ education team :2024-08-01, 2024-12-15 Train the trainers / group lessons :2024-08-15, 2024-11-30 Tribal engagement :2024-09-01, 2024-12-15 Ethical Space training :2024-09-15, 2024-12-31 section CI Design and Build Data library (repository) :2024-08-01, 2024-10-30 Analytics library (repository) :2024-08-15, 2024-11-15 Containers (repository) :2024-09-01, 2024-11-30 Cloud infrastructure templates (repository) :2024-09-15, 2024-12-15 Tribal resilience Data Cube :2024-10-01, 2024-12-31 %%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': {'rotateCommitLabel': true}} }%% gitGraph commit id: \"Start from template\" branch c1 commit id: \"Set up SSH key pair\" commit id: \"Modify _config.yml for GitHub Pages\" commit id: \"Initial website structure\" commit id: \"Add new markdown pages\" commit id: \"Update navigation tree\" commit id: \"Edit existing pages\" commit id: \"Delete old markdown pages\" commit id: \"Finalize website updates\" commit id: \"Add new markdown pages\" commit id: \"Update navigation tree\" checkout c1 branch b1 commit commit checkout c1 merge b1 %%{init: {\"quadrantChart\": {\"chartWidth\": 400, \"chartHeight\": 400}, \"themeVariables\": {\"quadrant1TextFill\": \"#ff0000\"} }}%% quadrantChart x-axis Urgent --> Not Urgent y-axis Not Important --> \"Important \u2764\" quadrant-1 Plan quadrant-2 Do quadrant-3 Delegate quadrant-4 Delete timeline title Major Events in Environmental Science and Data Science section Environmental Science 19th century : Foundations in Ecology and Conservation 1962 : Publication of 'Silent Spring' by Rachel Carson 1970 : First Earth Day 1987 : Brundtland Report introduces Sustainable Development 1992 : Rio Earth Summit 2015 : Paris Agreement on Climate Change section Data Science 1960s-1970s : Development of Database Management Systems 1980s : Emergence of Data Warehousing 1990s : Growth of the World Wide Web and Data Mining 2000s : Big Data and Predictive Analytics 2010s : AI and Machine Learning Revolution 2020s : Integration of AI in Environmental Research erDiagram CAR ||--o{ NAMED-DRIVER : allows CAR { string registrationNumber string make string model } PERSON ||--o{ NAMED-DRIVER : is PERSON { string firstName string lastName int age } --- config: sankey: showValues: false --- sankey-beta NASA Data,Big Data Harmonization,100 Satellite Imagery,Big Data Harmonization,80 Open Environmental Data,Big Data Harmonization,70 Remote Sensing Data,Big Data Harmonization,90 Big Data Harmonization, Data Analysis and Integration,340 Data Analysis and Integration,Climate Change Research,100 Data Analysis and Integration,Biodiversity Monitoring,80 Data Analysis and Integration,Geospatial Mapping,60 Data Analysis and Integration,Urban Planning,50 Data Analysis and Integration,Disaster Response,50","title":"4. Diagrams with Mermaid"},{"location":"collaborating-on-the-cloud/markdown_basics/#5-interactive-elements","text":"Hyperlinks : Use the format [link text](URL) . Google Play Tetris Embedding Interactive Content : Use HTML tags or specific platform embed codes. <iframe src=\"https://example.com/interactive-content\" width=\"600\" height=\"400\"></iframe>","title":"5. Interactive Elements"},{"location":"collaborating-on-the-cloud/markdown_basics/#6-math-notation","text":"Markdown can be combined with LaTeX for mathematical notation, useful in environmental data science for expressing statistical distributions, coordinate systems, and more. This requires a Markdown renderer with LaTeX support (like MathJax or KaTeX). Inline Math : Use single dollar signs for inline math expressions. Representing the normal distribution. Example: The probability density function of the normal distribution is given by $f(x|\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}$.` Display Math : Use double dollar signs for standalone equations. Example: $$ f(x|\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} $$ Common LaTeX Elements for Environmental Data Science : Statistical Distributions : Normal Distribution: \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} for $\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}$ Poisson Distribution: P(k; \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!} for $P(k; \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}$ Coordinate Systems : Spherical Coordinates: (r, \\theta, \\phi) for $(r, \\theta, \\phi)$ Cartesian Coordinates: (x, y, z) for $(x, y, z)$ Geospatial Equations : Haversine Formula for Distance: a = \\sin^2\\left(\\frac{\\Delta\\phi}{2}\\right) + \\cos(\\phi_1)\\cos(\\phi_2)\\sin^2\\left(\\frac{\\Delta\\lambda}{2}\\right) for $a = \\sin^2\\left(\\frac{\\Delta\\phi}{2}\\right) + \\cos(\\phi_1)\\cos(\\phi_2)\\sin^2\\left(\\frac{\\Delta\\lambda}{2}\\right)$ Note: The rendering of these equations as formatted math will depend on your Markdown viewer's LaTeX capabilities.","title":"6. Math Notation"},{"location":"collaborating-on-the-cloud/markdown_basics/#7-effective-citations-in-markdown","text":"","title":"7. Effective Citations in Markdown"},{"location":"collaborating-on-the-cloud/markdown_basics/#inline-citations","text":"Objective: Learn how to use inline citations in Markdown. Example Usage: Inline citation of a single work: Some text with an inline citation. [@jones:envstudy:2020] Inline citation with specific page or section: More text with a specific section cited. [See @jones:envstudy:2020, \u00a74.2] Contrasting views: Discussion of a topic with a contrasting view. [Contra @smith:climatechange:2019, p. 78]","title":"Inline Citations"},{"location":"collaborating-on-the-cloud/markdown_basics/#footnote-citations","text":"Objective: Understand how to use footnote citations in Markdown. Example Usage: Citing with a footnote: Some statement in the text.[^1] Multiple references to the same footnote: Another statement referring to the same source.[^1] A different citation: Additional comment with a new citation.[^2]","title":"Footnote Citations"},{"location":"collaborating-on-the-cloud/markdown_basics/#creating-footnotes","text":"Example Syntax: [^1]: First reference details. Example: Emma Jones, \"Environmental Study,\" Nature Journal, May 2020, https://nature-journal.com/envstudy2020. [^2]: Second reference details. Example: David Smith, \"Climate Change Controversies,\" Science Daily, August 2019, https://sciencedaily.com/climatechange2019.","title":"Creating Footnotes"},{"location":"data-library/Pull_Sentinal2_l2_data/","text":"Pulling Sentinal 2 data Ty Tuff, ESIIL Data Scientist 2023-10-27 Set Java Options # Run these Java options before anything else. options(java.parameters = \"-Xmx64G\") options(timeout = max(600, getOption(\"timeout\"))) R libraries and global setting. #library(Rcpp) library(sf) library(gdalcubes) library(rstac) library(gdalUtils) library(terra) library(rgdal) library(reshape2) library(osmdata) library(terra) library(dplyr) #library(glue) library(stars) library(ggplot2) library(colorspace) library(geos) #library(glue) library(osmdata) library(ggthemes) library(tidyr) gdalcubes_options(parallel = 8) sf::sf_extSoftVersion() GEOS GDAL proj.4 GDAL_with_GEOS USE_PROJ_H \"3.11.0\" \"3.5.3\" \"9.1.0\" \"true\" \"true\" PROJ \"9.1.0\" gdalcubes_gdal_has_geos() [1] TRUE Start timer start <- Sys.time() Set color palette library(ggtern) our_yellow <- rgb2hex(r = 253, g = 201, b = 51) our_green <- rgb2hex(r = 10, g = 84, b = 62) our_grey <- rgb2hex(r = 92, g = 96, b = 95) our_white <- rgb2hex(r = 255, g = 255, b = 255) Load area of interest # Read the shapefile into an sf object aoi_total <- st_read(\"/Users/ty/Documents/Github/Southern_California_Edison_Fire_Risk/SCE_Fire_Zone_V2/SCE_Fire_Zone_V2.shp\") %>% st_as_sf() Reading layer `SCE_Fire_Zone_V2' from data source `/Users/ty/Documents/Github/Southern_California_Edison_Fire_Risk/SCE_Fire_Zone_V2/SCE_Fire_Zone_V2.shp' using driver `ESRI Shapefile' Simple feature collection with 12 features and 5 fields Geometry type: POLYGON Dimension: XY Bounding box: xmin: 176062.4 ymin: 3674043 xmax: 764123.1 ymax: 4254012 Projected CRS: NAD83 / UTM zone 11N # Plot the entire spatial dataset plot(aoi_total) # Filter the dataset to obtain the geometry with OBJECTID 5 aoi <- aoi_total %>% filter(OBJECTID == 5) # Obtain and plot the bounding box of the filtered geometry shape_bbox <- st_bbox(aoi) plot(aoi) # Transform the filtered geometry to EPSG:4326 and store its bounding box aoi %>% st_transform(\"EPSG:4326\") %>% st_bbox() -> bbox_4326 # Transform the filtered geometry to EPSG:32618 and store its bounding box aoi %>% st_transform(\"EPSG:32618\") %>% st_bbox() -> bbox_32618 Arrange STAC collection In this code chunk, the primary goal is to search for and obtain satellite imagery data. The data source being tapped into is a SpatioTemporal Asset Catalog (STAC) provided by an online service (earth-search by Element84). Here\u2019s a breakdown: A connection is established with the STAC service, searching specifically within the \u201csentinel-s2-l2a-cogs\u201d collection. -The search is spatially constrained to a bounding box (bbox_4326) and temporally limited to a range of one day, between May 15 and May 16, 2021. -Once the search is conducted, the desired assets or spectral bands from the returned satellite images are defined, ranging from Band 1 (B01) to Band 12 (B12) and including the Scene Classification Layer (SCL). -These bands are then organized into an image collection for further processing or analysis. # Initialize STAC connection s = stac(\"https://earth-search.aws.element84.com/v0\") # Search for Sentinel-2 images within specified bounding box and date range items = s %>% stac_search(collections = \"sentinel-s2-l2a-cogs\", bbox = c(bbox_4326[\"xmin\"], bbox_4326[\"ymin\"], bbox_4326[\"xmax\"], bbox_4326[\"ymax\"]), datetime = \"2021-05-15/2021-05-16\") %>% post_request() %>% items_fetch(progress = FALSE) # Print number of found items length(items$features) [1] 12 # Prepare the assets for analysis library(gdalcubes) assets = c(\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\", \"SCL\") s2_collection = stac_image_collection(items$features, asset_names = assets) # Display the image collection s2_collection Image collection object, referencing 12 images with 13 bands Images: name left top bottom right 1 S2B_11SNS_20210515_1_L2A -117.0002 33.43957 32.44372 -115.8191 2 S2B_11SPS_20210515_1_L2A -115.9361 33.43490 32.42937 -114.7436 3 S2B_11SQS_20210515_0_L2A -114.8732 33.42092 32.41918 -113.9566 4 S2B_12STB_20210515_0_L2A -114.2244 33.40433 32.61015 -113.9559 5 S2B_11SNT_20210515_0_L2A -117.0002 34.34164 33.34577 -115.8066 6 S2B_11SPT_20210515_0_L2A -115.9253 34.33683 33.33091 -114.7198 datetime srs 1 2021-05-15T18:35:13 EPSG:32611 2 2021-05-15T18:35:10 EPSG:32611 3 2021-05-15T18:35:06 EPSG:32611 4 2021-05-15T18:35:01 EPSG:32612 5 2021-05-15T18:34:59 EPSG:32611 6 2021-05-15T18:34:55 EPSG:32611 [ omitted 6 images ] Bands: name offset scale unit nodata image_count 1 B01 0 1 12 2 B02 0 1 12 3 B03 0 1 12 4 B04 0 1 12 5 B05 0 1 12 6 B06 0 1 12 7 B07 0 1 12 8 B08 0 1 12 9 B09 0 1 12 10 B11 0 1 12 11 B12 0 1 12 12 B8A 0 1 12 13 SCL 0 1 12 Define view window In this code chunk, a \u2018view\u2019 on the previously obtained satellite image collection is being defined. Think of this as setting up a specific lens or perspective to look at the satellite data: -The view is set to the coordinate reference system EPSG:32618. -Spatial resolution is defined as 100x100 meters. -Temporal resolution is defined monthly (P1M), even though the actual range is only one day. -When there are multiple values in a grid cell or timeframe, they are aggregated using the median value. -If any resampling is needed, the nearest neighbor method is used (near). -The spatial and temporal extents are constrained to specific values. -By defining this view, it allows for consistent analysis and visualization of the image collection within the specified spatial and temporal resolutions and extents. # Define a specific view on the satellite image collection v = cube_view( srs = \"EPSG:32618\", dx = 100, dy = 100, dt = \"P1M\", aggregation = \"median\", resampling = \"near\", extent = list( t0 = \"2021-05-15\", t1 = \"2021-05-16\", left = bbox_32618[\"xmin\"], right = bbox_32618[\"xmax\"], top = bbox_32618[\"ymax\"], bottom = bbox_32618[\"ymin\"] ) ) # Display the defined view v A data cube view object Dimensions: low high count pixel_size t 2021-05-01 2021-05-31 1 P1M y 4471226.41402451 4741326.41402451 2701 100 x -3463720.00044994 -3191420.00044994 2723 100 SRS: \"EPSG:32618\" Temporal aggregation method: \"median\" Spatial resampling method: \"near\" Pull data In this chunk, the primary aim is to transform and prepare satellite imagery data for analysis: -The current time is stored in variable a for tracking the time taken by the process. -The previously defined \u2018view\u2019 on the satellite imagery, v, is used to create a raster cube, a multi-dimensional array containing the satellite data. This raster cube contains spatial, spectral, and temporal data. -The desired spectral bands are selected. -The data is limited to a specific area of interest, aoi. -The band names are renamed to their respective wavelengths in nanometers for clarity. -A subset of the data, comprising 50,000 random samples, is selected. -Unwanted columns are removed, and the dataset is transformed into a long format, where each row represents a particular date and wavelength combination. -The entire process duration is computed by taking the difference between the end time (b) and the start time (a). -The transformed dataset y is then displayed. # Record start time a <- Sys.time() # Transform the satellite image collection into a raster cube x <- s2_collection %>% raster_cube(v) %>% select_bands(c(\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\")) %>% extract_geom(aoi) %>% rename( \"time\" = \"time\", \"443\" = \"B01\", \"490\" = \"B02\", \"560\" = \"B03\", \"665\" = \"B04\", \"705\" = \"B05\", \"740\" = \"B06\", \"783\" = \"B07\", \"842\" = \"B08\", \"865\" = \"B8A\", \"940\" = \"B09\", \"1610\" = \"B11\", \"2190\" = \"B12\" ) # Sample, transform and prepare data for analysis y <- x %>% slice_sample(n = 50000) %>% select(-FID) %>% pivot_longer(!time, names_to = \"wavelength_nm\", values_to = \"reflectance\") %>% mutate(wavelength_nm = as.numeric(wavelength_nm)) # Record end time and compute duration b <- Sys.time() processing_time <- difftime(b, a) # Display the processing time and transformed dataset processing_time Time difference of 1.23593 mins y # A tibble: 600,000 \u00d7 3 time wavelength_nm reflectance <chr> <dbl> <dbl> 1 2021-05-01 443 1855 2 2021-05-01 490 2255 3 2021-05-01 560 2884 4 2021-05-01 665 3711 5 2021-05-01 705 3990 6 2021-05-01 740 4009 7 2021-05-01 783 4078 8 2021-05-01 842 4219 9 2021-05-01 865 4060 10 2021-05-01 940 4120 # \u2139 599,990 more rows Base plot # Set custom colors for the plot our_green <- \"#4CAF50\" our_white <- \"#FFFFFF\" our_yellow <- \"#FFEB3B\" # Create a 2D density plot day_density <- ggplot(data = y, aes(x = wavelength_nm, y = reflectance, group = time)) + stat_smooth(color = our_green, fill = \"lightgrey\") + geom_density2d(colour = \"black\", bins = 10, alpha = 0.1) + stat_density2d(aes(alpha = ..level.., fill = ..level..), linewidth = 2, bins = 10, geom = \"polygon\") + scale_fill_gradient(low = our_white, high = our_yellow) + scale_alpha(range = c(0.00, 0.8), guide = FALSE) + theme_tufte() + xlab(\"wavelength\") + ylab(\"reflectance\") + ylim(0, 16000) + theme( aspect.ratio = 5/14, axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, colour = c(\"darkblue\", \"blue\", \"green\", \"red\", \"darkred\", \"darkred\", \"darkred\", \"darkred\", \"darkred\", \"black\", \"black\", \"black\", \"black\")), axis.title.x = element_blank(), axis.title.y = element_blank(), plot.margin = margin(t = 30, r = 10, b = 40, l = 18) ) + scale_x_continuous(breaks = c(443, 490, 560, 665, 705, 740, 783, 842, 865, 940, 1610, 2190)) # Display the plot day_density Inlay 1 - geographic zone guide_map <- ggplot(data= aoi_total) + geom_sf(fill=our_yellow, color=our_white) + geom_sf(data= aoi, fill=our_green, color=our_white) + theme_tufte()+ ggtitle(\"Zone 5\")+ theme(axis.text.x=element_blank(), #remove x axis labels axis.ticks.x=element_blank(), #remove x axis ticks axis.text.y=element_blank(), #remove y axis labels axis.ticks.y=element_blank() #remove y axis ticks, bg=none )+ theme(plot.title = element_text(hjust=0.8, vjust = -2)) guide_map Inlay 2 - date text library(geosphere) aoi_total |> st_centroid() |> st_transform(crs=\"+proj=longlat\") |> st_coordinates() |> colMeans() -> lat_long daylength_line <- daylength(lat = lat_long[2], 1:365) daylengths <- data.frame(time= 1:365, daylength = daylength_line) library(lubridate) # Create a template date object date <- as.POSIXlt(\"2021-05-15\") doy <- format(date, format = \"%j\") |> as.numeric() display_date <- format(date, format=\"%e %B %Y \") Inlay 3 - daylength date_inlay <- ggplot(data=daylengths) + ggtitle(\"Daylength\")+ geom_ribbon(aes(x=time, ymin=daylength, ymax=15), fill=our_grey, alpha=0.5) + geom_ribbon(aes(x=time, ymax=daylength, ymin=9), fill=our_yellow, alpha=1) + geom_hline(yintercept=12, color=our_white) + geom_vline(xintercept=doy, color=our_green, size=1) + theme_tufte() + ylim(9,15) + theme(axis.text.y=element_blank(), axis.ticks.y=element_blank(), axis.title.y=element_blank(), axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank()) + theme(plot.title = element_text(hjust=0.5, vjust = 0)) date_inlay Ensemble map assembly library(cowplot) library(magick) map_overlay <- ggdraw(day_density) + draw_plot(guide_map, x = 1.08, y = 1, hjust = 1, vjust = 1, width = 0.3, height = 0.3)+ draw_plot(date_inlay, x = 1, y = 0.35, hjust = 1, vjust = 1, width = 0.1, height = 0.25)+ geom_text(aes(x=1, y=0.08, label=display_date, hjust = 1), color=our_grey, cex=3, fontface='bold') + # draw_image(\"Ty_powerline_plots/Southern_California_Edison_Logo.png\", x = -0.24, y = 0.38, scale=.3)+ # draw_image(\"Ty_powerline_plots/earthlab_logo.png\", x = -0.38, y = 0.38, scale=.25)+ geom_text(aes(x=0.4, y=.9, label=\"Spectral library - Monthly average\"), color=our_green, hjust = 0, cex=8, fontface='bold') + geom_text(aes(x=0.01, y=.04, label=\"Created by ESIIL (T. Tuff) for Fall Hackathon -- October 2023. Sentinel 2 Data from 'https://earth-search.aws.element84.com/v0'\"), color=our_grey, hjust = 0, cex=3) + geom_text(aes(x=0.4, y=.1, label=\"wavelength (nm)\"), color=our_grey, hjust = 0, cex=4, fontface='bold') + geom_text(aes(x=0.01, y=.5,angle = 90, label=\"reflectance\"), color=our_grey, hjust = 0, cex=4, fontface='bold') map_overlay Save map ggsave(map_overlay, file=\"day_density_15_May_2021_zone_5.png\", bg=\"white\", dpi = 600, width = 12, height = 5) End timer end <- Sys.time() difftime(end,start) Time difference of 3.2202 mins","title":"TUTORIAL Sentinal 2 STAC example"},{"location":"data-library/Pull_Sentinal2_l2_data/#pulling-sentinal-2-data","text":"Ty Tuff, ESIIL Data Scientist 2023-10-27","title":"Pulling Sentinal 2 data"},{"location":"data-library/Pull_Sentinal2_l2_data/#set-java-options","text":"# Run these Java options before anything else. options(java.parameters = \"-Xmx64G\") options(timeout = max(600, getOption(\"timeout\")))","title":"Set Java Options"},{"location":"data-library/Pull_Sentinal2_l2_data/#r-libraries-and-global-setting","text":"#library(Rcpp) library(sf) library(gdalcubes) library(rstac) library(gdalUtils) library(terra) library(rgdal) library(reshape2) library(osmdata) library(terra) library(dplyr) #library(glue) library(stars) library(ggplot2) library(colorspace) library(geos) #library(glue) library(osmdata) library(ggthemes) library(tidyr) gdalcubes_options(parallel = 8) sf::sf_extSoftVersion() GEOS GDAL proj.4 GDAL_with_GEOS USE_PROJ_H \"3.11.0\" \"3.5.3\" \"9.1.0\" \"true\" \"true\" PROJ \"9.1.0\" gdalcubes_gdal_has_geos() [1] TRUE","title":"R libraries and global setting."},{"location":"data-library/Pull_Sentinal2_l2_data/#start-timer","text":"start <- Sys.time()","title":"Start timer"},{"location":"data-library/Pull_Sentinal2_l2_data/#set-color-palette","text":"library(ggtern) our_yellow <- rgb2hex(r = 253, g = 201, b = 51) our_green <- rgb2hex(r = 10, g = 84, b = 62) our_grey <- rgb2hex(r = 92, g = 96, b = 95) our_white <- rgb2hex(r = 255, g = 255, b = 255)","title":"Set color palette"},{"location":"data-library/Pull_Sentinal2_l2_data/#load-area-of-interest","text":"# Read the shapefile into an sf object aoi_total <- st_read(\"/Users/ty/Documents/Github/Southern_California_Edison_Fire_Risk/SCE_Fire_Zone_V2/SCE_Fire_Zone_V2.shp\") %>% st_as_sf() Reading layer `SCE_Fire_Zone_V2' from data source `/Users/ty/Documents/Github/Southern_California_Edison_Fire_Risk/SCE_Fire_Zone_V2/SCE_Fire_Zone_V2.shp' using driver `ESRI Shapefile' Simple feature collection with 12 features and 5 fields Geometry type: POLYGON Dimension: XY Bounding box: xmin: 176062.4 ymin: 3674043 xmax: 764123.1 ymax: 4254012 Projected CRS: NAD83 / UTM zone 11N # Plot the entire spatial dataset plot(aoi_total) # Filter the dataset to obtain the geometry with OBJECTID 5 aoi <- aoi_total %>% filter(OBJECTID == 5) # Obtain and plot the bounding box of the filtered geometry shape_bbox <- st_bbox(aoi) plot(aoi) # Transform the filtered geometry to EPSG:4326 and store its bounding box aoi %>% st_transform(\"EPSG:4326\") %>% st_bbox() -> bbox_4326 # Transform the filtered geometry to EPSG:32618 and store its bounding box aoi %>% st_transform(\"EPSG:32618\") %>% st_bbox() -> bbox_32618","title":"Load area of interest"},{"location":"data-library/Pull_Sentinal2_l2_data/#arrange-stac-collection","text":"In this code chunk, the primary goal is to search for and obtain satellite imagery data. The data source being tapped into is a SpatioTemporal Asset Catalog (STAC) provided by an online service (earth-search by Element84). Here\u2019s a breakdown: A connection is established with the STAC service, searching specifically within the \u201csentinel-s2-l2a-cogs\u201d collection. -The search is spatially constrained to a bounding box (bbox_4326) and temporally limited to a range of one day, between May 15 and May 16, 2021. -Once the search is conducted, the desired assets or spectral bands from the returned satellite images are defined, ranging from Band 1 (B01) to Band 12 (B12) and including the Scene Classification Layer (SCL). -These bands are then organized into an image collection for further processing or analysis. # Initialize STAC connection s = stac(\"https://earth-search.aws.element84.com/v0\") # Search for Sentinel-2 images within specified bounding box and date range items = s %>% stac_search(collections = \"sentinel-s2-l2a-cogs\", bbox = c(bbox_4326[\"xmin\"], bbox_4326[\"ymin\"], bbox_4326[\"xmax\"], bbox_4326[\"ymax\"]), datetime = \"2021-05-15/2021-05-16\") %>% post_request() %>% items_fetch(progress = FALSE) # Print number of found items length(items$features) [1] 12 # Prepare the assets for analysis library(gdalcubes) assets = c(\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\", \"SCL\") s2_collection = stac_image_collection(items$features, asset_names = assets) # Display the image collection s2_collection Image collection object, referencing 12 images with 13 bands Images: name left top bottom right 1 S2B_11SNS_20210515_1_L2A -117.0002 33.43957 32.44372 -115.8191 2 S2B_11SPS_20210515_1_L2A -115.9361 33.43490 32.42937 -114.7436 3 S2B_11SQS_20210515_0_L2A -114.8732 33.42092 32.41918 -113.9566 4 S2B_12STB_20210515_0_L2A -114.2244 33.40433 32.61015 -113.9559 5 S2B_11SNT_20210515_0_L2A -117.0002 34.34164 33.34577 -115.8066 6 S2B_11SPT_20210515_0_L2A -115.9253 34.33683 33.33091 -114.7198 datetime srs 1 2021-05-15T18:35:13 EPSG:32611 2 2021-05-15T18:35:10 EPSG:32611 3 2021-05-15T18:35:06 EPSG:32611 4 2021-05-15T18:35:01 EPSG:32612 5 2021-05-15T18:34:59 EPSG:32611 6 2021-05-15T18:34:55 EPSG:32611 [ omitted 6 images ] Bands: name offset scale unit nodata image_count 1 B01 0 1 12 2 B02 0 1 12 3 B03 0 1 12 4 B04 0 1 12 5 B05 0 1 12 6 B06 0 1 12 7 B07 0 1 12 8 B08 0 1 12 9 B09 0 1 12 10 B11 0 1 12 11 B12 0 1 12 12 B8A 0 1 12 13 SCL 0 1 12","title":"Arrange STAC collection"},{"location":"data-library/Pull_Sentinal2_l2_data/#define-view-window","text":"In this code chunk, a \u2018view\u2019 on the previously obtained satellite image collection is being defined. Think of this as setting up a specific lens or perspective to look at the satellite data: -The view is set to the coordinate reference system EPSG:32618. -Spatial resolution is defined as 100x100 meters. -Temporal resolution is defined monthly (P1M), even though the actual range is only one day. -When there are multiple values in a grid cell or timeframe, they are aggregated using the median value. -If any resampling is needed, the nearest neighbor method is used (near). -The spatial and temporal extents are constrained to specific values. -By defining this view, it allows for consistent analysis and visualization of the image collection within the specified spatial and temporal resolutions and extents. # Define a specific view on the satellite image collection v = cube_view( srs = \"EPSG:32618\", dx = 100, dy = 100, dt = \"P1M\", aggregation = \"median\", resampling = \"near\", extent = list( t0 = \"2021-05-15\", t1 = \"2021-05-16\", left = bbox_32618[\"xmin\"], right = bbox_32618[\"xmax\"], top = bbox_32618[\"ymax\"], bottom = bbox_32618[\"ymin\"] ) ) # Display the defined view v A data cube view object Dimensions: low high count pixel_size t 2021-05-01 2021-05-31 1 P1M y 4471226.41402451 4741326.41402451 2701 100 x -3463720.00044994 -3191420.00044994 2723 100 SRS: \"EPSG:32618\" Temporal aggregation method: \"median\" Spatial resampling method: \"near\"","title":"Define view window"},{"location":"data-library/Pull_Sentinal2_l2_data/#pull-data","text":"In this chunk, the primary aim is to transform and prepare satellite imagery data for analysis: -The current time is stored in variable a for tracking the time taken by the process. -The previously defined \u2018view\u2019 on the satellite imagery, v, is used to create a raster cube, a multi-dimensional array containing the satellite data. This raster cube contains spatial, spectral, and temporal data. -The desired spectral bands are selected. -The data is limited to a specific area of interest, aoi. -The band names are renamed to their respective wavelengths in nanometers for clarity. -A subset of the data, comprising 50,000 random samples, is selected. -Unwanted columns are removed, and the dataset is transformed into a long format, where each row represents a particular date and wavelength combination. -The entire process duration is computed by taking the difference between the end time (b) and the start time (a). -The transformed dataset y is then displayed. # Record start time a <- Sys.time() # Transform the satellite image collection into a raster cube x <- s2_collection %>% raster_cube(v) %>% select_bands(c(\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\")) %>% extract_geom(aoi) %>% rename( \"time\" = \"time\", \"443\" = \"B01\", \"490\" = \"B02\", \"560\" = \"B03\", \"665\" = \"B04\", \"705\" = \"B05\", \"740\" = \"B06\", \"783\" = \"B07\", \"842\" = \"B08\", \"865\" = \"B8A\", \"940\" = \"B09\", \"1610\" = \"B11\", \"2190\" = \"B12\" ) # Sample, transform and prepare data for analysis y <- x %>% slice_sample(n = 50000) %>% select(-FID) %>% pivot_longer(!time, names_to = \"wavelength_nm\", values_to = \"reflectance\") %>% mutate(wavelength_nm = as.numeric(wavelength_nm)) # Record end time and compute duration b <- Sys.time() processing_time <- difftime(b, a) # Display the processing time and transformed dataset processing_time Time difference of 1.23593 mins y # A tibble: 600,000 \u00d7 3 time wavelength_nm reflectance <chr> <dbl> <dbl> 1 2021-05-01 443 1855 2 2021-05-01 490 2255 3 2021-05-01 560 2884 4 2021-05-01 665 3711 5 2021-05-01 705 3990 6 2021-05-01 740 4009 7 2021-05-01 783 4078 8 2021-05-01 842 4219 9 2021-05-01 865 4060 10 2021-05-01 940 4120 # \u2139 599,990 more rows","title":"Pull data"},{"location":"data-library/Pull_Sentinal2_l2_data/#base-plot","text":"# Set custom colors for the plot our_green <- \"#4CAF50\" our_white <- \"#FFFFFF\" our_yellow <- \"#FFEB3B\" # Create a 2D density plot day_density <- ggplot(data = y, aes(x = wavelength_nm, y = reflectance, group = time)) + stat_smooth(color = our_green, fill = \"lightgrey\") + geom_density2d(colour = \"black\", bins = 10, alpha = 0.1) + stat_density2d(aes(alpha = ..level.., fill = ..level..), linewidth = 2, bins = 10, geom = \"polygon\") + scale_fill_gradient(low = our_white, high = our_yellow) + scale_alpha(range = c(0.00, 0.8), guide = FALSE) + theme_tufte() + xlab(\"wavelength\") + ylab(\"reflectance\") + ylim(0, 16000) + theme( aspect.ratio = 5/14, axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, colour = c(\"darkblue\", \"blue\", \"green\", \"red\", \"darkred\", \"darkred\", \"darkred\", \"darkred\", \"darkred\", \"black\", \"black\", \"black\", \"black\")), axis.title.x = element_blank(), axis.title.y = element_blank(), plot.margin = margin(t = 30, r = 10, b = 40, l = 18) ) + scale_x_continuous(breaks = c(443, 490, 560, 665, 705, 740, 783, 842, 865, 940, 1610, 2190)) # Display the plot day_density","title":"Base plot"},{"location":"data-library/Pull_Sentinal2_l2_data/#inlay-1-geographic-zone","text":"guide_map <- ggplot(data= aoi_total) + geom_sf(fill=our_yellow, color=our_white) + geom_sf(data= aoi, fill=our_green, color=our_white) + theme_tufte()+ ggtitle(\"Zone 5\")+ theme(axis.text.x=element_blank(), #remove x axis labels axis.ticks.x=element_blank(), #remove x axis ticks axis.text.y=element_blank(), #remove y axis labels axis.ticks.y=element_blank() #remove y axis ticks, bg=none )+ theme(plot.title = element_text(hjust=0.8, vjust = -2)) guide_map","title":"Inlay 1 - geographic zone"},{"location":"data-library/Pull_Sentinal2_l2_data/#inlay-2-date-text","text":"library(geosphere) aoi_total |> st_centroid() |> st_transform(crs=\"+proj=longlat\") |> st_coordinates() |> colMeans() -> lat_long daylength_line <- daylength(lat = lat_long[2], 1:365) daylengths <- data.frame(time= 1:365, daylength = daylength_line) library(lubridate) # Create a template date object date <- as.POSIXlt(\"2021-05-15\") doy <- format(date, format = \"%j\") |> as.numeric() display_date <- format(date, format=\"%e %B %Y \")","title":"Inlay 2 - date text"},{"location":"data-library/Pull_Sentinal2_l2_data/#inlay-3-daylength","text":"date_inlay <- ggplot(data=daylengths) + ggtitle(\"Daylength\")+ geom_ribbon(aes(x=time, ymin=daylength, ymax=15), fill=our_grey, alpha=0.5) + geom_ribbon(aes(x=time, ymax=daylength, ymin=9), fill=our_yellow, alpha=1) + geom_hline(yintercept=12, color=our_white) + geom_vline(xintercept=doy, color=our_green, size=1) + theme_tufte() + ylim(9,15) + theme(axis.text.y=element_blank(), axis.ticks.y=element_blank(), axis.title.y=element_blank(), axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank()) + theme(plot.title = element_text(hjust=0.5, vjust = 0)) date_inlay","title":"Inlay 3 - daylength"},{"location":"data-library/Pull_Sentinal2_l2_data/#ensemble-map-assembly","text":"library(cowplot) library(magick) map_overlay <- ggdraw(day_density) + draw_plot(guide_map, x = 1.08, y = 1, hjust = 1, vjust = 1, width = 0.3, height = 0.3)+ draw_plot(date_inlay, x = 1, y = 0.35, hjust = 1, vjust = 1, width = 0.1, height = 0.25)+ geom_text(aes(x=1, y=0.08, label=display_date, hjust = 1), color=our_grey, cex=3, fontface='bold') + # draw_image(\"Ty_powerline_plots/Southern_California_Edison_Logo.png\", x = -0.24, y = 0.38, scale=.3)+ # draw_image(\"Ty_powerline_plots/earthlab_logo.png\", x = -0.38, y = 0.38, scale=.25)+ geom_text(aes(x=0.4, y=.9, label=\"Spectral library - Monthly average\"), color=our_green, hjust = 0, cex=8, fontface='bold') + geom_text(aes(x=0.01, y=.04, label=\"Created by ESIIL (T. Tuff) for Fall Hackathon -- October 2023. Sentinel 2 Data from 'https://earth-search.aws.element84.com/v0'\"), color=our_grey, hjust = 0, cex=3) + geom_text(aes(x=0.4, y=.1, label=\"wavelength (nm)\"), color=our_grey, hjust = 0, cex=4, fontface='bold') + geom_text(aes(x=0.01, y=.5,angle = 90, label=\"reflectance\"), color=our_grey, hjust = 0, cex=4, fontface='bold') map_overlay","title":"Ensemble map assembly"},{"location":"data-library/Pull_Sentinal2_l2_data/#save-map","text":"ggsave(map_overlay, file=\"day_density_15_May_2021_zone_5.png\", bg=\"white\", dpi = 600, width = 12, height = 5)","title":"Save map"},{"location":"data-library/Pull_Sentinal2_l2_data/#end-timer","text":"end <- Sys.time() difftime(end,start) Time difference of 3.2202 mins","title":"End timer"},{"location":"data-library/disturbance-stack/","text":"Earth Lab Disturbance Stack derived from Landfire The CU Boulder Earth Lab has integrated annual (1999-2020) disturbance presence data from Landfire with a new index of hotter drought into an easily managed raster data stack. To accelerate your access to this dataset, the ESIIL team has made disturbance stack data for the Southern Rockies available on the Cyverse data store at the below directory: ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/disturbance The stack data is in two versions, full and simplified. The full version (dist_stack_Southern_Rockies.tif) has the below values: Code Landfire disturbance status Hotter-drought status 0 none no hotter-drought/fewer than 4 thresholds exceeded 1 fire no hotter-drought/fewer than 4 thresholds exceeded 2 insect/disease no hotter-drought/fewer than 4 thresholds exceeded 3 other Landfire disturbance no hotter-drought/fewer than 4 thresholds exceeded 4 none hotter-drought with 4 thresholds exceeded 5 fire hotter-drought with 4 thresholds exceeded 6 insects/disease hotter-drought with 4 thresholds exceeded 7 other Landfire disturbance hotter-drought with 4 thresholds exceeded 8 none hotter-drought with 5 thresholds exceeded 9 fire hotter-drought with 5 thresholds exceeded 10 insects/disease hotter-drought with 5 thresholds exceeded 11 other Landfire disturbance hotter-drought with 5 thresholds exceeded 12 none hotter-drought with 6 thresholds exceeded 13 fire hotter-drought with 6 thresholds exceeded 14 insects/disease hotter-drought with 6 thresholds exceeded 15 other Landfire disturbance hotter-drought with 6 thresholds exceeded The simplified version (simple_dist_stack_Southern_Rockies.tif) has the below values, and only includes the most extreme hot drought: Code Landfire disturbance status Hotter-drought status 0 none no hotter-drought/fewer than 6 thresholds exceeded 1 fire no hotter-drought/fewer than 6 thresholds exceeded 2 insect/disease no hotter-drought/fewer than 6 thresholds exceeded 3 none hotter-drought with 6 thresholds exceeded 4 fire hotter-drought with 6 thresholds exceeded 5 insect/disease hhotter-drought with 6 thresholds exceeded Additional MODIS data is best accessed via VSI or STAC.","title":"Disturbance stack"},{"location":"data-library/disturbance-stack/#earth-lab-disturbance-stack-derived-from-landfire","text":"The CU Boulder Earth Lab has integrated annual (1999-2020) disturbance presence data from Landfire with a new index of hotter drought into an easily managed raster data stack. To accelerate your access to this dataset, the ESIIL team has made disturbance stack data for the Southern Rockies available on the Cyverse data store at the below directory: ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/disturbance The stack data is in two versions, full and simplified. The full version (dist_stack_Southern_Rockies.tif) has the below values: Code Landfire disturbance status Hotter-drought status 0 none no hotter-drought/fewer than 4 thresholds exceeded 1 fire no hotter-drought/fewer than 4 thresholds exceeded 2 insect/disease no hotter-drought/fewer than 4 thresholds exceeded 3 other Landfire disturbance no hotter-drought/fewer than 4 thresholds exceeded 4 none hotter-drought with 4 thresholds exceeded 5 fire hotter-drought with 4 thresholds exceeded 6 insects/disease hotter-drought with 4 thresholds exceeded 7 other Landfire disturbance hotter-drought with 4 thresholds exceeded 8 none hotter-drought with 5 thresholds exceeded 9 fire hotter-drought with 5 thresholds exceeded 10 insects/disease hotter-drought with 5 thresholds exceeded 11 other Landfire disturbance hotter-drought with 5 thresholds exceeded 12 none hotter-drought with 6 thresholds exceeded 13 fire hotter-drought with 6 thresholds exceeded 14 insects/disease hotter-drought with 6 thresholds exceeded 15 other Landfire disturbance hotter-drought with 6 thresholds exceeded The simplified version (simple_dist_stack_Southern_Rockies.tif) has the below values, and only includes the most extreme hot drought: Code Landfire disturbance status Hotter-drought status 0 none no hotter-drought/fewer than 6 thresholds exceeded 1 fire no hotter-drought/fewer than 6 thresholds exceeded 2 insect/disease no hotter-drought/fewer than 6 thresholds exceeded 3 none hotter-drought with 6 thresholds exceeded 4 fire hotter-drought with 6 thresholds exceeded 5 insect/disease hhotter-drought with 6 thresholds exceeded Additional MODIS data is best accessed via VSI or STAC.","title":"Earth Lab Disturbance Stack derived from Landfire"},{"location":"data-library/drought/","text":"Drought Indices There are a wide variety of drought indices and variables used to describe various forms of drought. This data is best accessed via VSI and STAC to enable climate data summarization at the desired temporal and spatial resolution. To accelerate your access to basic drought data, the ESIIL team has made annual averages of SPEI and PDSI for the Southern Rockies available on the Cyverse data store at the below directory: ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/Disturbance/drought SPEI, or the Standardised Precipitation-Evapotranspiration Index, is a multiscalar drought index based on climatic data. It can be used for determining the onset, duration and magnitude of drought conditions with respect to normal conditions in a variety of natural and managed systems such as crops, ecosystems, rivers, water resources, etc. An overview of SPEI is available here . The pre-compiled datasets are at the 30 day, 1 year, and 5 year time scales and are from the TerraClimate dataset . PDSI, or the Palmer Drought Severity Index, uses readily available temperature and precipitation data to estimate relative dryness. However, it is not multiscalar. An overview of PDSI from NCAR is here . The pre-compiled dataset is from the TerraClimate dataset .","title":"Drought indices (SPEI & PDSI)"},{"location":"data-library/drought/#drought-indices","text":"There are a wide variety of drought indices and variables used to describe various forms of drought. This data is best accessed via VSI and STAC to enable climate data summarization at the desired temporal and spatial resolution. To accelerate your access to basic drought data, the ESIIL team has made annual averages of SPEI and PDSI for the Southern Rockies available on the Cyverse data store at the below directory: ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/Disturbance/drought SPEI, or the Standardised Precipitation-Evapotranspiration Index, is a multiscalar drought index based on climatic data. It can be used for determining the onset, duration and magnitude of drought conditions with respect to normal conditions in a variety of natural and managed systems such as crops, ecosystems, rivers, water resources, etc. An overview of SPEI is available here . The pre-compiled datasets are at the 30 day, 1 year, and 5 year time scales and are from the TerraClimate dataset . PDSI, or the Palmer Drought Severity Index, uses readily available temperature and precipitation data to estimate relative dryness. However, it is not multiscalar. An overview of PDSI from NCAR is here . The pre-compiled dataset is from the TerraClimate dataset .","title":"Drought Indices"},{"location":"data-library/epa-ecoregions/","text":"EPA Ecoregions EPA ecoregions are a convenient spatial framework for ecosystem regions used by the United States Environmental Protection Agency. Full details on EPA ecoregions can be found here. A Roman numeral classification scheme has been adopted for different hierarchical levels of ecoregions, ranging from general regions to more detailed: Level I - 12 ecoregions in the continental U.S. Level II - 25 ecoregions in the continental U.S. Level III -105 ecoregions in the continental U.S. Level IV - 967 ecoregions in the conterminous U.S. Instructions for accessing spatial EPA ecoregion data can be found in the script code/create-data-library/access_epa_ecoregions.R. The script is also copied below: # This brief script demonstrates how to access level 3 and 4 EPA ecoregions for North America. # Directly accessing the files via VSI is recommended, as this uses cloud-hosted data. # A version for downloading the zipped files is also provided in case for some reason you need the actual files. # ESIIL, February 2024 # Tyler L. McIntosh ####### ACCESS SHAPEFILES DIRECTLY VIA VSI ######### require(glue) require(sf) epa_l3 <- glue::glue( \"/vsizip/vsicurl/\", #magic remote connection \"https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l3.zip\", #copied link to download location \"/us_eco_l3.shp\") |> #path inside zip file sf::st_read() epa_l4 <- glue::glue( \"/vsizip/vsicurl/\", #magic remote connection \"https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l4.zip\", #copied link to download location \"/us_eco_l4_no_st.shp\") |> #path inside zip file sf::st_read() ######### DOWNLOAD ZIPPED DATA FILES ######### #Set up directory directory <- \"~/data/ecoregions\" if (!dir.exists(directory)) { dir.create(directory) } #Avoid download timeout options(timeout = max(1000, getOption(\"timeout\"))) #URLs for downloads epaUrls <- c(\"https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l3.zip\", \"https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l4.zip\") destFiles <- file.path(directory, basename(epaUrls)) #Download mapply(FUN = function(url, destfile) {download.file(url = url, destfile = destfile, mode = \"wb\")}, url = epaUrls, destfile = destFiles) #Unzip downloaded files mapply(FUN = function(destfile, exdir) {unzip(zipfile = destfile, files = NULL, exdir = exdir)}, destfile = destFiles, exdir = gsub(pattern = \".zip\", replacement = \"\", x = destFiles))","title":"EPA Ecoregions"},{"location":"data-library/epa-ecoregions/#epa-ecoregions","text":"EPA ecoregions are a convenient spatial framework for ecosystem regions used by the United States Environmental Protection Agency. Full details on EPA ecoregions can be found here. A Roman numeral classification scheme has been adopted for different hierarchical levels of ecoregions, ranging from general regions to more detailed: Level I - 12 ecoregions in the continental U.S. Level II - 25 ecoregions in the continental U.S. Level III -105 ecoregions in the continental U.S. Level IV - 967 ecoregions in the conterminous U.S. Instructions for accessing spatial EPA ecoregion data can be found in the script code/create-data-library/access_epa_ecoregions.R. The script is also copied below: # This brief script demonstrates how to access level 3 and 4 EPA ecoregions for North America. # Directly accessing the files via VSI is recommended, as this uses cloud-hosted data. # A version for downloading the zipped files is also provided in case for some reason you need the actual files. # ESIIL, February 2024 # Tyler L. McIntosh ####### ACCESS SHAPEFILES DIRECTLY VIA VSI ######### require(glue) require(sf) epa_l3 <- glue::glue( \"/vsizip/vsicurl/\", #magic remote connection \"https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l3.zip\", #copied link to download location \"/us_eco_l3.shp\") |> #path inside zip file sf::st_read() epa_l4 <- glue::glue( \"/vsizip/vsicurl/\", #magic remote connection \"https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l4.zip\", #copied link to download location \"/us_eco_l4_no_st.shp\") |> #path inside zip file sf::st_read() ######### DOWNLOAD ZIPPED DATA FILES ######### #Set up directory directory <- \"~/data/ecoregions\" if (!dir.exists(directory)) { dir.create(directory) } #Avoid download timeout options(timeout = max(1000, getOption(\"timeout\"))) #URLs for downloads epaUrls <- c(\"https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l3.zip\", \"https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l4.zip\") destFiles <- file.path(directory, basename(epaUrls)) #Download mapply(FUN = function(url, destfile) {download.file(url = url, destfile = destfile, mode = \"wb\")}, url = epaUrls, destfile = destFiles) #Unzip downloaded files mapply(FUN = function(destfile, exdir) {unzip(zipfile = destfile, files = NULL, exdir = exdir)}, destfile = destFiles, exdir = gsub(pattern = \".zip\", replacement = \"\", x = destFiles))","title":"EPA Ecoregions"},{"location":"data-library/esiil-data-library/","text":"ESIIL Data Libraries ESIIL has compiled additional data libraries for use at summits and hackathons. Link to those data libraries are available here, along with a summary of their current contents (February 2024). General ESIIL Data Library Our data library features a diverse range of datasets, each with its own dedicated web page. To help you get started, we provide easy-to-use R and Python code snippets for downloading and working with each dataset. For more advanced users, we also offer comprehensive tutorials and vignettes tailored to individual datasets. Explore our rich collection and unlock the power of environmental data for your research today! Data Contents EDS in Indian Country Global native homelands USA federal tribal reservations All types of tribal land in USA Solving water NEON Aquatic instrument data EPA water quality USGS water services Data librarianship Public libraries survey Cutting-edge remote sensing NEON hyperspectral data Lidar-based canopy height Multispectral sentinel-2 on AWS Nature-based solutions and human development Open street map Extreme events and hazards EPA air quality data Fire Event Delineation (FIRED) US National Incident Management System Uranium mines Spatial scale Spatial occurrence as points Ecological forecasting National Ecological Observation Network (NEON) USA phenology network Forecasting NEON data NEON lidar after fire Data harmonization Data cataloged with publications NEON and LTER NEON lidar and organismal data Food supply UN Food and Agriculture Social justice Redlining Congressional voting Data science in decision making and policy US Census FDIC failed banks list AI in environmental data science WeatherBench Math, modeling, statistics NEON tick pathogen data Everglades food network Mammal primate association network EDS education Education statistics Nonprofit explorer MosAIc Data Library The data library from ESIIL's MosAIc Hackathon is located here . This data library contains some similar content to the general ESIIL data library, in addition to extra resources on cloud collaboration and AI. Data Contents Flood event inventory Flood event area (polygons) River geography River and basin features NEON Lakes NEON Rivers EPA water quality USGS Water Services Global Species Occurrence NEON LIDAR NEON biogeochemistry Open Street Map US Census Remote sensing","title":"Additional ESIIL Data Libraries"},{"location":"data-library/esiil-data-library/#esiil-data-libraries","text":"ESIIL has compiled additional data libraries for use at summits and hackathons. Link to those data libraries are available here, along with a summary of their current contents (February 2024).","title":"ESIIL Data Libraries"},{"location":"data-library/esiil-data-library/#general-esiil-data-library","text":"Our data library features a diverse range of datasets, each with its own dedicated web page. To help you get started, we provide easy-to-use R and Python code snippets for downloading and working with each dataset. For more advanced users, we also offer comprehensive tutorials and vignettes tailored to individual datasets. Explore our rich collection and unlock the power of environmental data for your research today!","title":"General ESIIL Data Library"},{"location":"data-library/esiil-data-library/#data-contents","text":"EDS in Indian Country Global native homelands USA federal tribal reservations All types of tribal land in USA Solving water NEON Aquatic instrument data EPA water quality USGS water services Data librarianship Public libraries survey Cutting-edge remote sensing NEON hyperspectral data Lidar-based canopy height Multispectral sentinel-2 on AWS Nature-based solutions and human development Open street map Extreme events and hazards EPA air quality data Fire Event Delineation (FIRED) US National Incident Management System Uranium mines Spatial scale Spatial occurrence as points Ecological forecasting National Ecological Observation Network (NEON) USA phenology network Forecasting NEON data NEON lidar after fire Data harmonization Data cataloged with publications NEON and LTER NEON lidar and organismal data Food supply UN Food and Agriculture Social justice Redlining Congressional voting Data science in decision making and policy US Census FDIC failed banks list AI in environmental data science WeatherBench Math, modeling, statistics NEON tick pathogen data Everglades food network Mammal primate association network EDS education Education statistics Nonprofit explorer","title":"Data Contents"},{"location":"data-library/esiil-data-library/#mosaic-data-library","text":"The data library from ESIIL's MosAIc Hackathon is located here . This data library contains some similar content to the general ESIIL data library, in addition to extra resources on cloud collaboration and AI.","title":"MosAIc Data Library"},{"location":"data-library/esiil-data-library/#data-contents_1","text":"Flood event inventory Flood event area (polygons) River geography River and basin features NEON Lakes NEON Rivers EPA water quality USGS Water Services Global Species Occurrence NEON LIDAR NEON biogeochemistry Open Street Map US Census Remote sensing","title":"Data Contents"},{"location":"data-library/fia/","text":"Forest Inventory and Analysis Database (FIA or FIADB) Database description The Forest Inventory and Analysis (FIA) program of the USDA Forest Service Research and Development Branch collects, processes, analyzes, and reports on data necessary for assessing the extent and condition of forest resources in the United States. This data is collected at the plot level across the US, and includes information such as tree quantity and identifications, downed woody materials, tree regeneration, and more. If you are looking for spatially continuous data, TreeMap is a data product derived from FIA data and uses machine learning algorithms to assign each forested pixel across the US with the id of the FIA plot that best matches it. This is an overview of the FIA program. This is the most recent user guide for the FIADB. Prepared data access functions FIA data is available from the FIA DataMart . Two R functions have been prepared for your use in downloading FIA data directly to your cloud instance. Those functions can be found at code/create-data-library/download_fia.R The functions are also copied here: # This script contains functions to download both individual # FIA data csv files as well as bulk download data types. The two key functions # described are fia_download_individual_data_files and fia_bulk_download_data_files # ESIIL, February 2024 # Tyler L. McIntosh options(timeout = 300) ################################ # DOWNLOAD INDIVIDUAL FIA DATASETS # # This function will download individual FIA datasets requested and return the filenames # It will create a new subdirectory for the files, \"fia_individual_data_files\". # If you want to bulk download data by type, use function fia_bulk_download_data_files # Note that you may want to change your environment's download timeout option to allow longer downloads # (e.g. options(timeout = 300)) # #### PARAMETERS #### # state_abbreviations : a vector of state abbreviations as strings (e.g. c(\"CO\", \"WY\", \"NM\")) # file_suffixes : a vector of data file oracle table names (e.g. c(\"DWM_VISIT\", \"COUNTY\") from https://www.fs.usda.gov/research/understory/forest-inventory-and-analysis-database-user-guide-phase-2 # directory : the directory in which to store the data (a new subdirectory will be created for the new files) # #### Example call to the function and read of the data #### # downloaded_files <- fia_download_individual_data_files( # state_abbreviations = c(\"CO\"), # file_suffixes = c(\"DWM_VISIT\", \"COUNTY\"), # directory = \"~/data\") # data_list <- downloaded_files |> lapply(readr::read_csv) # names(data_list) <- basename(downloaded_files) # fia_download_individual_data_files <- function(state_abbreviations, file_suffixes, directory) { #Ensure directory exists if (!dir.exists(directory)) { dir.create(directory) } base_url <- \"https://apps.fs.usda.gov/fia/datamart/CSV/\" # Define the subdirectory path subdirectory_path <- file.path(directory, \"fia_individual_data_files\") # Create the subdirectory if it does not exist if (!dir.exists(subdirectory_path)) { dir.create(subdirectory_path, recursive = TRUE) } downloaded_files <- c() # Initialize an empty vector to store downloaded filenames for (state in state_abbreviations) { for (suffix in file_suffixes) { # Replace underscores with spaces to match the naming convention in the URL url_suffix <- gsub(\"_\", \" \", suffix) url_suffix <- gsub(\" \", \"_\", toupper(url_suffix)) # URL seems to be uppercase # Construct the URL and filename using the subdirectory path url <- paste0(base_url, state, \"_\", url_suffix, \".csv\") filename <- paste0(subdirectory_path, \"/\", state, \"_\", suffix, \".csv\") # Attempt to download the file tryCatch({ download.file(url, destfile = filename, mode = \"wb\") downloaded_files <- c(downloaded_files, filename) # Add the filename to the vector message(\"Downloaded \", filename) }, error = function(e) { message(\"Failed to download \", url, \": \", e$message) }) } } return(downloaded_files) # Return the vector of downloaded filenames } ################################ # BULK DOWNLOAD FIA DATASETS # # This function will bulk download FIA datasets requested into associated subdirectories and return the filenames # as a named list of vectors, where each vector contains the files included in that bulk data set. # All bulk data subdirectories will be put into a directory called 'fia_bulk_data_files' # Note that you may want to change your environment's download timeout option to allow longer downloads # (e.g. options(timeout = 300)) # #### PARAMETERS #### # state_abbreviations : a vector of state abbreviations as strings (e.g. c(\"CO\", \"WY\", \"NM\")) # directory : the directory in which to store the data # bulk_data_types : a vector of bulk download mappings as strings (e.g. c(\"location level\", \"plot\")) # Available data mappings are: # \"location level\" # \"tree level\" # \"invasives and understory vegetation\" # \"down woody material\" # \"tree regeneration\" # \"ground cover\" # \"soils\" # \"population\" # \"plot\" # \"reference\" # Full descriptions of each of these data mappings can be found at the FIA user guide, # with each mapping associated with a different chapter of tables: # https://www.fs.usda.gov/research/understory/forest-inventory-and-analysis-database-user-guide-phase-2 # #### Example call to the function for multiple bulk data types and read in the data #### # downloaded_files <- fia_bulk_download_data_files( # state = c(\"CO\"), # directory = \"~/data\", # bulk_data_types = c(\"down woody material\", \"plot\") # ) # data_list_dwm <- downloaded_files$`down woody material`|> lapply(readr::read_csv) # names(data_list_dwm) <- basename(downloaded_files$`down woody material`) # fia_bulk_download_data_files <- function(state, directory, bulk_data_types) { #Ensure directory exists if (!dir.exists(directory)) { dir.create(directory) } # Map bulk data types to their corresponding file suffixes bulk_data_mappings <- list( \"down woody material\" = c( \"DWM_VISIT\", \"DWM_COARSE_WOODY_DEBRIS\", \"DWM_DUFF_LITTER_FUEL\", \"DWM_FINE_WOODY_DEBRIS\", \"DWM_MICROPLOT_FUEL\", \"DWM_RESIDUAL_PILE\", \"DWM_TRANSECT_SEGMENT\", \"COND_DWM_CALC\" ), \"location level\" = c( \"SURVEY\", \"PROJECT\", \"COUNTY\", \"PLOT\", \"COND\", \"SUBPLOT\", \"SUBP_COND\", #\"BOUNDARY\", \"SUBP_COND_CHNG_MTRX\" ), \"tree level\" = c( \"TREE\", \"WOODLAND_STEMS\", \"GRM_COMPONENT\", \"GRM_THRESHOLD\", \"GRM_MIDPT\", \"GRM_BEGIN\", \"GRM_ESTN\", \"BEGINEND\", \"SEEDLING\", \"SITETREE\" ), \"invasives and understory vegetation\" = c( \"INVASIVE_SUBPLOT_SPP\", \"P2VEG_SUBPLOT_SPP\", \"P2VEG_SUBP_STRUCTURE\" ), \"tree regeneration\" = c( \"PLOT_REGEN\", \"SUBPLOT_REGEN\", \"SEEDLING_REGEN\" ), \"ground cover\" = c( \"GRND_CVR\", \"GRND_LYR_FNCTL_GRP\", \"GRND_LYR_MICROQUAD\" ), \"soils\" = c( \"SUBP_SOIL_SAMPLE_LOC\", \"SUBP_SOIL_SAMPLE_LAYER\" ), \"population\" = c( \"POP_ESTN_UNIT\", \"POP_EVAL\", \"POP_EVAL_ATTRIBUTE\", \"POP_EVAL_GRP\", \"POP_EVAL_TYP\", \"POP_PLOT_STRATUM_ASSGN\", \"POP_STRATUM\" ), \"plot\" = c( \"PLOTGEOM\", \"PLOTSNAP\" ), \"reference\" = c( \"REF_POP_ATTRIBUTE\", \"REF_POP_EVAL_TYP_DESCR\", \"REF_FOREST_TYPE\", \"REF_FOREST_TYPE_GROUP\", \"REF_SPECIES\", \"REF_PLANT_DICTIONARY\", \"REF_SPECIES_GROUP\", \"REF_INVASIVE_SPECIES\", \"REF_HABTYP_DESCRIPTION\", \"REF_HABTYP_PUBLICATION\", \"REF_CITATION\", \"REF_FIADB_VERSION\", \"REF_STATE_ELEV\", \"REF_UNIT\", \"REF_RESEARCH_STATION\", \"REF_NVCS_HIERARCHY_STRICT\", \"REF_NVCS_LEVEL_1_CODES\", \"REF_NVCS_LEVEL_2_CODES\", \"REF_NVCS_LEVEL_3_CODES\", \"REF_NVCS_LEVEL_4_CODES\", \"REF_NVCS_LEVEL_5_CODES\", \"REF_NVCS_LEVEL_6_CODES\", \"REF_NVCS_LEVEL_7_CODES\", \"REF_NVCS_LEVEL_8_CODES\", \"REF_AGENT\", \"REF_DAMAGE_AGENT\", \"REF_DAMAGE_AGENT_GROUP\", \"REF_FVS_VAR_NAME\", \"REF_FVS_LOC_NAME\", \"REF_OWNGRP_CD\", \"REF_DIFFERENCE_TEST_PER_ACRE\", \"REF_DIFFERENCE_TEST_TOTALS\", \"REF_EQUATION_TABLE\", \"REF_SEQN\", \"REF_GRM_TYPE\", \"REF_INTL_TO_DOYLE_FACTOR\", \"REF_TREE_CARBON_RATIO_DEAD\", \"REF_TREE_DECAY_PROP\", \"REF_TREE_STAND_DEAD_CR_PROP\", \"REF_GRND_LYR\" ) ) # Initialize a named list to store the filenames for each bulk data type all_downloaded_files <- setNames(vector(\"list\", length(bulk_data_types)), bulk_data_types) # Define and create the main bulk data directory main_bulk_dir <- file.path(directory, \"fia_bulk_data_files\") if (!dir.exists(main_bulk_dir)) { dir.create(main_bulk_dir, recursive = TRUE) } # Loop through each bulk data type for (bulk_data_type in bulk_data_types) { # Check if the bulk data type is known if (!bulk_data_type %in% names(bulk_data_mappings)) { stop(\"Unknown bulk data type: \", bulk_data_type) } # Create a subdirectory name by replacing spaces with underscores subdirectory <- gsub(\" \", \"_\", bulk_data_type) subdirectory_path <- file.path(main_bulk_dir, subdirectory) # Create the subdirectory if it does not exist if (!dir.exists(subdirectory_path)) { dir.create(subdirectory_path, recursive = TRUE) } # Retrieve the correct set of file suffixes for the current bulk data type file_suffixes <- bulk_data_mappings[[bulk_data_type]] # Call the download function for each file suffix and save in the new subdirectory downloaded_files <- download_data_files( state_abbreviations = state, file_suffixes = file_suffixes, location = subdirectory_path ) # Store the downloaded filenames in the named list under the current bulk data type all_downloaded_files[[bulk_data_type]] <- downloaded_files } # Return the named list of vectors with filenames return(all_downloaded_files) }","title":"Forest Inventory Analysis Database"},{"location":"data-library/fia/#forest-inventory-and-analysis-database-fia-or-fiadb","text":"","title":"Forest Inventory and Analysis Database (FIA or FIADB)"},{"location":"data-library/fia/#database-description","text":"The Forest Inventory and Analysis (FIA) program of the USDA Forest Service Research and Development Branch collects, processes, analyzes, and reports on data necessary for assessing the extent and condition of forest resources in the United States. This data is collected at the plot level across the US, and includes information such as tree quantity and identifications, downed woody materials, tree regeneration, and more. If you are looking for spatially continuous data, TreeMap is a data product derived from FIA data and uses machine learning algorithms to assign each forested pixel across the US with the id of the FIA plot that best matches it. This is an overview of the FIA program. This is the most recent user guide for the FIADB.","title":"Database description"},{"location":"data-library/fia/#prepared-data-access-functions","text":"FIA data is available from the FIA DataMart . Two R functions have been prepared for your use in downloading FIA data directly to your cloud instance. Those functions can be found at code/create-data-library/download_fia.R The functions are also copied here: # This script contains functions to download both individual # FIA data csv files as well as bulk download data types. The two key functions # described are fia_download_individual_data_files and fia_bulk_download_data_files # ESIIL, February 2024 # Tyler L. McIntosh options(timeout = 300) ################################ # DOWNLOAD INDIVIDUAL FIA DATASETS # # This function will download individual FIA datasets requested and return the filenames # It will create a new subdirectory for the files, \"fia_individual_data_files\". # If you want to bulk download data by type, use function fia_bulk_download_data_files # Note that you may want to change your environment's download timeout option to allow longer downloads # (e.g. options(timeout = 300)) # #### PARAMETERS #### # state_abbreviations : a vector of state abbreviations as strings (e.g. c(\"CO\", \"WY\", \"NM\")) # file_suffixes : a vector of data file oracle table names (e.g. c(\"DWM_VISIT\", \"COUNTY\") from https://www.fs.usda.gov/research/understory/forest-inventory-and-analysis-database-user-guide-phase-2 # directory : the directory in which to store the data (a new subdirectory will be created for the new files) # #### Example call to the function and read of the data #### # downloaded_files <- fia_download_individual_data_files( # state_abbreviations = c(\"CO\"), # file_suffixes = c(\"DWM_VISIT\", \"COUNTY\"), # directory = \"~/data\") # data_list <- downloaded_files |> lapply(readr::read_csv) # names(data_list) <- basename(downloaded_files) # fia_download_individual_data_files <- function(state_abbreviations, file_suffixes, directory) { #Ensure directory exists if (!dir.exists(directory)) { dir.create(directory) } base_url <- \"https://apps.fs.usda.gov/fia/datamart/CSV/\" # Define the subdirectory path subdirectory_path <- file.path(directory, \"fia_individual_data_files\") # Create the subdirectory if it does not exist if (!dir.exists(subdirectory_path)) { dir.create(subdirectory_path, recursive = TRUE) } downloaded_files <- c() # Initialize an empty vector to store downloaded filenames for (state in state_abbreviations) { for (suffix in file_suffixes) { # Replace underscores with spaces to match the naming convention in the URL url_suffix <- gsub(\"_\", \" \", suffix) url_suffix <- gsub(\" \", \"_\", toupper(url_suffix)) # URL seems to be uppercase # Construct the URL and filename using the subdirectory path url <- paste0(base_url, state, \"_\", url_suffix, \".csv\") filename <- paste0(subdirectory_path, \"/\", state, \"_\", suffix, \".csv\") # Attempt to download the file tryCatch({ download.file(url, destfile = filename, mode = \"wb\") downloaded_files <- c(downloaded_files, filename) # Add the filename to the vector message(\"Downloaded \", filename) }, error = function(e) { message(\"Failed to download \", url, \": \", e$message) }) } } return(downloaded_files) # Return the vector of downloaded filenames } ################################ # BULK DOWNLOAD FIA DATASETS # # This function will bulk download FIA datasets requested into associated subdirectories and return the filenames # as a named list of vectors, where each vector contains the files included in that bulk data set. # All bulk data subdirectories will be put into a directory called 'fia_bulk_data_files' # Note that you may want to change your environment's download timeout option to allow longer downloads # (e.g. options(timeout = 300)) # #### PARAMETERS #### # state_abbreviations : a vector of state abbreviations as strings (e.g. c(\"CO\", \"WY\", \"NM\")) # directory : the directory in which to store the data # bulk_data_types : a vector of bulk download mappings as strings (e.g. c(\"location level\", \"plot\")) # Available data mappings are: # \"location level\" # \"tree level\" # \"invasives and understory vegetation\" # \"down woody material\" # \"tree regeneration\" # \"ground cover\" # \"soils\" # \"population\" # \"plot\" # \"reference\" # Full descriptions of each of these data mappings can be found at the FIA user guide, # with each mapping associated with a different chapter of tables: # https://www.fs.usda.gov/research/understory/forest-inventory-and-analysis-database-user-guide-phase-2 # #### Example call to the function for multiple bulk data types and read in the data #### # downloaded_files <- fia_bulk_download_data_files( # state = c(\"CO\"), # directory = \"~/data\", # bulk_data_types = c(\"down woody material\", \"plot\") # ) # data_list_dwm <- downloaded_files$`down woody material`|> lapply(readr::read_csv) # names(data_list_dwm) <- basename(downloaded_files$`down woody material`) # fia_bulk_download_data_files <- function(state, directory, bulk_data_types) { #Ensure directory exists if (!dir.exists(directory)) { dir.create(directory) } # Map bulk data types to their corresponding file suffixes bulk_data_mappings <- list( \"down woody material\" = c( \"DWM_VISIT\", \"DWM_COARSE_WOODY_DEBRIS\", \"DWM_DUFF_LITTER_FUEL\", \"DWM_FINE_WOODY_DEBRIS\", \"DWM_MICROPLOT_FUEL\", \"DWM_RESIDUAL_PILE\", \"DWM_TRANSECT_SEGMENT\", \"COND_DWM_CALC\" ), \"location level\" = c( \"SURVEY\", \"PROJECT\", \"COUNTY\", \"PLOT\", \"COND\", \"SUBPLOT\", \"SUBP_COND\", #\"BOUNDARY\", \"SUBP_COND_CHNG_MTRX\" ), \"tree level\" = c( \"TREE\", \"WOODLAND_STEMS\", \"GRM_COMPONENT\", \"GRM_THRESHOLD\", \"GRM_MIDPT\", \"GRM_BEGIN\", \"GRM_ESTN\", \"BEGINEND\", \"SEEDLING\", \"SITETREE\" ), \"invasives and understory vegetation\" = c( \"INVASIVE_SUBPLOT_SPP\", \"P2VEG_SUBPLOT_SPP\", \"P2VEG_SUBP_STRUCTURE\" ), \"tree regeneration\" = c( \"PLOT_REGEN\", \"SUBPLOT_REGEN\", \"SEEDLING_REGEN\" ), \"ground cover\" = c( \"GRND_CVR\", \"GRND_LYR_FNCTL_GRP\", \"GRND_LYR_MICROQUAD\" ), \"soils\" = c( \"SUBP_SOIL_SAMPLE_LOC\", \"SUBP_SOIL_SAMPLE_LAYER\" ), \"population\" = c( \"POP_ESTN_UNIT\", \"POP_EVAL\", \"POP_EVAL_ATTRIBUTE\", \"POP_EVAL_GRP\", \"POP_EVAL_TYP\", \"POP_PLOT_STRATUM_ASSGN\", \"POP_STRATUM\" ), \"plot\" = c( \"PLOTGEOM\", \"PLOTSNAP\" ), \"reference\" = c( \"REF_POP_ATTRIBUTE\", \"REF_POP_EVAL_TYP_DESCR\", \"REF_FOREST_TYPE\", \"REF_FOREST_TYPE_GROUP\", \"REF_SPECIES\", \"REF_PLANT_DICTIONARY\", \"REF_SPECIES_GROUP\", \"REF_INVASIVE_SPECIES\", \"REF_HABTYP_DESCRIPTION\", \"REF_HABTYP_PUBLICATION\", \"REF_CITATION\", \"REF_FIADB_VERSION\", \"REF_STATE_ELEV\", \"REF_UNIT\", \"REF_RESEARCH_STATION\", \"REF_NVCS_HIERARCHY_STRICT\", \"REF_NVCS_LEVEL_1_CODES\", \"REF_NVCS_LEVEL_2_CODES\", \"REF_NVCS_LEVEL_3_CODES\", \"REF_NVCS_LEVEL_4_CODES\", \"REF_NVCS_LEVEL_5_CODES\", \"REF_NVCS_LEVEL_6_CODES\", \"REF_NVCS_LEVEL_7_CODES\", \"REF_NVCS_LEVEL_8_CODES\", \"REF_AGENT\", \"REF_DAMAGE_AGENT\", \"REF_DAMAGE_AGENT_GROUP\", \"REF_FVS_VAR_NAME\", \"REF_FVS_LOC_NAME\", \"REF_OWNGRP_CD\", \"REF_DIFFERENCE_TEST_PER_ACRE\", \"REF_DIFFERENCE_TEST_TOTALS\", \"REF_EQUATION_TABLE\", \"REF_SEQN\", \"REF_GRM_TYPE\", \"REF_INTL_TO_DOYLE_FACTOR\", \"REF_TREE_CARBON_RATIO_DEAD\", \"REF_TREE_DECAY_PROP\", \"REF_TREE_STAND_DEAD_CR_PROP\", \"REF_GRND_LYR\" ) ) # Initialize a named list to store the filenames for each bulk data type all_downloaded_files <- setNames(vector(\"list\", length(bulk_data_types)), bulk_data_types) # Define and create the main bulk data directory main_bulk_dir <- file.path(directory, \"fia_bulk_data_files\") if (!dir.exists(main_bulk_dir)) { dir.create(main_bulk_dir, recursive = TRUE) } # Loop through each bulk data type for (bulk_data_type in bulk_data_types) { # Check if the bulk data type is known if (!bulk_data_type %in% names(bulk_data_mappings)) { stop(\"Unknown bulk data type: \", bulk_data_type) } # Create a subdirectory name by replacing spaces with underscores subdirectory <- gsub(\" \", \"_\", bulk_data_type) subdirectory_path <- file.path(main_bulk_dir, subdirectory) # Create the subdirectory if it does not exist if (!dir.exists(subdirectory_path)) { dir.create(subdirectory_path, recursive = TRUE) } # Retrieve the correct set of file suffixes for the current bulk data type file_suffixes <- bulk_data_mappings[[bulk_data_type]] # Call the download function for each file suffix and save in the new subdirectory downloaded_files <- download_data_files( state_abbreviations = state, file_suffixes = file_suffixes, location = subdirectory_path ) # Store the downloaded filenames in the named list under the current bulk data type all_downloaded_files[[bulk_data_type]] <- downloaded_files } # Return the named list of vectors with filenames return(all_downloaded_files) }","title":"Prepared data access functions"},{"location":"data-library/fire-cbi/","text":"Fire severity: Composite Burn Index (CBI) The Composite Burn Index (CBI) is a commonly used and ecologically meaningful measure of fire severity. Unlike some other measures of fire burn severity (e.g. MTBS fire severity), CBI is more readily comparable across large regions. To calculate this stack of CBI data the ESIIL team used the method described in Parks et al. (2019) , which uses random forests regression to calculate CBI based on Relativized Burn Ratio (RBR), latitude, climatic water deficit, and other factors. RBR was calculated using pre- and post-fire image composites of Landsat 4-9 imagery (Collection 2) during the growing season. A correction was applied to the CBI estimates to prevent overprediction at low values (Parks et al., 2019). This dataset has a layer for each year of data, with NA values at any location that was unburned during that year. Fire disturbance events documented in the Landfire fire events database will appear in these rasters. The data is pre-loaded onto the Cyverse data store and is located in the below file: ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/Disturbance/SR_landfire_fire_events_cbi_bc.tif","title":"Fire Severity (CBI)"},{"location":"data-library/fire-cbi/#fire-severity-composite-burn-index-cbi","text":"The Composite Burn Index (CBI) is a commonly used and ecologically meaningful measure of fire severity. Unlike some other measures of fire burn severity (e.g. MTBS fire severity), CBI is more readily comparable across large regions. To calculate this stack of CBI data the ESIIL team used the method described in Parks et al. (2019) , which uses random forests regression to calculate CBI based on Relativized Burn Ratio (RBR), latitude, climatic water deficit, and other factors. RBR was calculated using pre- and post-fire image composites of Landsat 4-9 imagery (Collection 2) during the growing season. A correction was applied to the CBI estimates to prevent overprediction at low values (Parks et al., 2019). This dataset has a layer for each year of data, with NA values at any location that was unburned during that year. Fire disturbance events documented in the Landfire fire events database will appear in these rasters. The data is pre-loaded onto the Cyverse data store and is located in the below file: ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/Disturbance/SR_landfire_fire_events_cbi_bc.tif","title":"Fire severity: Composite Burn Index (CBI)"},{"location":"data-library/gedi/","text":"GEDI data overview The Global Ecosystem Dynamics Investigation (GEDI) is a joint mission between NASA and the University of Maryland, with the instrument installed aboard the International Space Station. Data acquired using the instrument\u2019s three lasers are used to construct detailed three-dimensional (3D) maps of forest canopy height and the distribution of branches and leaves. By accurately measuring forests in 3D, GEDI data play an important role in understanding the amounts of biomass and carbon forests store and how much they lose when disturbed \u2013 vital information for understanding Earth\u2019s carbon cycle and how it is changing. GEDI data also can be used to study plant and animal habitats and biodiversity, and how these change over time. The GEDI homepage is located here . GEDI data is collected in footprints of ~25m along the track of the sensor. Each footprint is separated by 60m. GEDI footprint based aboveground biomass density (Mg/ha) over the Southern Rocky Mountains have been downloaded by Dr. Nayani Ilangakoon and placed on the Cyverse data store at the below path. The data are from 2019-2022, and are in the form of tiled CSV files. ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/GEDI Brief scripts in both R and Python are available in the GitHub repository demonstrating how to access and manipulate the data. The R script is copied below. ### This file reads, filter basedo on qulaity flag and ecoregion, and plots GEDI biomass data in csv format. # ESIIL, 2024 # Nayani Ilangakoon # Load necessary libraries library(readr) # For read_csv library(dplyr) # For data manipulation library(ggplot2) # For plotting library(tidyr) # For data tidying library(forcats) ############### # NOTE: This script is reading the data directly from the data store. It is only actually opening and processing a single csv # If you want to use all of the GEDI data that has been made available for your use, you will want to move it # to your cyverse instance to improve performance ############### # Define the root path to the data drive ROOT_PATH <- \"~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest\" # Create the path to the GEDI data by appending the directory name to the root path indir <- file.path(ROOT_PATH, \"GEDI/GEDI_SR_footprint_data/GEDI_biomass_SR\") # List the contents of the indir directory list.files(indir) # List all files that end with .csv in indir polyfiles <- list.files(indir, pattern = \"\\\\.csv$\", full.names = TRUE) # Print the list of .csv files polyfiles out_csv <- file.path(indir, \"recovery_treat_bms_64.csv\") # Reading the csv file created in the last step l4a_df <- read_csv(out_csv) # Assign \"NA\" to the values that needs to be discarded. l4a_df <- l4a_df %>% mutate(agbd = if_else(agbd == -9999,NA_real_,agbd)) l4a_df <- na.omit(l4a_df) # MCD12Q1 PFT types pft_legend <- c('Water Bodies', 'Evergreen Needleleaf Trees', 'Evergreen Broadleaf Trees', 'Deciduous Needleleaf Trees', 'Deciduous Broadleaf Trees', 'Shrub', 'Grass', 'Cereal Croplands', 'Broadleaf Croplands', 'Urban and Built-up Lands', 'Permanent Snow and Ice', 'Barren', 'Unclassified') # label PFT classes with numbers names(pft_legend) <- as.character(0:12) # Creating mask with good quality shots and trees/shrubs pft class mask <- l4a_df$l4_quality_flag == 1 & l4a_df$`land_cover_data/pft_class` <= 5 # Filter the dataframe based on the mask filtered_df <- l4a_df[mask, ] # Transforming the PFT class to a factor with labels filtered_df$`land_cover_data/pft_class` <- factor(filtered_df$`land_cover_data/pft_class`, levels = names(pft_legend), labels = pft_legend) # Plotting the distribution of GEDI L4A AGBD estimates by PFTs ggplot(filtered_df, aes(x = agbd, fill = `land_cover_data/pft_class`)) + geom_histogram(bins = 30, alpha = 0.6, position = \"identity\") + scale_fill_manual(values = rainbow(length(unique(filtered_df$`land_cover_data/pft_class`)))) + labs(title = 'Distribution of GEDI L4A AGBD estimates by PFTs (Plant Functional Types) in ACA in 2020', x = 'agbd (Mg / ha)', y = 'Frequency') + theme_minimal() + guides(fill = guide_legend(title = \"PFT Class\")) + theme(legend.position = \"bottom\") # Saving the plot ggsave(\"test.png\", width = 15, height = 5, units = \"in\") # Assuming l4a_df and mask have been defined as before # Binning the elevation data l4a_df <- l4a_df %>% mutate(elev_bin = cut(elev_lowestmode, breaks = seq(0, 5000, by = 500))) # Ensure PFT class is a factor with proper labels l4a_df$`land_cover_data/pft_class` <- factor(l4a_df$`land_cover_data/pft_class`, levels = names(pft_legend), labels = pft_legend) # Filtering the dataframe based on mask and ensure it is applied correctly filtered_df <- l4a_df %>% filter(mask) # Creating the boxplot g <- ggplot(filtered_df, aes(x = elev_bin, y = agbd)) + geom_boxplot() + facet_wrap(~`land_cover_data/pft_class`, scales = \"free\", labeller = labeller(`land_cover_data/pft_class` = as_labeller(pft_legend))) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + labs(x = \"Elevation (m)\", y = \"agbd\", title = \"AGBD by Elevation and PFT Class\") + theme_minimal() # Print the plot print(g) # Save the plot ggsave(\"agbd_category.png\", plot = g, width = 15, height = 10, units = \"in\")","title":"GEDI"},{"location":"data-library/gedi/#gedi-data-overview","text":"The Global Ecosystem Dynamics Investigation (GEDI) is a joint mission between NASA and the University of Maryland, with the instrument installed aboard the International Space Station. Data acquired using the instrument\u2019s three lasers are used to construct detailed three-dimensional (3D) maps of forest canopy height and the distribution of branches and leaves. By accurately measuring forests in 3D, GEDI data play an important role in understanding the amounts of biomass and carbon forests store and how much they lose when disturbed \u2013 vital information for understanding Earth\u2019s carbon cycle and how it is changing. GEDI data also can be used to study plant and animal habitats and biodiversity, and how these change over time. The GEDI homepage is located here . GEDI data is collected in footprints of ~25m along the track of the sensor. Each footprint is separated by 60m. GEDI footprint based aboveground biomass density (Mg/ha) over the Southern Rocky Mountains have been downloaded by Dr. Nayani Ilangakoon and placed on the Cyverse data store at the below path. The data are from 2019-2022, and are in the form of tiled CSV files. ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/GEDI Brief scripts in both R and Python are available in the GitHub repository demonstrating how to access and manipulate the data. The R script is copied below. ### This file reads, filter basedo on qulaity flag and ecoregion, and plots GEDI biomass data in csv format. # ESIIL, 2024 # Nayani Ilangakoon # Load necessary libraries library(readr) # For read_csv library(dplyr) # For data manipulation library(ggplot2) # For plotting library(tidyr) # For data tidying library(forcats) ############### # NOTE: This script is reading the data directly from the data store. It is only actually opening and processing a single csv # If you want to use all of the GEDI data that has been made available for your use, you will want to move it # to your cyverse instance to improve performance ############### # Define the root path to the data drive ROOT_PATH <- \"~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest\" # Create the path to the GEDI data by appending the directory name to the root path indir <- file.path(ROOT_PATH, \"GEDI/GEDI_SR_footprint_data/GEDI_biomass_SR\") # List the contents of the indir directory list.files(indir) # List all files that end with .csv in indir polyfiles <- list.files(indir, pattern = \"\\\\.csv$\", full.names = TRUE) # Print the list of .csv files polyfiles out_csv <- file.path(indir, \"recovery_treat_bms_64.csv\") # Reading the csv file created in the last step l4a_df <- read_csv(out_csv) # Assign \"NA\" to the values that needs to be discarded. l4a_df <- l4a_df %>% mutate(agbd = if_else(agbd == -9999,NA_real_,agbd)) l4a_df <- na.omit(l4a_df) # MCD12Q1 PFT types pft_legend <- c('Water Bodies', 'Evergreen Needleleaf Trees', 'Evergreen Broadleaf Trees', 'Deciduous Needleleaf Trees', 'Deciduous Broadleaf Trees', 'Shrub', 'Grass', 'Cereal Croplands', 'Broadleaf Croplands', 'Urban and Built-up Lands', 'Permanent Snow and Ice', 'Barren', 'Unclassified') # label PFT classes with numbers names(pft_legend) <- as.character(0:12) # Creating mask with good quality shots and trees/shrubs pft class mask <- l4a_df$l4_quality_flag == 1 & l4a_df$`land_cover_data/pft_class` <= 5 # Filter the dataframe based on the mask filtered_df <- l4a_df[mask, ] # Transforming the PFT class to a factor with labels filtered_df$`land_cover_data/pft_class` <- factor(filtered_df$`land_cover_data/pft_class`, levels = names(pft_legend), labels = pft_legend) # Plotting the distribution of GEDI L4A AGBD estimates by PFTs ggplot(filtered_df, aes(x = agbd, fill = `land_cover_data/pft_class`)) + geom_histogram(bins = 30, alpha = 0.6, position = \"identity\") + scale_fill_manual(values = rainbow(length(unique(filtered_df$`land_cover_data/pft_class`)))) + labs(title = 'Distribution of GEDI L4A AGBD estimates by PFTs (Plant Functional Types) in ACA in 2020', x = 'agbd (Mg / ha)', y = 'Frequency') + theme_minimal() + guides(fill = guide_legend(title = \"PFT Class\")) + theme(legend.position = \"bottom\") # Saving the plot ggsave(\"test.png\", width = 15, height = 5, units = \"in\") # Assuming l4a_df and mask have been defined as before # Binning the elevation data l4a_df <- l4a_df %>% mutate(elev_bin = cut(elev_lowestmode, breaks = seq(0, 5000, by = 500))) # Ensure PFT class is a factor with proper labels l4a_df$`land_cover_data/pft_class` <- factor(l4a_df$`land_cover_data/pft_class`, levels = names(pft_legend), labels = pft_legend) # Filtering the dataframe based on mask and ensure it is applied correctly filtered_df <- l4a_df %>% filter(mask) # Creating the boxplot g <- ggplot(filtered_df, aes(x = elev_bin, y = agbd)) + geom_boxplot() + facet_wrap(~`land_cover_data/pft_class`, scales = \"free\", labeller = labeller(`land_cover_data/pft_class` = as_labeller(pft_legend))) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + labs(x = \"Elevation (m)\", y = \"agbd\", title = \"AGBD by Elevation and PFT Class\") + theme_minimal() # Print the plot print(g) # Save the plot ggsave(\"agbd_category.png\", plot = g, width = 15, height = 10, units = \"in\")","title":"GEDI data overview"},{"location":"data-library/landfire-events/","text":"LANDFIRE Public Events Geodatabase From 'LANDFIRE Product Descriptions with References' The LF National (LF 1.X) Public Events Geodatabase is a collection of recent natural disturbance and land management activities used to update existing vegetation and fuel layers during LF Program deliverables. Public Events exclude proprietary and/or sensitive data. This geodatabase includes three feature classes - Raw Events, Model Ready Events, and Exotics. The Public Raw and Model Ready Event feature classes include natural disturbance and vegetation/fuel treatment data. The Public Exotics feature class contains data on the occurrence of exotic or invasive plant species. There is also a look up table for the source code (lutSource_Code), an attribute found in all three feature classes. The source code is an LF internal code assigned to each data source. Consult thetable\u201clutSource_Code\u201d in thegeodatabases for more information about the data sources included in, and excluded from, releases. The data compiled in the three feature classes are collected from disparate sources including federal, state, local, and private organizations. All data submitted to LF are evaluated for inclusion into the LF Events geodatabase. Acceptable Event data must have the following minimum requirements to be included in the Events geodatabase: 1) be represented by a polygon on the landscape and have a defined spatial coordinate system 2) have an acceptable event type (Appendix B) or exotics plant species 3) be attributed with year of occurrence or observation of the current data call. Metadata The LANDFIRE public events geodatabase contents description is available here . This document provides a description of how polygon data of disturbans and treatments are evaluated and processed into the LANDFIRE Events geodatabase. The Raw and Model Ready Events Data Dictionary is available here . Note that this is a large geodatabase (> 1 million polygons). Recommend filtering as soon as possible. The relevant layers within the .gdb file are: CONUS_230_PublicExotics CONUS_230_PublicModelReadyEvents CONUS_230_PublicRawEvents Access Storage location: ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/Disturbance/LF_Public_Events_1999_2022 Example access script: system(\"cp -r ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/Disturbance/LF_Public_Events_1999_2022 ~/LF_Events\") #move the data first!! landfireEvents <- sf::st_read(\"~/LF_Events/LF_Public_Events_1999_2022.gdb\", layer = \"CONUS_230_PublicModelReadyEvents\") unique(landfireEvents$Event_Type) # [1] \"Thinning\" \"Other Mechanical\" \"Prescribed Fire\" \"Herbicide\" # [5] \"Clearcut\" \"Harvest\" \"Wildfire\" \"Mastication\" # [9] \"Wildland Fire\" \"Chemical\" \"Development\" \"Biological\" # [13] \"Weather\" \"Planting\" \"Reforestation\" \"Insects\" # [17] \"Seeding\" \"Disease\" \"Wildland Fire Use\" \"Insects/Disease\" # [21] \"Insecticide\" landfireFireEvents <- landfireEvents |> dplyr::filter(Event_Type == \"Wildfire\" | Event_Type == \"Wildland Fire Use\" | Event_Type == \"Prescribed Fire\" | Event_Type == \"Wildland Fire\" | Event_Type == \"Fire\")","title":"Landfire Events"},{"location":"data-library/landfire-events/#landfire-public-events-geodatabase","text":"From 'LANDFIRE Product Descriptions with References' The LF National (LF 1.X) Public Events Geodatabase is a collection of recent natural disturbance and land management activities used to update existing vegetation and fuel layers during LF Program deliverables. Public Events exclude proprietary and/or sensitive data. This geodatabase includes three feature classes - Raw Events, Model Ready Events, and Exotics. The Public Raw and Model Ready Event feature classes include natural disturbance and vegetation/fuel treatment data. The Public Exotics feature class contains data on the occurrence of exotic or invasive plant species. There is also a look up table for the source code (lutSource_Code), an attribute found in all three feature classes. The source code is an LF internal code assigned to each data source. Consult thetable\u201clutSource_Code\u201d in thegeodatabases for more information about the data sources included in, and excluded from, releases. The data compiled in the three feature classes are collected from disparate sources including federal, state, local, and private organizations. All data submitted to LF are evaluated for inclusion into the LF Events geodatabase. Acceptable Event data must have the following minimum requirements to be included in the Events geodatabase: 1) be represented by a polygon on the landscape and have a defined spatial coordinate system 2) have an acceptable event type (Appendix B) or exotics plant species 3) be attributed with year of occurrence or observation of the current data call.","title":"LANDFIRE Public Events Geodatabase"},{"location":"data-library/landfire-events/#metadata","text":"The LANDFIRE public events geodatabase contents description is available here . This document provides a description of how polygon data of disturbans and treatments are evaluated and processed into the LANDFIRE Events geodatabase. The Raw and Model Ready Events Data Dictionary is available here . Note that this is a large geodatabase (> 1 million polygons). Recommend filtering as soon as possible. The relevant layers within the .gdb file are: CONUS_230_PublicExotics CONUS_230_PublicModelReadyEvents CONUS_230_PublicRawEvents","title":"Metadata"},{"location":"data-library/landfire-events/#access","text":"Storage location: ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/Disturbance/LF_Public_Events_1999_2022 Example access script: system(\"cp -r ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/Disturbance/LF_Public_Events_1999_2022 ~/LF_Events\") #move the data first!! landfireEvents <- sf::st_read(\"~/LF_Events/LF_Public_Events_1999_2022.gdb\", layer = \"CONUS_230_PublicModelReadyEvents\") unique(landfireEvents$Event_Type) # [1] \"Thinning\" \"Other Mechanical\" \"Prescribed Fire\" \"Herbicide\" # [5] \"Clearcut\" \"Harvest\" \"Wildfire\" \"Mastication\" # [9] \"Wildland Fire\" \"Chemical\" \"Development\" \"Biological\" # [13] \"Weather\" \"Planting\" \"Reforestation\" \"Insects\" # [17] \"Seeding\" \"Disease\" \"Wildland Fire Use\" \"Insects/Disease\" # [21] \"Insecticide\" landfireFireEvents <- landfireEvents |> dplyr::filter(Event_Type == \"Wildfire\" | Event_Type == \"Wildland Fire Use\" | Event_Type == \"Prescribed Fire\" | Event_Type == \"Wildland Fire\" | Event_Type == \"Fire\")","title":"Access"},{"location":"data-library/lcmap/","text":"Land Change Monitoring, Assessment, and Projection Land Change Monitoring, Assessment, and Projection (LCMAP) represents a new generation of land cover mapping and change monitoring from the U.S. Geological Survey\u2019s Earth Resources Observation and Science (EROS) Center. LCMAP answers a need for higher quality results at greater frequency with additional land cover and change variables than previous efforts. The USGS website for LCMAP is here. Collection 1.3 of the LCMAP product contains 10 different science products ( details here ). To accelerate your access to this dataset, the ESIIL team has made LCMAP 1.3 Primary Land Cover product (LCPRI) data for the Southern Rockies available on the Cyverse data store at the below directory: ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/LCMAP_SR_1985_2021 Additional LCMAP layers and products may be accessed via STAC and VSI (see below for example). The script used to download the LCMAP data already available is located in the GitHub repo at /code/create-data-library/LCMAP_Direct_Access-adapted.ipynb. The code is from the LCMAP data access tutorial. #Access LCMAP data from STAC #Adapted from 'Download data from a STAC API using R, rstac, and GDAL' #https://stacspec.org/en/tutorials/1-download-data-using-r/ require(glue) require(sf) require(terra) require(rstac) #Access ecoregiosn via VSI epa_l3 <- glue::glue( \"/vsizip/vsicurl/\", #magic remote connection \"https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l3.zip\", #copied link to download location \"/us_eco_l3.shp\") |> #path inside zip file sf::st_read() #Get just S.Rockies and ensure that it is in EPSG:4326 southernRockies <- epa_l3 |> dplyr::filter(US_L3NAME == \"Southern Rockies\") |> dplyr::group_by(US_L3NAME) |> dplyr::summarize(geometry = sf::st_union(geometry)) |> sf::st_transform(\"EPSG:4326\") bboxSR4326 <- sf::st_bbox(southernRockies) # Create a stac query for just the 2021 LCMAP data stac_query <- rstac::stac( \"https://planetarycomputer.microsoft.com/api/stac/v1\" ) |> rstac::stac_search( collections = \"usgs-lcmap-conus-v13\", bbox = bboxSR4326, datetime = \"2021-01-01/2021-12-31\" ) |> rstac::get_request() #A function to get a vsicurl url form a base url make_lcmap_vsicurl_url <- function(base_url) { paste0( \"/vsicurl\", \"?pc_url_signing=yes\", \"&pc_collection=usgs-lcmap-conus-v13\", \"&url=\", base_url ) } lcpri_url <- make_lcmap_vsicurl_url(rstac::assets_url(stac_query, \"lcpri\")) #Pull the file out_file <- tempfile(fileext = \".tif\") sf::gdal_utils( \"warp\", source = lcpri_url, destination = out_file, options = c( \"-t_srs\", sf::st_crs(southernRockies)$wkt, \"-te\", sf::st_bbox(southernRockies) ) ) #Create the raster and plot! terra::rast(out_file) |> terra::plot() southernRockies |> sf::st_geometry() |> plot(lwd = 3, add = TRUE)","title":"LCMAP (Land cover)"},{"location":"data-library/lcmap/#land-change-monitoring-assessment-and-projection","text":"Land Change Monitoring, Assessment, and Projection (LCMAP) represents a new generation of land cover mapping and change monitoring from the U.S. Geological Survey\u2019s Earth Resources Observation and Science (EROS) Center. LCMAP answers a need for higher quality results at greater frequency with additional land cover and change variables than previous efforts. The USGS website for LCMAP is here. Collection 1.3 of the LCMAP product contains 10 different science products ( details here ). To accelerate your access to this dataset, the ESIIL team has made LCMAP 1.3 Primary Land Cover product (LCPRI) data for the Southern Rockies available on the Cyverse data store at the below directory: ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/LCMAP_SR_1985_2021 Additional LCMAP layers and products may be accessed via STAC and VSI (see below for example). The script used to download the LCMAP data already available is located in the GitHub repo at /code/create-data-library/LCMAP_Direct_Access-adapted.ipynb. The code is from the LCMAP data access tutorial. #Access LCMAP data from STAC #Adapted from 'Download data from a STAC API using R, rstac, and GDAL' #https://stacspec.org/en/tutorials/1-download-data-using-r/ require(glue) require(sf) require(terra) require(rstac) #Access ecoregiosn via VSI epa_l3 <- glue::glue( \"/vsizip/vsicurl/\", #magic remote connection \"https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l3.zip\", #copied link to download location \"/us_eco_l3.shp\") |> #path inside zip file sf::st_read() #Get just S.Rockies and ensure that it is in EPSG:4326 southernRockies <- epa_l3 |> dplyr::filter(US_L3NAME == \"Southern Rockies\") |> dplyr::group_by(US_L3NAME) |> dplyr::summarize(geometry = sf::st_union(geometry)) |> sf::st_transform(\"EPSG:4326\") bboxSR4326 <- sf::st_bbox(southernRockies) # Create a stac query for just the 2021 LCMAP data stac_query <- rstac::stac( \"https://planetarycomputer.microsoft.com/api/stac/v1\" ) |> rstac::stac_search( collections = \"usgs-lcmap-conus-v13\", bbox = bboxSR4326, datetime = \"2021-01-01/2021-12-31\" ) |> rstac::get_request() #A function to get a vsicurl url form a base url make_lcmap_vsicurl_url <- function(base_url) { paste0( \"/vsicurl\", \"?pc_url_signing=yes\", \"&pc_collection=usgs-lcmap-conus-v13\", \"&url=\", base_url ) } lcpri_url <- make_lcmap_vsicurl_url(rstac::assets_url(stac_query, \"lcpri\")) #Pull the file out_file <- tempfile(fileext = \".tif\") sf::gdal_utils( \"warp\", source = lcpri_url, destination = out_file, options = c( \"-t_srs\", sf::st_crs(southernRockies)$wkt, \"-te\", sf::st_bbox(southernRockies) ) ) #Create the raster and plot! terra::rast(out_file) |> terra::plot() southernRockies |> sf::st_geometry() |> plot(lwd = 3, add = TRUE)","title":"Land Change Monitoring, Assessment, and Projection"},{"location":"data-library/modis-vcf/","text":"MODIS Vegetation Continuous Fields (VCF) The MODIS VCF product is derived from the MODIS satellite. The dataset provides proportional estimates of varying cover types. This data is developed from global training data derived using high-resolution imagery. The training data and phenological metrics are used with a regression tree to derive percent cover globally. The model is then used to estimate areal proportions of life form, leaf type, and leaf longevity. MODIS Vegetation Continuous Fields (MOD44B) provides global sub-pixel estimates of three land cover components (percent tree cover; percent non-tree vegetation; and percent non-vegetated) at 250 m spatial resolution. NASA MODIS information here . To accelerate your access to this dataset, the ESIIL team has made MODIS VCF data for the Southern Rockies available on the Cyverse data store at the below directory: ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/modis-vcf Additional MODIS data is best accessed via VSI or STAC.","title":"MODIS Vegetation Continuous Fields (VCF)"},{"location":"data-library/modis-vcf/#modis-vegetation-continuous-fields-vcf","text":"The MODIS VCF product is derived from the MODIS satellite. The dataset provides proportional estimates of varying cover types. This data is developed from global training data derived using high-resolution imagery. The training data and phenological metrics are used with a regression tree to derive percent cover globally. The model is then used to estimate areal proportions of life form, leaf type, and leaf longevity. MODIS Vegetation Continuous Fields (MOD44B) provides global sub-pixel estimates of three land cover components (percent tree cover; percent non-tree vegetation; and percent non-vegetated) at 250 m spatial resolution. NASA MODIS information here . To accelerate your access to this dataset, the ESIIL team has made MODIS VCF data for the Southern Rockies available on the Cyverse data store at the below directory: ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/modis-vcf Additional MODIS data is best accessed via VSI or STAC.","title":"MODIS Vegetation Continuous Fields (VCF)"},{"location":"data-library/mounting-via-vsi/","text":"Mounting data directly from a URL ESIIL, 2024 Tyler McIntosh Data can be directly accessed from where it is hosted on the internet, without the need to download the entire file to your local machine. For spatial data, special protocols from the GDAL library can be used. The first part of enabling remote access is \"vsicurl\". VSI is GDAL's Virtual File System. This is a virtual file system handler allows access to files hosted on remote servers over protocols like HTTP, HTTPS, and FTP. When you prepend \"vsicurl/\" to a URL, GDAL reads the file directly from the remote location without downloading it entirely to the local disk. It's particularly useful for large files, as it only fetches the portions of the file needed for the current operation. The second part of enabling remote access to a zipped file (most large data files hosted online) is \"vsizip\". This is another virtual file system handler in GDAL that enables reading files inside zip archives as if they were unzipped, without the need to extract them manually. By using \"vsizip/\", you can directly access the contents of a zip file. When combined, \"/vsizip/vsicurl/\" allows GDAL (and, subsequently, a package such as 'terra' or 'sf' in R, or similar Python packages) to access files inside of a zip archive on a remote server. The URL following this protocol specifies the remote location of the zip file, and the path after the URL specifies the particular file within the zip archive that you want to access. Example For example, you may have a url to a spatial dataset that you want to use, \"https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l3.zip\". You may have found this link on a website. Figure out your archive contents In order to open a specific file within the zip archive, you need to know the names of the files within the archive. You can either: Download the archive once, view the data structure, and then access it remotely from then on, or, a better solution is to... Access the contents of the zip file using GDAL from a command-line environment To access the contents from a command-line environment, you would use a line of code like this: gdalinfo /vsizip/vsicurl/https://example.com/data.zip Or, in our example: gdalinfo /vsizip/vsicurl/https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l3.zip If you would like to do this without leaving your R or Python environment, you can use R or Python to execute command line calls: R, using \"system\": zip_url = \"/vsizip//vsicurl/https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l3.zip\" system(paste(\"gdalinfo\", zip_url)) Python, using \"subprocess.run\": import subprocess zip_url = \"/vsizip//vsicurl/https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l3.zip\" subprocess.run([\"gdalinfo\", zip_url]) This will tell you that the archive contains several files, one of which is \"us_eco_l3.shp\" - our shapefile of interest. (If there were subdirectories within the directory, repeat the process). Mounting the data We now know the full path to our file of interest: \"/vsizip//vsicurl/https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l3.zip/us_eco_l3.shp\" To mount the data, we simply feed this string to our spatial data package just as we would any other data location. For example, in R, we could do: require(glue) require(sf) epa_l3 <- glue::glue( \"/vsizip/vsicurl/\", #magic remote connection \"https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l3.zip\", #copied link to download location \"/us_eco_l3.shp\") |> #path inside zip file sf::st_read() From this point, we now have the data mounted in our epa_l3 variable, and can manipulate it as usual. Note that, since vsicurl only fetches the portions of the file needed for an operation, the data mounted very quickly. Only once you attempt an operation with the data that requires the entire dataset will it actually fetch the entire dataset!","title":"TUTORIAL Mounting data directly from a URL"},{"location":"data-library/mounting-via-vsi/#mounting-data-directly-from-a-url","text":"ESIIL, 2024 Tyler McIntosh Data can be directly accessed from where it is hosted on the internet, without the need to download the entire file to your local machine. For spatial data, special protocols from the GDAL library can be used. The first part of enabling remote access is \"vsicurl\". VSI is GDAL's Virtual File System. This is a virtual file system handler allows access to files hosted on remote servers over protocols like HTTP, HTTPS, and FTP. When you prepend \"vsicurl/\" to a URL, GDAL reads the file directly from the remote location without downloading it entirely to the local disk. It's particularly useful for large files, as it only fetches the portions of the file needed for the current operation. The second part of enabling remote access to a zipped file (most large data files hosted online) is \"vsizip\". This is another virtual file system handler in GDAL that enables reading files inside zip archives as if they were unzipped, without the need to extract them manually. By using \"vsizip/\", you can directly access the contents of a zip file. When combined, \"/vsizip/vsicurl/\" allows GDAL (and, subsequently, a package such as 'terra' or 'sf' in R, or similar Python packages) to access files inside of a zip archive on a remote server. The URL following this protocol specifies the remote location of the zip file, and the path after the URL specifies the particular file within the zip archive that you want to access.","title":"Mounting data directly from a URL"},{"location":"data-library/mounting-via-vsi/#example","text":"For example, you may have a url to a spatial dataset that you want to use, \"https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l3.zip\". You may have found this link on a website.","title":"Example"},{"location":"data-library/mounting-via-vsi/#figure-out-your-archive-contents","text":"In order to open a specific file within the zip archive, you need to know the names of the files within the archive. You can either: Download the archive once, view the data structure, and then access it remotely from then on, or, a better solution is to... Access the contents of the zip file using GDAL from a command-line environment To access the contents from a command-line environment, you would use a line of code like this: gdalinfo /vsizip/vsicurl/https://example.com/data.zip Or, in our example: gdalinfo /vsizip/vsicurl/https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l3.zip If you would like to do this without leaving your R or Python environment, you can use R or Python to execute command line calls: R, using \"system\": zip_url = \"/vsizip//vsicurl/https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l3.zip\" system(paste(\"gdalinfo\", zip_url)) Python, using \"subprocess.run\": import subprocess zip_url = \"/vsizip//vsicurl/https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l3.zip\" subprocess.run([\"gdalinfo\", zip_url]) This will tell you that the archive contains several files, one of which is \"us_eco_l3.shp\" - our shapefile of interest. (If there were subdirectories within the directory, repeat the process).","title":"Figure out your archive contents"},{"location":"data-library/mounting-via-vsi/#mounting-the-data","text":"We now know the full path to our file of interest: \"/vsizip//vsicurl/https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l3.zip/us_eco_l3.shp\" To mount the data, we simply feed this string to our spatial data package just as we would any other data location. For example, in R, we could do: require(glue) require(sf) epa_l3 <- glue::glue( \"/vsizip/vsicurl/\", #magic remote connection \"https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l3.zip\", #copied link to download location \"/us_eco_l3.shp\") |> #path inside zip file sf::st_read() From this point, we now have the data mounted in our epa_l3 variable, and can manipulate it as usual. Note that, since vsicurl only fetches the portions of the file needed for an operation, the data mounted very quickly. Only once you attempt an operation with the data that requires the entire dataset will it actually fetch the entire dataset!","title":"Mounting the data"},{"location":"data-library/move-data-to-instance/","text":"Moving data to your instance from the data store Some data has been pre-downloaded for you and stored on the CyVerse data store in order to help expedite your projects. While you CAN access that data directly on the data store, it is HIGHLY recommended that you copy the data over to your instance (see \"Cyverse data management\" under \"Collaborating on the cloud\" for more information). This is because your work with the data will be dramatically faster with it located on your instance. Take, for instance, the treemap data. If we load and plot the data without moving it, it takes just a few seconds (i.e. ~2.973 seconds). Not bad. require(terra) require(tictoc) tictoc::tic() treemap <- terra::rast(\"~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/TreeMap/treemap2016_southernrockies.tif\") terra::plot(treemap) tictoc::toc() However, if we load and plot the data after moving it, it takes less than a second (i.e. ~0.302 seconds). Even better! This 10x increase in speed will add up incredibly quickly as soon as you start working more intensively with the data. require(terra) require(tictoc) system(\"cp -r ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/TreeMap ~/TreeMap\") #move the data first!! tictoc::tic() treemap <- terra::rast(\"~/TreeMap/treemap2016_southernrockies.tif\") terra::plot(treemap) tictoc::toc() Takeaway: seriously, just copy the data over.","title":"TUTORIAL Moving data to your instance from the data store"},{"location":"data-library/move-data-to-instance/#moving-data-to-your-instance-from-the-data-store","text":"Some data has been pre-downloaded for you and stored on the CyVerse data store in order to help expedite your projects. While you CAN access that data directly on the data store, it is HIGHLY recommended that you copy the data over to your instance (see \"Cyverse data management\" under \"Collaborating on the cloud\" for more information). This is because your work with the data will be dramatically faster with it located on your instance. Take, for instance, the treemap data. If we load and plot the data without moving it, it takes just a few seconds (i.e. ~2.973 seconds). Not bad. require(terra) require(tictoc) tictoc::tic() treemap <- terra::rast(\"~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/TreeMap/treemap2016_southernrockies.tif\") terra::plot(treemap) tictoc::toc() However, if we load and plot the data after moving it, it takes less than a second (i.e. ~0.302 seconds). Even better! This 10x increase in speed will add up incredibly quickly as soon as you start working more intensively with the data. require(terra) require(tictoc) system(\"cp -r ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/TreeMap ~/TreeMap\") #move the data first!! tictoc::tic() treemap <- terra::rast(\"~/TreeMap/treemap2016_southernrockies.tif\") terra::plot(treemap) tictoc::toc() Takeaway: seriously, just copy the data over.","title":"Moving data to your instance from the data store"},{"location":"data-library/stac_mount_save/","text":"The art of making a data cube Ty Tuff, ESIIL Data Scientist 2023-10-27 #library(Rcpp) library(sf) library(gdalcubes) library(rstac) library(gdalUtils) library(terra) library(rgdal) library(reshape2) library(osmdata) library(terra) library(dplyr) library(stars) library(ggplot2) library(colorspace) library(geos) library(osmdata) library(ggthemes) library(tidyr) gdalcubes_options(parallel = 8) sf::sf_extSoftVersion() ## GEOS GDAL proj.4 GDAL_with_GEOS USE_PROJ_H ## \"3.11.0\" \"3.5.3\" \"9.1.0\" \"true\" \"true\" ## PROJ ## \"9.1.0\" gdalcubes_gdal_has_geos() ## [1] TRUE library(osmdata) library(dplyr) library(sf) library(terra) library(tidyterra) library(glue) library(ggplot2) library(ggthemes) library(stars) library(magrittr) library(landsat) The philosophy of moving data in the cloud The philosophy of moving data in the cloud represents a paradigm shift in how we approach data within our analytical processes. Instead of the traditional method of transferring entire datasets to our local environments, the cloud encourages a more efficient model: bring your analysis to the data. This approach minimizes data movement and leverages the cloud\u2019s computational power and scalability. By utilizing cloud-native tools and services, we can run our analyses directly on the data where it resides, selectively accessing and processing only what is necessary. This not only streamlines workflows but also significantly reduces overheads related to data transfer and storage management. In essence, the focus is on diverting computational resources to the data rather than the cumbersome and resource-intensive practice of moving large datasets to and fro. \u2018To Make\u2019 or \u2018To Take\u2019 a photo The distinction between making and taking a photograph lies in the approach and intent behind the camera. Taking a photo is often a reactive process, where the photographer captures moments as they naturally unfold, seizing the spontaneity of life without alteration. It\u2019s a passive form of photography where the emphasis is on the right timing and the natural interplay of elements within the frame. On the other hand, making a photo is a proactive and deliberate act. It is akin to craftsmanship, where a professional photographer starts with a concept and utilizes a variety of tools and techniques to stage and construct the desired scene. They actively manipulate lighting, composition, and subjects to create a photograph that aligns with their pre-visualized artistic vision. While both methods use a camera to produce a photograph, making a photo involves a creation process, whereas taking a photo is about finding the scene. David Yarrow is a famous photographer who \u2018makes\u2019 his photographs. What does it mean to \u2018make\u2019 a data cube? The artistry of Ansel Adams\u2019 photography serves as a compelling analogy for the meticulous craft of building a data cube from cloud data sources using tools like STAC and GDAL VSI. Just as Adams would survey the vastness of a landscape, discerning the interplay of light and shadow upon the mountains before him, a data architect surveys the expanse of available data. In this analogy, the raw data are the majestic mountains and sweeping landscapes waiting to be captured. The STAC collection acts as the photographer\u2019s deliberate choice of scene, pointing the camera lens\u2014our data tools\u2014towards the most telling and coherent dataset. Just as Adams\u2019 photographs are more than mere records of a landscape, but rather a confluence of his vision, technique, and the scene\u2019s natural beauty, so too is the data cube more than the sum of its parts. It is the artful synthesis of information, crafted and composed with the skill and intent of an artist, producing not just a tool for analysis but a harmonized, data-driven portrait of the world it represents. The builder of the data cube is, indeed, an artist, and the data cube their masterpiece, revealing not just data, but a story, a perspective, a landscape sewn from the raw material of cloud-sourced information. As Adams would adjust his viewfinder, setting the boundaries of his photographic frame, the data builder sets the view window, filtering and transferring relevant data to their own medium, akin to Adams\u2019 film. This is where the raw data is transformed, organized into the structured form of a data frame or data cube, a process not unlike the careful development of a photograph in a darkroom. Here, the data cube creator, much like Adams with his careful dodging and burning, harmonizes disparate elements into a cohesive whole, each decision reflecting an intention and vision for the final product. 1) The Rat through the Snake Problem: Scalability with Cloud Computing Just like a snake that swallows a rat, traditional computing systems often struggle to process the large volumes of environmental data \u2014 they\u2019re constrained by their static hardware limitations. Cloud computing introduces a python-esque capability: massive scalability. By migrating to the cloud, we essentially make the snake bigger, allowing it to handle larger \u201cprey.\u201d Scalable computers in the cloud can grow with the demand, providing the necessary computational power to process extensive datasets, which is vital in a field where data volumes are increasing exponentially. 2) The Antelope through the Python Problem: Streamlining with GDAL VSI As we scale up, we encounter a new challenge: trying to pass an antelope through a python \u2014 a metaphor for the next level of complexity in data processing. The sheer size and complexity of the data can become overwhelming. This is where GDAL\u2019s Virtual File System (VSI) becomes our ecological adaptation. VSI allows us to access remote data transparently and more efficiently. Instead of ingesting the entire \u201cantelope,\u201d VSI enables the \u201cpython\u201d to dynamically access and process only the parts of the data it needs, when it needs them, much like constriction before digestion. This selective access minimizes the need for local storage and expedites the data handling process. 3) Drinking from a Fire Hose: Accelerated Inference with AI and ML Once we\u2019ve enabled the flow of large amounts of digestible data, we encounter the metaphorical challenge of drinking from a fire hose. The data, now flowing and accessible, is immense and rapid \u2014 posing a challenge not just to store and process, but to understand and derive meaning from in real-time. This is where artificial intelligence (AI) and machine learning (ML) step in. These technologies act as a sophisticated filtration system, enabling us to drink safely and beneficially from the torrent. AI and ML can analyze patterns, make predictions, and infer insights at a pace that keeps up with the fast stream of data, turning raw information into actionable knowledge. By addressing these three pivotal challenges with cloud computing, GDAL VSI, and AI/ML, we not only manage to consume the data effectively but also transform our capabilities in environmental data science. We can move from mere data ingestion to meaningful data interpretation, all at a scale and speed necessary for impactful environmental analysis. Mounting data A void-filled Digital Elevation Model (DEM) is a comprehensive topographical representation where any missing data points, known as voids, have been filled in. These voids can occur due to various reasons, such as clouds or technical errors during data collection. In a void-filled DEM, these gaps are interpolated or estimated using the surrounding data to create a continuous, seamless surface model. This process enhances the utility and accuracy of the DEM for hydrological modeling, terrain analysis, and other geographical applications. The HydroSHEDS website (https://www.hydrosheds.org/hydrosheds-core-downloads) provides access to high-quality, void-filled DEM datasets like the DEM_continuous_CONUS_15s, which users can download and easily integrate into spatial analysis workflows using tools such as \u2018terra\u2019 in R, allowing for sophisticated environmental and geographical research and planning. # Record start time a <- Sys.time() # Create a string with the file path using glue, then download and read the DEM file as a raster object DEM_continuous_CONUS_15s <- glue( \"/vsizip/vsicurl/\", #magic remote connection \"https://data.hydrosheds.org/file/hydrosheds-v1-dem/hyd_na_dem_15s.zip\", #copied link to download location \"/hyd_na_dem_15s.tif\") %>% #path inside zip file terra::rast() # The 'glue' function constructs the file path string, which is then passed to 'terra::rast()' to read the DEM file into R as a raster layer. '/vsizip/vsicurl/' is a special GDAL virtual file system syntax that allows reading directly from a zipped file on a remote server. # Record end time and calculate the time difference b <- Sys.time() difftime(b, a) ## Time difference of 4.603666 secs # The resulting raster object is stored in 'DEM_continuous_CONUS_15s', which now contains the void-filled DEM data ready for use DEM_continuous_CONUS_15s # Prints out the details of the 'DEM_continuous_CONUS_15s' raster object ## class : SpatRaster ## dimensions : 13920, 20640, 1 (nrow, ncol, nlyr) ## resolution : 0.004166667, 0.004166667 (x, y) ## extent : -138, -52, 5, 63 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## source : hyd_na_dem_15s.tif ## name : Band_1 # output is a SpatRaster, which is the object type associated with the 'terra' package. Continuous DEM for North America # Record start time a <- Sys.time() ggplot() + geom_spatraster(data=DEM_continuous_CONUS_15s) + theme_tufte() b <- Sys.time() difftime(b, a) ## Time difference of 52.49061 secs Calculate Slope from that DEM SLOPE_continuous_CONUS_15s <- terra::terrain(DEM_continuous_CONUS_15s, \"slope\") SLOPE_continuous_CONUS_15s ## class : SpatRaster ## dimensions : 13920, 20640, 1 (nrow, ncol, nlyr) ## resolution : 0.004166667, 0.004166667 (x, y) ## extent : -138, -52, 5, 63 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## source(s) : memory ## name : slope ## min value : 0.00000 ## max value : 56.98691 # Record start time a <- Sys.time() ggplot() + geom_spatraster(data=SLOPE_continuous_CONUS_15s) + theme_tufte() b <- Sys.time() difftime(b, a) ## Time difference of 3.859545 secs Calculate aspect from DEM ASPECT_continuous_CONUS_15s <- terra::terrain(DEM_continuous_CONUS_15s, \"aspect\") ASPECT_continuous_CONUS_15s ## class : SpatRaster ## dimensions : 13920, 20640, 1 (nrow, ncol, nlyr) ## resolution : 0.004166667, 0.004166667 (x, y) ## extent : -138, -52, 5, 63 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## source(s) : memory ## name : aspect ## min value : 0 ## max value : 360 # Record start time a <- Sys.time() ggplot() + geom_spatraster(data=ASPECT_continuous_CONUS_15s) + theme_tufte() b <- Sys.time() difftime(b, a) ## Time difference of 3.650267 secs Create a cube from those layers! mini_stack <- c(DEM_continuous_CONUS_15s, SLOPE_continuous_CONUS_15s,ASPECT_continuous_CONUS_15s) mini_stack ## class : SpatRaster ## dimensions : 13920, 20640, 3 (nrow, ncol, nlyr) ## resolution : 0.004166667, 0.004166667 (x, y) ## extent : -138, -52, 5, 63 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## sources : hyd_na_dem_15s.tif ## memory ## memory ## names : Band_1, slope, aspect ## min values : ? , 0.00000, 0 ## max values : ? , 56.98691, 360 Reproject and return the bounding box coordinates for our Area of Interest # Transform the filtered geometry to EPSG:4326 and store its bounding box # Record start time a <- Sys.time() DEM_continuous_CONUS_15s |> stars::st_as_stars() |> st_transform(\"EPSG:4326\") |> st_bbox() -> bbox_4326 DEM_continuous_CONUS_15s |> stars::st_as_stars() |> st_transform(\"EPSG:32618\") |> st_bbox() -> bbox_32618 b <- Sys.time() difftime(b, a) ## Time difference of 3.7653 mins Get a polygon for Boulder County, reproject, and return bounding box. This is so I can make a smaller search in the stac catalog. boulder_county <- getbb(\"boulder, co\", format_out=\"sf_polygon\") boulder_county$multipolygon |> st_transform(crs =4326 ) |> st_bbox() -> bbox_4326_boulder boulder_county$multipolygon |> st_transform(crs =32720 ) |> st_bbox() -> bbox_32720_boulder Get a polygon for the United States and crop it to be the same size as the DEM above. aoi <- getbb(\"United States\", format_out=\"sf_polygon\") conus <- aoi$multipolygon |> st_crop(bbox_4326) ggplot(data=conus) + geom_sf() Search the Stac catalog. STAC, or SpatioTemporal Asset Catalog, is an open-source specification designed to standardize the way geospatial data is indexed and discovered. Developed by Element 84 among others, it facilitates better interoperability and sharing of geospatial assets by providing a common language for describing them. STAC\u2019s flexible design allows for easy cataloging of data, making it simpler for individuals and systems to search and retrieve geospatial information. By effectively organizing data about the Earth\u2019s spatial and temporal characteristics, STAC enables users to harness the full power of the cloud and modern data processing technologies, optimizing the way we access and analyze environmental data on a global scale. stac(\"https://earth-search.aws.element84.com/v1\") |> get_request() ## ###STACCatalog ## - id: earth-search-aws ## - description: A STAC API of public datasets on AWS ## - field(s): stac_version, type, id, title, description, links, conformsTo Element 84\u2019s Earth Search is a STAC compliant search and discovery API that offers users access to a vast collection of geospatial open datasets hosted on AWS. It serves as a centralized search catalog providing standardized metadata for these open datasets, designed to be freely used and integrated into various applications. Alongside the API, Element 84 also provides a web application named Earth Search Console, which is map-centric and allows users to explore and visualize the data contained within the Earth Search API\u2019s catalog. This suite of tools is part of Element 84\u2019s initiative to make geospatial data more accessible and actionable for a wide range of users and applications. collection_formats() ## CHIRPS_v2_0_daily_p05_tif | Image collection format for CHIRPS v 2.0 daily ## | global precipitation dataset (0.05 degrees ## | resolution) from GeoTIFFs, expects list of .tif ## | or .tif.gz files as input. [TAGS: CHIRPS, ## | precipitation] ## CHIRPS_v2_0_monthly_p05_tif | Image collection format for CHIRPS v 2.0 monthly ## | global precipitation dataset (0.05 degrees ## | resolution) from GeoTIFFs, expects list of .tif ## | or .tif.gz files as input. [TAGS: CHIRPS, ## | precipitation] ## ESA_CCI_SM_ACTIVE | Collection format for ESA CCI soil moisture ## | active product (version 4.7) [TAGS: Soil ## | Moisture, ESA, CCI] ## ESA_CCI_SM_PASSIVE | Collection format for ESA CCI soil moisture ## | passive product (version 4.7) [TAGS: Soil ## | Moisture, ESA, CCI] ## GPM_IMERG_3B_DAY_GIS_V06A | Collection format for daily ## | IMERG_3B_DAY_GIS_V06A data [TAGS: Precipitation, ## | GPM, IMERG] ## L8_L1TP | Collection format for Landsat 8 Level 1 TP ## | product [TAGS: Landsat, USGS, Level 1, NASA] ## L8_SR | Collection format for Landsat 8 surface ## | reflectance product [TAGS: Landsat, USGS, Level ## | 2, NASA, surface reflectance] ## MAXAR | Preliminary collection format for MAXAR open ## | data, visual only (under development) [TAGS: ] ## MxD09GA | Collection format for selected bands from the ## | MODIS MxD09GA (Aqua and Terra) product [TAGS: ## | MODIS, surface reflectance] ## MxD10A2 | Collection format for selected bands from the ## | MODIS MxD10A2 (Aqua and Terra) v006 Snow Cover ## | product [TAGS: MODIS, Snow Cover] ## MxD11A1 | Collection format for selected bands from the ## | MODIS MxD11A2 (Aqua and Terra) v006 Land Surface ## | Temperature product [TAGS: MODIS, LST] ## MxD11A2 | Collection format for selected bands from the ## | MODIS MxD11A2 (Aqua and Terra) v006 Land Surface ## | Temperature product [TAGS: MODIS, LST] ## MxD13A2 | Collection format for selected bands from the ## | MODIS MxD13A2 (Aqua and Terra) product [TAGS: ## | MODIS, VI, NDVI, EVI] ## MxD13A3 | Collection format for selected bands from the ## | MODIS MxD13A3 (Aqua and Terra) product [TAGS: ## | MODIS, VI, NDVI, EVI] ## MxD13Q1 | Collection format for selected bands from the ## | MODIS MxD13Q1 (Aqua and Terra) product [TAGS: ## | MODIS, VI, NDVI, EVI] ## MxD14A2 | Collection format for the MODIS MxD14A2 (Aqua ## | and Terra) product [TAGS: MODIS, Fire] ## PlanetScope_3B_AnalyticMS_SR | Image collection format for PlanetScope 4-band ## | scenes [TAGS: PlanetScope, BOA, Surface ## | Reflectance] ## Sentinel2_L1C | Image collection format for Sentinel 2 Level 1C ## | data as downloaded from the Copernicus Open ## | Access Hub, expects a list of file paths as ## | input. The format works on original ZIP ## | compressed as well as uncompressed imagery. ## | [TAGS: Sentinel, Copernicus, ESA, TOA] ## Sentinel2_L1C_AWS | Image collection format for Sentinel 2 Level 1C ## | data in AWS [TAGS: Sentinel, Copernicus, ESA, ## | TOA] ## Sentinel2_L2A | Image collection format for Sentinel 2 Level 2A ## | data as downloaded from the Copernicus Open ## | Access Hub, expects a list of file paths as ## | input. The format should work on original ZIP ## | compressed as well as uncompressed imagery. ## | [TAGS: Sentinel, Copernicus, ESA, BOA, Surface ## | Reflectance] ## Sentinel2_L2A_THEIA | Image collection format for Sentinel 2 Level 2A ## | data as downloaded from Theia. [TAGS: Sentinel, ## | ESA, Flat Reflectance, Theia] Building a stac collection by aiming your camera at the landscape Creating a STAC collection is akin to a photographer framing a shot; the landscape is rich with diverse data, mirroring a scene bustling with potential subjects, colors, and light. Just as a photographer selects a portion of the vista to capture, focusing on elements that will compose a compelling image, a data scientist must similarly navigate the vast data terrain. They must \u2018point their camera\u2019 judiciously, ensuring that the \u2018frame\u2019 encapsulates the precise data needed. This careful selection is crucial, as it determines the relevance and quality of the data collection, much like the photographer\u2019s choice dictates the story a photograph will tell. # Record start time a <- Sys.time() # Initialize STAC connection s = stac(\"https://earth-search.aws.element84.com/v0\") # Search for Sentinel-2 images within specified bounding box and date range #22 Million items items = s |> stac_search(collections = \"sentinel-s2-l2a-cogs\", bbox = c(bbox_4326_boulder[\"xmin\"], bbox_4326_boulder[\"ymin\"], bbox_4326_boulder[\"xmax\"], bbox_4326_boulder[\"ymax\"]), datetime = \"2021-05-15/2021-05-16\") |> post_request() |> items_fetch(progress = FALSE) # Print number of found items length(items$features) ## [1] 1 # Prepare the assets for analysis library(gdalcubes) assets = c(\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\", \"SCL\") s2_collection = stac_image_collection(items$features, asset_names = assets, property_filter = function(x) {x[[\"eo:cloud_cover\"]] < 20}) #all images with less than 20% clouds b <- Sys.time() difftime(b, a) ## Time difference of 0.4706092 secs # Display the image collection s2_collection ## Image collection object, referencing 1 images with 13 bands ## Images: ## name left top bottom right ## 1 S2B_13TDE_20210516_0_L2A -106.1832 40.65079 39.65576 -104.8846 ## datetime srs ## 1 2021-05-16T18:02:54 EPSG:32613 ## ## Bands: ## name offset scale unit nodata image_count ## 1 B01 0 1 1 ## 2 B02 0 1 1 ## 3 B03 0 1 1 ## 4 B04 0 1 1 ## 5 B05 0 1 1 ## 6 B06 0 1 1 ## 7 B07 0 1 1 ## 8 B08 0 1 1 ## 9 B09 0 1 1 ## 10 B11 0 1 1 ## 11 B12 0 1 1 ## 12 B8A 0 1 1 ## 13 SCL 0 1 1 Setting up your camera and film The camera through which the data scientist frames the shot is multifaceted, akin to the tools and processes they employ. The camera\u2019s film, analogous to the data cube, defines the resolution and dimensions of the captured data, shaping how the final dataset will be utilized. The lens and its settings\u2014focus, aperture, and exposure\u2014determine the clarity, depth, and breadth of the captured information, much like the algorithms and parameters set by the data scientist dictate the granularity and scope of the data cube. The flash, like data enhancement techniques, can illuminate hidden details, ensuring that the data cube, the final product, is as informative and accurate as the landscape it represents. # Record start time a <- Sys.time() # Define a specific view on the satellite image collection v = cube_view( srs = \"EPSG:32720\", #this is harder than expected. dx = 100, dy = 100, dt = \"P1M\", aggregation = \"median\", resampling = \"near\", extent = list( t0 = \"2021-05-15\", t1 = \"2021-05-16\", left = bbox_32720_boulder[1], right = bbox_32720_boulder[2], top = bbox_32720_boulder[4], bottom = bbox_32720_boulder[3] ) ) b <- Sys.time() difftime(b, a) ## Time difference of 0.002738953 secs # Display the defined view v ## A data cube view object ## ## Dimensions: ## low high count pixel_size ## t 2021-05-01 2021-05-31 1 P1M ## y -3103099.52398788 15434400.4760121 185375 100 ## x -3178878.98542359 15369521.0145764 185484 100 ## ## SRS: \"EPSG:32720\" ## Temporal aggregation method: \"median\" ## Spatial resampling method: \"near\" Take a picture! Raster style # Record start time a <- Sys.time() s2_collection |> raster_cube(v) |> select_bands(c( \"B04\", \"B05\")) |> apply_pixel(c(\"(B05-B04)/(B05+B04)\"), names=\"NDVI\") |> write_tif() |> raster::stack() -> x x ## class : RasterStack ## dimensions : 185375, 185484, 34384096500, 1 (nrow, ncol, ncell, nlayers) ## resolution : 100, 100 (x, y) ## extent : -3178879, 15369521, -3103100, 15434400 (xmin, xmax, ymin, ymax) ## crs : +proj=utm +zone=20 +south +datum=WGS84 +units=m +no_defs ## names : NDVI b <- Sys.time() difftime(b, a) ## Time difference of 4.132932 mins STARS style # Record start time a <- Sys.time() s2_collection |> raster_cube(v) |> select_bands(c(\"B04\",\"B05\")) |> apply_pixel(c(\"(B05-B04)/(B05+B04)\"), names=\"NDVI\") |> stars::st_as_stars() -> y b <- Sys.time() difftime(b, a) ## Time difference of 1.459866 mins y ## stars_proxy object with 1 attribute in 1 file(s): ## $NDVI ## [1] \"[...]/filec5982c38536c.nc:NDVI\" ## ## dimension(s): ## from to offset delta refsys point ## x 1 185484 -3178879 100 WGS 84 / UTM zone 20S NA ## y 1 185375 15434400 -100 WGS 84 / UTM zone 20S NA ## time 1 1 NA NA POSIXct FALSE ## values x/y ## x NULL [x] ## y NULL [y] ## time [2021-05-01,2021-06-01) Extract data # Record start time a <- Sys.time() x <- s2_collection |> raster_cube(v) |> select_bands(c(\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\")) |> extract_geom(boulder_county$multipolygon) |> rename( \"time\" = \"time\", \"443\" = \"B01\", \"490\" = \"B02\", \"560\" = \"B03\", \"665\" = \"B04\", \"705\" = \"B05\", \"740\" = \"B06\", \"783\" = \"B07\", \"842\" = \"B08\", \"865\" = \"B8A\", \"940\" = \"B09\", \"1610\" = \"B11\", \"2190\" = \"B12\" ) b <- Sys.time() difftime(b, a) ## Time difference of 1.699016 mins head(x) ## FID time 443 490 560 665 705 740 783 842 865 940 1610 ## 1 1 2021-05-01 11096 10929 10224 9893 9956 9706 9715 9641 9511 8459 5682 ## 2 1 2021-05-01 11631 11282 10550 10234 10288 10031 10032 9988 9828 9153 5802 ## 3 1 2021-05-01 11900 11393 10666 10337 10398 10142 10138 10093 9927 9461 5754 ## 4 1 2021-05-01 11406 10597 9928 9626 9694 9481 9516 9338 9336 8959 5726 ## 5 1 2021-05-01 11399 10939 10237 9905 9978 9738 9746 9633 9555 8925 5831 ## 6 1 2021-05-01 11600 11174 10462 10147 10209 9952 9960 9890 9760 9153 5773 ## 2190 ## 1 3917 ## 2 3981 ## 3 3937 ## 4 4054 ## 5 4097 ## 6 3990 Make a timeseries # Record start time a <- Sys.time() items <- s |> stac_search(collections = \"sentinel-s2-l2a-cogs\", bbox = c(-105.694362, 39.912886, -105.052774, 40.262785), datetime = \"2020-01-01/2022-12-31\", limit = 500) %>% post_request() S2.mask = image_mask(\"SCL\", values=c(3,8,9)) col = stac_image_collection(items$features, asset_names = assets, property_filter = function(x) {x[[\"eo:cloud_cover\"]] < 30}) v = cube_view(srs = \"EPSG:4326\", extent = list(t0 = \"2020-01-01\", t1 = \"2022-12-31\", left = -105.694362, right = -105.052774, top = 40.262785, bottom = 39.912886), dx = 0.001, dy = 0.001, dt = \"P1M\", aggregation = \"median\", resampling = \"bilinear\") library(colorspace) ndvi.col = function(n) { rev(sequential_hcl(n, \"Green-Yellow\")) } library(gdalcubes) raster_cube(col, v, mask = S2.mask) |> select_bands(c(\"B04\", \"B08\")) |> apply_pixel(\"(B08-B04)/(B08+B04)\", \"NDVI\") |> gdalcubes::animate(col = ndvi.col, zlim=c(-0.2,1), key.pos = 1, save_as = \"anim.gif\", fps = 4) ## [1] \"/Users/ty/Documents/Github/hackathon2023_datacube/docs/code_for_building_cube/anim.gif\" b <- Sys.time() difftime(b, a) ## Time difference of 4.716672 mins y ## stars_proxy object with 1 attribute in 1 file(s): ## $NDVI ## [1] \"[...]/filec5982c38536c.nc:NDVI\" ## ## dimension(s): ## from to offset delta refsys point ## x 1 185484 -3178879 100 WGS 84 / UTM zone 20S NA ## y 1 185375 15434400 -100 WGS 84 / UTM zone 20S NA ## time 1 1 NA NA POSIXct FALSE ## values x/y ## x NULL [x] ## y NULL [y] ## time [2021-05-01,2021-06-01) Saving Data Cubes to Local Storage There are occasions when we need to manipulate data cubes using other software. For such purposes, we can save data cubes to our local disk as individual netCDF files or as a series of GeoTIFF files. In the case of the latter, each temporal segment of the cube is saved as a separate (multiband) GeoTIFF file. Both netCDF and GeoTIFF formats allow for file size reduction through compression and data packing. This process involves transforming double precision numbers into smaller integer values using a scale and offset, which can be particularly useful for managing disk space (for more details, refer to the ?write_ncdf and ?write_tif documentation). gdalcubes_options(ncdf_compression_level = 1) write_ncdf(cube, file.path(\"~/Desktop\", basename(tempfile(fileext = \".nc\")))) gdalcubes_options(ncdf_compression_level = 0) write_tif() and write_ncdf() both return the path(s) to created file(s) as a character vector. items_2020 <- s |> stac_search(collections = \"sentinel-s2-l2a-cogs\", bbox = c(-105.694362, 39.912886, -105.052774, 40.262785), datetime = \"2020-05-01/2020-06-30\") |> post_request() items_2021 <- s |> stac_search(collections = \"sentinel-s2-l2a-cogs\", bbox = c(-105.694362, 39.912886, -105.052774, 40.262785), datetime = \"2021-05-01/2021-06-30\") |> post_request() col_2020 = stac_image_collection(items_2020$features, asset_names = assets) col_2021 = stac_image_collection(items_2021$features, asset_names = assets) v_2020 = cube_view(srs = \"EPSG:32720\", extent = list(t0 = \"2020-05-01\", t1 = \"2020-06-30\", left = bbox_32720_boulder[\"xmin\"], right = bbox_32720_boulder[\"xmax\"], top = bbox_32720_boulder[\"ymax\"], bottom = bbox_32720_boulder[\"ymin\"]), dx = 100, dy = 100, dt = \"P1D\", aggregation = \"median\", resampling = \"bilinear\") v_2021 = cube_view(v_2020, extent = list(t0 = \"2021-05-01\", t1 = \"2021-06-30\")) max_ndvi_mosaic <- function(col, v) { raster_cube(col, v) |> select_bands(c(\"B04\", \"B08\")) |> apply_pixel(c(\"(B08-B04)/(B08+B04)\"), names=\"NDVI\") |> reduce_time(\"max(NDVI)\") } suppressPackageStartupMessages(library(stars)) max_ndvi_mosaic(col_2020, v_2020) -> maxndvi_2020 max_ndvi_mosaic(col_2021, v_2021) -> maxndvi_2021 maxndvi_2021 maxndvi_2020 difference = maxndvi_2021 - maxndvi_2020 difference[difference > -0.15] = NA names(difference) <- \"Difference of max NDVI (2020 - 2019)\" flood_polygon_data3 <- glue(\"/vsizip/vsicurl/https://data.hydrosheds.org/file/hydrosheds-associated/gloric/GloRiC_v10_shapefile.zip/GloRiC_v10_shapefile/GloRiC_v10.shp\") |> st_read() |> st_as_sf(coords = c(\"lon\",\"lat\")) flood_polygon_data3 #st_read(\"/Users/ty/Downloads/GloRiC_v10_geodatabase/GloRiC_v10.gdb\") flood_polygon_data3 <- glue(\"/vsizip/vsicurl/https://data.hydrosheds.org/file/hydrosheds-associated/gloric/GloRiC_v10_geodatabase.zip/GloRiC_v10_geodatabase/GloRiC_v10.gdb\") |> st_read() |> st_as_sf(coords = c(\"lon\",\"lat\")) flood_polygon_data3","title":"TUTORIAL STAC & VSI"},{"location":"data-library/stac_mount_save/#the-art-of-making-a-data-cube","text":"Ty Tuff, ESIIL Data Scientist 2023-10-27 #library(Rcpp) library(sf) library(gdalcubes) library(rstac) library(gdalUtils) library(terra) library(rgdal) library(reshape2) library(osmdata) library(terra) library(dplyr) library(stars) library(ggplot2) library(colorspace) library(geos) library(osmdata) library(ggthemes) library(tidyr) gdalcubes_options(parallel = 8) sf::sf_extSoftVersion() ## GEOS GDAL proj.4 GDAL_with_GEOS USE_PROJ_H ## \"3.11.0\" \"3.5.3\" \"9.1.0\" \"true\" \"true\" ## PROJ ## \"9.1.0\" gdalcubes_gdal_has_geos() ## [1] TRUE library(osmdata) library(dplyr) library(sf) library(terra) library(tidyterra) library(glue) library(ggplot2) library(ggthemes) library(stars) library(magrittr) library(landsat)","title":"The art of making a data cube"},{"location":"data-library/stac_mount_save/#the-philosophy-of-moving-data-in-the-cloud","text":"The philosophy of moving data in the cloud represents a paradigm shift in how we approach data within our analytical processes. Instead of the traditional method of transferring entire datasets to our local environments, the cloud encourages a more efficient model: bring your analysis to the data. This approach minimizes data movement and leverages the cloud\u2019s computational power and scalability. By utilizing cloud-native tools and services, we can run our analyses directly on the data where it resides, selectively accessing and processing only what is necessary. This not only streamlines workflows but also significantly reduces overheads related to data transfer and storage management. In essence, the focus is on diverting computational resources to the data rather than the cumbersome and resource-intensive practice of moving large datasets to and fro.","title":"The philosophy of moving data in the cloud"},{"location":"data-library/stac_mount_save/#to-make-or-to-take-a-photo","text":"The distinction between making and taking a photograph lies in the approach and intent behind the camera. Taking a photo is often a reactive process, where the photographer captures moments as they naturally unfold, seizing the spontaneity of life without alteration. It\u2019s a passive form of photography where the emphasis is on the right timing and the natural interplay of elements within the frame. On the other hand, making a photo is a proactive and deliberate act. It is akin to craftsmanship, where a professional photographer starts with a concept and utilizes a variety of tools and techniques to stage and construct the desired scene. They actively manipulate lighting, composition, and subjects to create a photograph that aligns with their pre-visualized artistic vision. While both methods use a camera to produce a photograph, making a photo involves a creation process, whereas taking a photo is about finding the scene. David Yarrow is a famous photographer who \u2018makes\u2019 his photographs.","title":"\u2018To Make\u2019 or \u2018To Take\u2019 a photo"},{"location":"data-library/stac_mount_save/#what-does-it-mean-to-make-a-data-cube","text":"The artistry of Ansel Adams\u2019 photography serves as a compelling analogy for the meticulous craft of building a data cube from cloud data sources using tools like STAC and GDAL VSI. Just as Adams would survey the vastness of a landscape, discerning the interplay of light and shadow upon the mountains before him, a data architect surveys the expanse of available data. In this analogy, the raw data are the majestic mountains and sweeping landscapes waiting to be captured. The STAC collection acts as the photographer\u2019s deliberate choice of scene, pointing the camera lens\u2014our data tools\u2014towards the most telling and coherent dataset. Just as Adams\u2019 photographs are more than mere records of a landscape, but rather a confluence of his vision, technique, and the scene\u2019s natural beauty, so too is the data cube more than the sum of its parts. It is the artful synthesis of information, crafted and composed with the skill and intent of an artist, producing not just a tool for analysis but a harmonized, data-driven portrait of the world it represents. The builder of the data cube is, indeed, an artist, and the data cube their masterpiece, revealing not just data, but a story, a perspective, a landscape sewn from the raw material of cloud-sourced information. As Adams would adjust his viewfinder, setting the boundaries of his photographic frame, the data builder sets the view window, filtering and transferring relevant data to their own medium, akin to Adams\u2019 film. This is where the raw data is transformed, organized into the structured form of a data frame or data cube, a process not unlike the careful development of a photograph in a darkroom. Here, the data cube creator, much like Adams with his careful dodging and burning, harmonizes disparate elements into a cohesive whole, each decision reflecting an intention and vision for the final product.","title":"What does it mean to \u2018make\u2019 a data cube?"},{"location":"data-library/stac_mount_save/#1-the-rat-through-the-snake-problem-scalability-with-cloud-computing","text":"Just like a snake that swallows a rat, traditional computing systems often struggle to process the large volumes of environmental data \u2014 they\u2019re constrained by their static hardware limitations. Cloud computing introduces a python-esque capability: massive scalability. By migrating to the cloud, we essentially make the snake bigger, allowing it to handle larger \u201cprey.\u201d Scalable computers in the cloud can grow with the demand, providing the necessary computational power to process extensive datasets, which is vital in a field where data volumes are increasing exponentially.","title":"1) The Rat through the Snake Problem: Scalability with Cloud Computing"},{"location":"data-library/stac_mount_save/#2-the-antelope-through-the-python-problem-streamlining-with-gdal-vsi","text":"As we scale up, we encounter a new challenge: trying to pass an antelope through a python \u2014 a metaphor for the next level of complexity in data processing. The sheer size and complexity of the data can become overwhelming. This is where GDAL\u2019s Virtual File System (VSI) becomes our ecological adaptation. VSI allows us to access remote data transparently and more efficiently. Instead of ingesting the entire \u201cantelope,\u201d VSI enables the \u201cpython\u201d to dynamically access and process only the parts of the data it needs, when it needs them, much like constriction before digestion. This selective access minimizes the need for local storage and expedites the data handling process.","title":"2) The Antelope through the Python Problem: Streamlining with GDAL VSI"},{"location":"data-library/stac_mount_save/#3-drinking-from-a-fire-hose-accelerated-inference-with-ai-and-ml","text":"Once we\u2019ve enabled the flow of large amounts of digestible data, we encounter the metaphorical challenge of drinking from a fire hose. The data, now flowing and accessible, is immense and rapid \u2014 posing a challenge not just to store and process, but to understand and derive meaning from in real-time. This is where artificial intelligence (AI) and machine learning (ML) step in. These technologies act as a sophisticated filtration system, enabling us to drink safely and beneficially from the torrent. AI and ML can analyze patterns, make predictions, and infer insights at a pace that keeps up with the fast stream of data, turning raw information into actionable knowledge. By addressing these three pivotal challenges with cloud computing, GDAL VSI, and AI/ML, we not only manage to consume the data effectively but also transform our capabilities in environmental data science. We can move from mere data ingestion to meaningful data interpretation, all at a scale and speed necessary for impactful environmental analysis.","title":"3) Drinking from a Fire Hose: Accelerated Inference with AI and ML"},{"location":"data-library/stac_mount_save/#mounting-data","text":"A void-filled Digital Elevation Model (DEM) is a comprehensive topographical representation where any missing data points, known as voids, have been filled in. These voids can occur due to various reasons, such as clouds or technical errors during data collection. In a void-filled DEM, these gaps are interpolated or estimated using the surrounding data to create a continuous, seamless surface model. This process enhances the utility and accuracy of the DEM for hydrological modeling, terrain analysis, and other geographical applications. The HydroSHEDS website (https://www.hydrosheds.org/hydrosheds-core-downloads) provides access to high-quality, void-filled DEM datasets like the DEM_continuous_CONUS_15s, which users can download and easily integrate into spatial analysis workflows using tools such as \u2018terra\u2019 in R, allowing for sophisticated environmental and geographical research and planning. # Record start time a <- Sys.time() # Create a string with the file path using glue, then download and read the DEM file as a raster object DEM_continuous_CONUS_15s <- glue( \"/vsizip/vsicurl/\", #magic remote connection \"https://data.hydrosheds.org/file/hydrosheds-v1-dem/hyd_na_dem_15s.zip\", #copied link to download location \"/hyd_na_dem_15s.tif\") %>% #path inside zip file terra::rast() # The 'glue' function constructs the file path string, which is then passed to 'terra::rast()' to read the DEM file into R as a raster layer. '/vsizip/vsicurl/' is a special GDAL virtual file system syntax that allows reading directly from a zipped file on a remote server. # Record end time and calculate the time difference b <- Sys.time() difftime(b, a) ## Time difference of 4.603666 secs # The resulting raster object is stored in 'DEM_continuous_CONUS_15s', which now contains the void-filled DEM data ready for use DEM_continuous_CONUS_15s # Prints out the details of the 'DEM_continuous_CONUS_15s' raster object ## class : SpatRaster ## dimensions : 13920, 20640, 1 (nrow, ncol, nlyr) ## resolution : 0.004166667, 0.004166667 (x, y) ## extent : -138, -52, 5, 63 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## source : hyd_na_dem_15s.tif ## name : Band_1 # output is a SpatRaster, which is the object type associated with the 'terra' package. Continuous DEM for North America # Record start time a <- Sys.time() ggplot() + geom_spatraster(data=DEM_continuous_CONUS_15s) + theme_tufte() b <- Sys.time() difftime(b, a) ## Time difference of 52.49061 secs Calculate Slope from that DEM SLOPE_continuous_CONUS_15s <- terra::terrain(DEM_continuous_CONUS_15s, \"slope\") SLOPE_continuous_CONUS_15s ## class : SpatRaster ## dimensions : 13920, 20640, 1 (nrow, ncol, nlyr) ## resolution : 0.004166667, 0.004166667 (x, y) ## extent : -138, -52, 5, 63 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## source(s) : memory ## name : slope ## min value : 0.00000 ## max value : 56.98691 # Record start time a <- Sys.time() ggplot() + geom_spatraster(data=SLOPE_continuous_CONUS_15s) + theme_tufte() b <- Sys.time() difftime(b, a) ## Time difference of 3.859545 secs Calculate aspect from DEM ASPECT_continuous_CONUS_15s <- terra::terrain(DEM_continuous_CONUS_15s, \"aspect\") ASPECT_continuous_CONUS_15s ## class : SpatRaster ## dimensions : 13920, 20640, 1 (nrow, ncol, nlyr) ## resolution : 0.004166667, 0.004166667 (x, y) ## extent : -138, -52, 5, 63 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## source(s) : memory ## name : aspect ## min value : 0 ## max value : 360 # Record start time a <- Sys.time() ggplot() + geom_spatraster(data=ASPECT_continuous_CONUS_15s) + theme_tufte() b <- Sys.time() difftime(b, a) ## Time difference of 3.650267 secs Create a cube from those layers! mini_stack <- c(DEM_continuous_CONUS_15s, SLOPE_continuous_CONUS_15s,ASPECT_continuous_CONUS_15s) mini_stack ## class : SpatRaster ## dimensions : 13920, 20640, 3 (nrow, ncol, nlyr) ## resolution : 0.004166667, 0.004166667 (x, y) ## extent : -138, -52, 5, 63 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## sources : hyd_na_dem_15s.tif ## memory ## memory ## names : Band_1, slope, aspect ## min values : ? , 0.00000, 0 ## max values : ? , 56.98691, 360 Reproject and return the bounding box coordinates for our Area of Interest # Transform the filtered geometry to EPSG:4326 and store its bounding box # Record start time a <- Sys.time() DEM_continuous_CONUS_15s |> stars::st_as_stars() |> st_transform(\"EPSG:4326\") |> st_bbox() -> bbox_4326 DEM_continuous_CONUS_15s |> stars::st_as_stars() |> st_transform(\"EPSG:32618\") |> st_bbox() -> bbox_32618 b <- Sys.time() difftime(b, a) ## Time difference of 3.7653 mins Get a polygon for Boulder County, reproject, and return bounding box. This is so I can make a smaller search in the stac catalog. boulder_county <- getbb(\"boulder, co\", format_out=\"sf_polygon\") boulder_county$multipolygon |> st_transform(crs =4326 ) |> st_bbox() -> bbox_4326_boulder boulder_county$multipolygon |> st_transform(crs =32720 ) |> st_bbox() -> bbox_32720_boulder Get a polygon for the United States and crop it to be the same size as the DEM above. aoi <- getbb(\"United States\", format_out=\"sf_polygon\") conus <- aoi$multipolygon |> st_crop(bbox_4326) ggplot(data=conus) + geom_sf() Search the Stac catalog. STAC, or SpatioTemporal Asset Catalog, is an open-source specification designed to standardize the way geospatial data is indexed and discovered. Developed by Element 84 among others, it facilitates better interoperability and sharing of geospatial assets by providing a common language for describing them. STAC\u2019s flexible design allows for easy cataloging of data, making it simpler for individuals and systems to search and retrieve geospatial information. By effectively organizing data about the Earth\u2019s spatial and temporal characteristics, STAC enables users to harness the full power of the cloud and modern data processing technologies, optimizing the way we access and analyze environmental data on a global scale. stac(\"https://earth-search.aws.element84.com/v1\") |> get_request() ## ###STACCatalog ## - id: earth-search-aws ## - description: A STAC API of public datasets on AWS ## - field(s): stac_version, type, id, title, description, links, conformsTo Element 84\u2019s Earth Search is a STAC compliant search and discovery API that offers users access to a vast collection of geospatial open datasets hosted on AWS. It serves as a centralized search catalog providing standardized metadata for these open datasets, designed to be freely used and integrated into various applications. Alongside the API, Element 84 also provides a web application named Earth Search Console, which is map-centric and allows users to explore and visualize the data contained within the Earth Search API\u2019s catalog. This suite of tools is part of Element 84\u2019s initiative to make geospatial data more accessible and actionable for a wide range of users and applications. collection_formats() ## CHIRPS_v2_0_daily_p05_tif | Image collection format for CHIRPS v 2.0 daily ## | global precipitation dataset (0.05 degrees ## | resolution) from GeoTIFFs, expects list of .tif ## | or .tif.gz files as input. [TAGS: CHIRPS, ## | precipitation] ## CHIRPS_v2_0_monthly_p05_tif | Image collection format for CHIRPS v 2.0 monthly ## | global precipitation dataset (0.05 degrees ## | resolution) from GeoTIFFs, expects list of .tif ## | or .tif.gz files as input. [TAGS: CHIRPS, ## | precipitation] ## ESA_CCI_SM_ACTIVE | Collection format for ESA CCI soil moisture ## | active product (version 4.7) [TAGS: Soil ## | Moisture, ESA, CCI] ## ESA_CCI_SM_PASSIVE | Collection format for ESA CCI soil moisture ## | passive product (version 4.7) [TAGS: Soil ## | Moisture, ESA, CCI] ## GPM_IMERG_3B_DAY_GIS_V06A | Collection format for daily ## | IMERG_3B_DAY_GIS_V06A data [TAGS: Precipitation, ## | GPM, IMERG] ## L8_L1TP | Collection format for Landsat 8 Level 1 TP ## | product [TAGS: Landsat, USGS, Level 1, NASA] ## L8_SR | Collection format for Landsat 8 surface ## | reflectance product [TAGS: Landsat, USGS, Level ## | 2, NASA, surface reflectance] ## MAXAR | Preliminary collection format for MAXAR open ## | data, visual only (under development) [TAGS: ] ## MxD09GA | Collection format for selected bands from the ## | MODIS MxD09GA (Aqua and Terra) product [TAGS: ## | MODIS, surface reflectance] ## MxD10A2 | Collection format for selected bands from the ## | MODIS MxD10A2 (Aqua and Terra) v006 Snow Cover ## | product [TAGS: MODIS, Snow Cover] ## MxD11A1 | Collection format for selected bands from the ## | MODIS MxD11A2 (Aqua and Terra) v006 Land Surface ## | Temperature product [TAGS: MODIS, LST] ## MxD11A2 | Collection format for selected bands from the ## | MODIS MxD11A2 (Aqua and Terra) v006 Land Surface ## | Temperature product [TAGS: MODIS, LST] ## MxD13A2 | Collection format for selected bands from the ## | MODIS MxD13A2 (Aqua and Terra) product [TAGS: ## | MODIS, VI, NDVI, EVI] ## MxD13A3 | Collection format for selected bands from the ## | MODIS MxD13A3 (Aqua and Terra) product [TAGS: ## | MODIS, VI, NDVI, EVI] ## MxD13Q1 | Collection format for selected bands from the ## | MODIS MxD13Q1 (Aqua and Terra) product [TAGS: ## | MODIS, VI, NDVI, EVI] ## MxD14A2 | Collection format for the MODIS MxD14A2 (Aqua ## | and Terra) product [TAGS: MODIS, Fire] ## PlanetScope_3B_AnalyticMS_SR | Image collection format for PlanetScope 4-band ## | scenes [TAGS: PlanetScope, BOA, Surface ## | Reflectance] ## Sentinel2_L1C | Image collection format for Sentinel 2 Level 1C ## | data as downloaded from the Copernicus Open ## | Access Hub, expects a list of file paths as ## | input. The format works on original ZIP ## | compressed as well as uncompressed imagery. ## | [TAGS: Sentinel, Copernicus, ESA, TOA] ## Sentinel2_L1C_AWS | Image collection format for Sentinel 2 Level 1C ## | data in AWS [TAGS: Sentinel, Copernicus, ESA, ## | TOA] ## Sentinel2_L2A | Image collection format for Sentinel 2 Level 2A ## | data as downloaded from the Copernicus Open ## | Access Hub, expects a list of file paths as ## | input. The format should work on original ZIP ## | compressed as well as uncompressed imagery. ## | [TAGS: Sentinel, Copernicus, ESA, BOA, Surface ## | Reflectance] ## Sentinel2_L2A_THEIA | Image collection format for Sentinel 2 Level 2A ## | data as downloaded from Theia. [TAGS: Sentinel, ## | ESA, Flat Reflectance, Theia] Building a stac collection by aiming your camera at the landscape Creating a STAC collection is akin to a photographer framing a shot; the landscape is rich with diverse data, mirroring a scene bustling with potential subjects, colors, and light. Just as a photographer selects a portion of the vista to capture, focusing on elements that will compose a compelling image, a data scientist must similarly navigate the vast data terrain. They must \u2018point their camera\u2019 judiciously, ensuring that the \u2018frame\u2019 encapsulates the precise data needed. This careful selection is crucial, as it determines the relevance and quality of the data collection, much like the photographer\u2019s choice dictates the story a photograph will tell. # Record start time a <- Sys.time() # Initialize STAC connection s = stac(\"https://earth-search.aws.element84.com/v0\") # Search for Sentinel-2 images within specified bounding box and date range #22 Million items items = s |> stac_search(collections = \"sentinel-s2-l2a-cogs\", bbox = c(bbox_4326_boulder[\"xmin\"], bbox_4326_boulder[\"ymin\"], bbox_4326_boulder[\"xmax\"], bbox_4326_boulder[\"ymax\"]), datetime = \"2021-05-15/2021-05-16\") |> post_request() |> items_fetch(progress = FALSE) # Print number of found items length(items$features) ## [1] 1 # Prepare the assets for analysis library(gdalcubes) assets = c(\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\", \"SCL\") s2_collection = stac_image_collection(items$features, asset_names = assets, property_filter = function(x) {x[[\"eo:cloud_cover\"]] < 20}) #all images with less than 20% clouds b <- Sys.time() difftime(b, a) ## Time difference of 0.4706092 secs # Display the image collection s2_collection ## Image collection object, referencing 1 images with 13 bands ## Images: ## name left top bottom right ## 1 S2B_13TDE_20210516_0_L2A -106.1832 40.65079 39.65576 -104.8846 ## datetime srs ## 1 2021-05-16T18:02:54 EPSG:32613 ## ## Bands: ## name offset scale unit nodata image_count ## 1 B01 0 1 1 ## 2 B02 0 1 1 ## 3 B03 0 1 1 ## 4 B04 0 1 1 ## 5 B05 0 1 1 ## 6 B06 0 1 1 ## 7 B07 0 1 1 ## 8 B08 0 1 1 ## 9 B09 0 1 1 ## 10 B11 0 1 1 ## 11 B12 0 1 1 ## 12 B8A 0 1 1 ## 13 SCL 0 1 1 Setting up your camera and film The camera through which the data scientist frames the shot is multifaceted, akin to the tools and processes they employ. The camera\u2019s film, analogous to the data cube, defines the resolution and dimensions of the captured data, shaping how the final dataset will be utilized. The lens and its settings\u2014focus, aperture, and exposure\u2014determine the clarity, depth, and breadth of the captured information, much like the algorithms and parameters set by the data scientist dictate the granularity and scope of the data cube. The flash, like data enhancement techniques, can illuminate hidden details, ensuring that the data cube, the final product, is as informative and accurate as the landscape it represents. # Record start time a <- Sys.time() # Define a specific view on the satellite image collection v = cube_view( srs = \"EPSG:32720\", #this is harder than expected. dx = 100, dy = 100, dt = \"P1M\", aggregation = \"median\", resampling = \"near\", extent = list( t0 = \"2021-05-15\", t1 = \"2021-05-16\", left = bbox_32720_boulder[1], right = bbox_32720_boulder[2], top = bbox_32720_boulder[4], bottom = bbox_32720_boulder[3] ) ) b <- Sys.time() difftime(b, a) ## Time difference of 0.002738953 secs # Display the defined view v ## A data cube view object ## ## Dimensions: ## low high count pixel_size ## t 2021-05-01 2021-05-31 1 P1M ## y -3103099.52398788 15434400.4760121 185375 100 ## x -3178878.98542359 15369521.0145764 185484 100 ## ## SRS: \"EPSG:32720\" ## Temporal aggregation method: \"median\" ## Spatial resampling method: \"near\" Take a picture! Raster style # Record start time a <- Sys.time() s2_collection |> raster_cube(v) |> select_bands(c( \"B04\", \"B05\")) |> apply_pixel(c(\"(B05-B04)/(B05+B04)\"), names=\"NDVI\") |> write_tif() |> raster::stack() -> x x ## class : RasterStack ## dimensions : 185375, 185484, 34384096500, 1 (nrow, ncol, ncell, nlayers) ## resolution : 100, 100 (x, y) ## extent : -3178879, 15369521, -3103100, 15434400 (xmin, xmax, ymin, ymax) ## crs : +proj=utm +zone=20 +south +datum=WGS84 +units=m +no_defs ## names : NDVI b <- Sys.time() difftime(b, a) ## Time difference of 4.132932 mins STARS style # Record start time a <- Sys.time() s2_collection |> raster_cube(v) |> select_bands(c(\"B04\",\"B05\")) |> apply_pixel(c(\"(B05-B04)/(B05+B04)\"), names=\"NDVI\") |> stars::st_as_stars() -> y b <- Sys.time() difftime(b, a) ## Time difference of 1.459866 mins y ## stars_proxy object with 1 attribute in 1 file(s): ## $NDVI ## [1] \"[...]/filec5982c38536c.nc:NDVI\" ## ## dimension(s): ## from to offset delta refsys point ## x 1 185484 -3178879 100 WGS 84 / UTM zone 20S NA ## y 1 185375 15434400 -100 WGS 84 / UTM zone 20S NA ## time 1 1 NA NA POSIXct FALSE ## values x/y ## x NULL [x] ## y NULL [y] ## time [2021-05-01,2021-06-01) Extract data # Record start time a <- Sys.time() x <- s2_collection |> raster_cube(v) |> select_bands(c(\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\")) |> extract_geom(boulder_county$multipolygon) |> rename( \"time\" = \"time\", \"443\" = \"B01\", \"490\" = \"B02\", \"560\" = \"B03\", \"665\" = \"B04\", \"705\" = \"B05\", \"740\" = \"B06\", \"783\" = \"B07\", \"842\" = \"B08\", \"865\" = \"B8A\", \"940\" = \"B09\", \"1610\" = \"B11\", \"2190\" = \"B12\" ) b <- Sys.time() difftime(b, a) ## Time difference of 1.699016 mins head(x) ## FID time 443 490 560 665 705 740 783 842 865 940 1610 ## 1 1 2021-05-01 11096 10929 10224 9893 9956 9706 9715 9641 9511 8459 5682 ## 2 1 2021-05-01 11631 11282 10550 10234 10288 10031 10032 9988 9828 9153 5802 ## 3 1 2021-05-01 11900 11393 10666 10337 10398 10142 10138 10093 9927 9461 5754 ## 4 1 2021-05-01 11406 10597 9928 9626 9694 9481 9516 9338 9336 8959 5726 ## 5 1 2021-05-01 11399 10939 10237 9905 9978 9738 9746 9633 9555 8925 5831 ## 6 1 2021-05-01 11600 11174 10462 10147 10209 9952 9960 9890 9760 9153 5773 ## 2190 ## 1 3917 ## 2 3981 ## 3 3937 ## 4 4054 ## 5 4097 ## 6 3990 Make a timeseries # Record start time a <- Sys.time() items <- s |> stac_search(collections = \"sentinel-s2-l2a-cogs\", bbox = c(-105.694362, 39.912886, -105.052774, 40.262785), datetime = \"2020-01-01/2022-12-31\", limit = 500) %>% post_request() S2.mask = image_mask(\"SCL\", values=c(3,8,9)) col = stac_image_collection(items$features, asset_names = assets, property_filter = function(x) {x[[\"eo:cloud_cover\"]] < 30}) v = cube_view(srs = \"EPSG:4326\", extent = list(t0 = \"2020-01-01\", t1 = \"2022-12-31\", left = -105.694362, right = -105.052774, top = 40.262785, bottom = 39.912886), dx = 0.001, dy = 0.001, dt = \"P1M\", aggregation = \"median\", resampling = \"bilinear\") library(colorspace) ndvi.col = function(n) { rev(sequential_hcl(n, \"Green-Yellow\")) } library(gdalcubes) raster_cube(col, v, mask = S2.mask) |> select_bands(c(\"B04\", \"B08\")) |> apply_pixel(\"(B08-B04)/(B08+B04)\", \"NDVI\") |> gdalcubes::animate(col = ndvi.col, zlim=c(-0.2,1), key.pos = 1, save_as = \"anim.gif\", fps = 4) ## [1] \"/Users/ty/Documents/Github/hackathon2023_datacube/docs/code_for_building_cube/anim.gif\" b <- Sys.time() difftime(b, a) ## Time difference of 4.716672 mins y ## stars_proxy object with 1 attribute in 1 file(s): ## $NDVI ## [1] \"[...]/filec5982c38536c.nc:NDVI\" ## ## dimension(s): ## from to offset delta refsys point ## x 1 185484 -3178879 100 WGS 84 / UTM zone 20S NA ## y 1 185375 15434400 -100 WGS 84 / UTM zone 20S NA ## time 1 1 NA NA POSIXct FALSE ## values x/y ## x NULL [x] ## y NULL [y] ## time [2021-05-01,2021-06-01) Saving Data Cubes to Local Storage There are occasions when we need to manipulate data cubes using other software. For such purposes, we can save data cubes to our local disk as individual netCDF files or as a series of GeoTIFF files. In the case of the latter, each temporal segment of the cube is saved as a separate (multiband) GeoTIFF file. Both netCDF and GeoTIFF formats allow for file size reduction through compression and data packing. This process involves transforming double precision numbers into smaller integer values using a scale and offset, which can be particularly useful for managing disk space (for more details, refer to the ?write_ncdf and ?write_tif documentation). gdalcubes_options(ncdf_compression_level = 1) write_ncdf(cube, file.path(\"~/Desktop\", basename(tempfile(fileext = \".nc\")))) gdalcubes_options(ncdf_compression_level = 0) write_tif() and write_ncdf() both return the path(s) to created file(s) as a character vector. items_2020 <- s |> stac_search(collections = \"sentinel-s2-l2a-cogs\", bbox = c(-105.694362, 39.912886, -105.052774, 40.262785), datetime = \"2020-05-01/2020-06-30\") |> post_request() items_2021 <- s |> stac_search(collections = \"sentinel-s2-l2a-cogs\", bbox = c(-105.694362, 39.912886, -105.052774, 40.262785), datetime = \"2021-05-01/2021-06-30\") |> post_request() col_2020 = stac_image_collection(items_2020$features, asset_names = assets) col_2021 = stac_image_collection(items_2021$features, asset_names = assets) v_2020 = cube_view(srs = \"EPSG:32720\", extent = list(t0 = \"2020-05-01\", t1 = \"2020-06-30\", left = bbox_32720_boulder[\"xmin\"], right = bbox_32720_boulder[\"xmax\"], top = bbox_32720_boulder[\"ymax\"], bottom = bbox_32720_boulder[\"ymin\"]), dx = 100, dy = 100, dt = \"P1D\", aggregation = \"median\", resampling = \"bilinear\") v_2021 = cube_view(v_2020, extent = list(t0 = \"2021-05-01\", t1 = \"2021-06-30\")) max_ndvi_mosaic <- function(col, v) { raster_cube(col, v) |> select_bands(c(\"B04\", \"B08\")) |> apply_pixel(c(\"(B08-B04)/(B08+B04)\"), names=\"NDVI\") |> reduce_time(\"max(NDVI)\") } suppressPackageStartupMessages(library(stars)) max_ndvi_mosaic(col_2020, v_2020) -> maxndvi_2020 max_ndvi_mosaic(col_2021, v_2021) -> maxndvi_2021 maxndvi_2021 maxndvi_2020 difference = maxndvi_2021 - maxndvi_2020 difference[difference > -0.15] = NA names(difference) <- \"Difference of max NDVI (2020 - 2019)\" flood_polygon_data3 <- glue(\"/vsizip/vsicurl/https://data.hydrosheds.org/file/hydrosheds-associated/gloric/GloRiC_v10_shapefile.zip/GloRiC_v10_shapefile/GloRiC_v10.shp\") |> st_read() |> st_as_sf(coords = c(\"lon\",\"lat\")) flood_polygon_data3 #st_read(\"/Users/ty/Downloads/GloRiC_v10_geodatabase/GloRiC_v10.gdb\") flood_polygon_data3 <- glue(\"/vsizip/vsicurl/https://data.hydrosheds.org/file/hydrosheds-associated/gloric/GloRiC_v10_geodatabase.zip/GloRiC_v10_geodatabase/GloRiC_v10.gdb\") |> st_read() |> st_as_sf(coords = c(\"lon\",\"lat\")) flood_polygon_data3","title":"Mounting data"},{"location":"data-library/stac_simple/","text":"Accessing data via STAC ESIIL, 2024 Ty Tuff & Tyler McIntosh SpatioTemporal Asset Catalog, is an open-source specification designed to standardize the way geospatial data is indexed and discovered. Developed by Element 84 among others, it facilitates better interoperability and sharing of geospatial assets by providing a common language for describing them. STAC\u2019s flexible design allows for easy cataloging of data, making it simpler for individuals and systems to search and retrieve geospatial information. By effectively organizing data about the Earth\u2019s spatial and temporal characteristics, STAC enables users to harness the full power of the cloud and modern data processing technologies, optimizing the way we access and analyze environmental data on a global scale. Element 84\u2019s Earth Search is a STAC compliant search and discovery API that offers users access to a vast collection of geospatial open datasets hosted on AWS. It serves as a centralized search catalog providing standardized metadata for these open datasets, designed to be freely used and integrated into various applications. Alongside the API, Element 84 also provides a web application named Earth Search Console, which is map-centric and allows users to explore and visualize the data contained within the Earth Search API\u2019s catalog. This suite of tools is part of Element 84\u2019s initiative to make geospatial data more accessible and actionable for a wide range of users and applications. First, we need an area of interest require(glue) require(sf) require(gdalcubes) require(rstac) #Access ecoregiosn via VSI epa_l3 <- glue::glue( \"/vsizip/vsicurl/\", #magic remote connection \"https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l3.zip\", #copied link to download location \"/us_eco_l3.shp\") |> #path inside zip file sf::st_read() #Get just S.Rockies and ensure that it is in EPSG:4326 southernRockies <- epa_l3 |> dplyr::filter(US_L3NAME == \"Southern Rockies\") |> dplyr::group_by(US_L3NAME) |> dplyr::summarize(geometry = sf::st_union(geometry)) |> sf::st_transform(\"EPSG:4326\") bboxSR4326 <- sf::st_bbox(southernRockies) To access data from STAC correctly, we need to request the data in a projected CRS. southernRockies <- southernRockies |> sf::st_transform(\"EPSG:32613\") bboxSRproj <- sf::st_bbox(southernRockies) Search the STAC catalog To get information about a STAC archive, you can use rstac::get_request(). You can also use gdalcubes::collection_formats() to see various collection formats that you may encounter. To search a STAC catalog online, stacindex.org is a useful tool. For example, here is the page for the Earth Search catalog by Element84 that we will use. stac(\"https://earth-search.aws.element84.com/v1\") |> get_request() ## ###STACCatalog ## - id: earth-search-aws ## - description: A STAC API of public datasets on AWS ## - field(s): stac_version, type, id, title, description, links, conformsTo collection_formats() Initialize a STAC connection (rstac::stac()) and search for data that you are interested in (rstac::stac_search()). Note that you will request a spatial area of interest as well as a temporal window of interest. To get more information on the data and how it is structured, you can examine the 'items' object we create. # Record start time a <- Sys.time() # Initialize STAC connection s = rstac::stac(\"https://earth-search.aws.element84.com/v0\") # Search for Sentinel-2 images within specified bounding box and date range #22 Million items items = s |> rstac::stac_search(collections = \"sentinel-s2-l2a-cogs\", bbox = c(bboxSR4326[\"xmin\"], bboxSR4326[\"ymin\"], bboxSR4326[\"xmax\"], bboxSR4326[\"ymax\"]), datetime = \"2021-05-15/2021-05-16\") |> post_request() |> items_fetch(progress = FALSE) # Print number of found items length(items$features) items There is data we want! Now, we need to prepare the assets for us to access. We will list the assets we want, and set any property filters that we would like to apply. # Prepare the assets for analysis library(gdalcubes) assets = c(\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\", \"SCL\") s2_collection = gdalcubes::stac_image_collection(items$features, asset_names = assets, property_filter = function(x) {x[[\"eo:cloud_cover\"]] < 20}) #all images with less than 20% clouds b <- Sys.time() difftime(b, a) # Display the image collection s2_collection Access the data First, we need to set up our view on the collection. We will set our spatial and temporal resolution, as well as how we want the data temporally aggregated and spatially resampled. We then also set our spatial and temporal window. Note that the spatial extent here should be in a projected CRS! # Record start time a <- Sys.time() # Define a specific view on the satellite image collection v = gdalcubes::cube_view( srs = \"EPSG:32613\", dx = 100, dy = 100, dt = \"P1M\", aggregation = \"median\", resampling = \"near\", extent = list( t0 = \"2021-05-15\", t1 = \"2021-05-16\", left = bboxSRproj[1], right = bboxSRproj[2], top = bboxSRproj[4], bottom = bboxSRproj[3] ) ) b <- Sys.time() difftime(b, a) # Display the defined view v Finally, let's take our snapshot of the data! Let's also calculate NDVI and then view the data. ``` Record start time a <- Sys.time() s2_collection |> raster_cube(v) |> select_bands(c( \"B04\", \"B05\")) |> apply_pixel(c(\"(B05-B04)/(B05+B04)\"), names=\"NDVI\") |> write_tif() |> raster::stack() -> x View the product x b <- Sys.time() difftime(b, a) Let's view the dat mapview::mapview(x, layer.name = \"NDVI\") + mapview::mapview(southernRockies)","title":"TUTORIAL Accessing data via STAC"},{"location":"data-library/stac_simple/#accessing-data-via-stac","text":"ESIIL, 2024 Ty Tuff & Tyler McIntosh SpatioTemporal Asset Catalog, is an open-source specification designed to standardize the way geospatial data is indexed and discovered. Developed by Element 84 among others, it facilitates better interoperability and sharing of geospatial assets by providing a common language for describing them. STAC\u2019s flexible design allows for easy cataloging of data, making it simpler for individuals and systems to search and retrieve geospatial information. By effectively organizing data about the Earth\u2019s spatial and temporal characteristics, STAC enables users to harness the full power of the cloud and modern data processing technologies, optimizing the way we access and analyze environmental data on a global scale. Element 84\u2019s Earth Search is a STAC compliant search and discovery API that offers users access to a vast collection of geospatial open datasets hosted on AWS. It serves as a centralized search catalog providing standardized metadata for these open datasets, designed to be freely used and integrated into various applications. Alongside the API, Element 84 also provides a web application named Earth Search Console, which is map-centric and allows users to explore and visualize the data contained within the Earth Search API\u2019s catalog. This suite of tools is part of Element 84\u2019s initiative to make geospatial data more accessible and actionable for a wide range of users and applications.","title":"Accessing data via STAC"},{"location":"data-library/stac_simple/#first-we-need-an-area-of-interest","text":"require(glue) require(sf) require(gdalcubes) require(rstac) #Access ecoregiosn via VSI epa_l3 <- glue::glue( \"/vsizip/vsicurl/\", #magic remote connection \"https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l3.zip\", #copied link to download location \"/us_eco_l3.shp\") |> #path inside zip file sf::st_read() #Get just S.Rockies and ensure that it is in EPSG:4326 southernRockies <- epa_l3 |> dplyr::filter(US_L3NAME == \"Southern Rockies\") |> dplyr::group_by(US_L3NAME) |> dplyr::summarize(geometry = sf::st_union(geometry)) |> sf::st_transform(\"EPSG:4326\") bboxSR4326 <- sf::st_bbox(southernRockies) To access data from STAC correctly, we need to request the data in a projected CRS. southernRockies <- southernRockies |> sf::st_transform(\"EPSG:32613\") bboxSRproj <- sf::st_bbox(southernRockies)","title":"First, we need an area of interest"},{"location":"data-library/stac_simple/#search-the-stac-catalog","text":"To get information about a STAC archive, you can use rstac::get_request(). You can also use gdalcubes::collection_formats() to see various collection formats that you may encounter. To search a STAC catalog online, stacindex.org is a useful tool. For example, here is the page for the Earth Search catalog by Element84 that we will use. stac(\"https://earth-search.aws.element84.com/v1\") |> get_request() ## ###STACCatalog ## - id: earth-search-aws ## - description: A STAC API of public datasets on AWS ## - field(s): stac_version, type, id, title, description, links, conformsTo collection_formats() Initialize a STAC connection (rstac::stac()) and search for data that you are interested in (rstac::stac_search()). Note that you will request a spatial area of interest as well as a temporal window of interest. To get more information on the data and how it is structured, you can examine the 'items' object we create. # Record start time a <- Sys.time() # Initialize STAC connection s = rstac::stac(\"https://earth-search.aws.element84.com/v0\") # Search for Sentinel-2 images within specified bounding box and date range #22 Million items items = s |> rstac::stac_search(collections = \"sentinel-s2-l2a-cogs\", bbox = c(bboxSR4326[\"xmin\"], bboxSR4326[\"ymin\"], bboxSR4326[\"xmax\"], bboxSR4326[\"ymax\"]), datetime = \"2021-05-15/2021-05-16\") |> post_request() |> items_fetch(progress = FALSE) # Print number of found items length(items$features) items There is data we want! Now, we need to prepare the assets for us to access. We will list the assets we want, and set any property filters that we would like to apply. # Prepare the assets for analysis library(gdalcubes) assets = c(\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\", \"SCL\") s2_collection = gdalcubes::stac_image_collection(items$features, asset_names = assets, property_filter = function(x) {x[[\"eo:cloud_cover\"]] < 20}) #all images with less than 20% clouds b <- Sys.time() difftime(b, a) # Display the image collection s2_collection","title":"Search the STAC catalog"},{"location":"data-library/stac_simple/#access-the-data","text":"First, we need to set up our view on the collection. We will set our spatial and temporal resolution, as well as how we want the data temporally aggregated and spatially resampled. We then also set our spatial and temporal window. Note that the spatial extent here should be in a projected CRS! # Record start time a <- Sys.time() # Define a specific view on the satellite image collection v = gdalcubes::cube_view( srs = \"EPSG:32613\", dx = 100, dy = 100, dt = \"P1M\", aggregation = \"median\", resampling = \"near\", extent = list( t0 = \"2021-05-15\", t1 = \"2021-05-16\", left = bboxSRproj[1], right = bboxSRproj[2], top = bboxSRproj[4], bottom = bboxSRproj[3] ) ) b <- Sys.time() difftime(b, a) # Display the defined view v Finally, let's take our snapshot of the data! Let's also calculate NDVI and then view the data. ```","title":"Access the data"},{"location":"data-library/stac_simple/#record-start-time","text":"a <- Sys.time() s2_collection |> raster_cube(v) |> select_bands(c( \"B04\", \"B05\")) |> apply_pixel(c(\"(B05-B04)/(B05+B04)\"), names=\"NDVI\") |> write_tif() |> raster::stack() -> x","title":"Record start time"},{"location":"data-library/stac_simple/#view-the-product","text":"x b <- Sys.time() difftime(b, a)","title":"View the product"},{"location":"data-library/stac_simple/#lets-view-the-dat","text":"mapview::mapview(x, layer.name = \"NDVI\") + mapview::mapview(southernRockies)","title":"Let's view the dat"},{"location":"data-library/treemap/","text":"TreeMap TreeMap 2016 is a USFS tree-level model of the forests of the conterminous United States created by using machine learning algorithms to match forest plot data from Forest Inventory and Analysis (FIA) to a 30x30 meter (m) grid. The main output of this project is a raster map of imputed plot identifiers at 30\u00d730 m spatial resolution for the conterminous U.S. for landscape conditions circa 2016. The plot identifiers can be associated with data from FIA plots held in the associated csv and SQL files. An overview of the data product can be found here. The TreeMap data dictionary PDF can be found here. A portion of the TreeMap dataset covering the Southern Rockies has been prepared and placed in the CyVerse data store at the below directroy. The associated CSV and SQL DB files are in the same location. A script showing how to access it, as well as how the raster was accessed, is available in the code repository, as well as copied below. ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/TreeMap # This script demonstrates how to open and access pre-downloaded TreeMap data from the data store # It also, at the bottom, shows how the data was accessed via VSI. # A similar approach could be used to access the SnagHazard data in the zip file via VSI if desired. (Path inside zip: Data/SnagHazard2016.tif) # ESIIL, 2024 # Tyler L. McIntosh require(terra) #Move data from data store to instance system(\"cp -r ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/TreeMap ~/TreeMap \") #Open the raster treemap <- terra::rast(\"~/TreeMap/treemap2016_southernrockies.tif\") terra::plot(treemap) #Open the csv treemapCsv <- readr::read_csv(\"~/TreeMap/TreeMap2016_tree_table.csv\") head(treemapCsv) ####################################################### # DATA ACCESS SCRIPT ####################################################### # Access treemap data, crop to southern rockies, and save to data store require(glue) require(terra) require(sf) #Access EPA L3 data for cropping epa_l3 <- glue::glue( \"/vsizip/vsicurl/\", #magic remote connection \"https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l3.zip\", #copied link to download location \"/us_eco_l3.shp\") |> #path inside zip file sf::st_read() #get just S.Rockies southernRockies <- epa_l3 |> dplyr::filter(US_L3NAME == \"Southern Rockies\") |> dplyr::group_by(US_L3NAME) |> dplyr::summarize(geometry = sf::st_union(geometry)) #Access treemap data treemap <- glue::glue( \"/vsizip/vsicurl/\", #magic remote connection \"https://s3-us-west-2.amazonaws.com/fs.usda.rds/RDS-2021-0074/RDS-2021-0074_Data.zip\", #copied link to download location \"/Data/TreeMap2016.tif\") |> #path inside zip file terra::rast() #Crop to s.rockies treemapSR <- treemap |> terra::crop(southernRockies, mask = FALSE) #check data terra::plot(treemapSR) #Write to instance terra::writeRaster(treemapSR, filename = '~/treemap2016_southernrockies.tif', overwrite = TRUE, gdal=c(\"COMPRESS=DEFLATE\")) #Move data to data store system(\"cp ~/treemap2016_southernrockies.tif ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/TreeMap/treemap2016_southernrockies_again.tif \")","title":"TreeMap"},{"location":"data-library/treemap/#treemap","text":"TreeMap 2016 is a USFS tree-level model of the forests of the conterminous United States created by using machine learning algorithms to match forest plot data from Forest Inventory and Analysis (FIA) to a 30x30 meter (m) grid. The main output of this project is a raster map of imputed plot identifiers at 30\u00d730 m spatial resolution for the conterminous U.S. for landscape conditions circa 2016. The plot identifiers can be associated with data from FIA plots held in the associated csv and SQL files. An overview of the data product can be found here. The TreeMap data dictionary PDF can be found here. A portion of the TreeMap dataset covering the Southern Rockies has been prepared and placed in the CyVerse data store at the below directroy. The associated CSV and SQL DB files are in the same location. A script showing how to access it, as well as how the raster was accessed, is available in the code repository, as well as copied below. ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/TreeMap # This script demonstrates how to open and access pre-downloaded TreeMap data from the data store # It also, at the bottom, shows how the data was accessed via VSI. # A similar approach could be used to access the SnagHazard data in the zip file via VSI if desired. (Path inside zip: Data/SnagHazard2016.tif) # ESIIL, 2024 # Tyler L. McIntosh require(terra) #Move data from data store to instance system(\"cp -r ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/TreeMap ~/TreeMap \") #Open the raster treemap <- terra::rast(\"~/TreeMap/treemap2016_southernrockies.tif\") terra::plot(treemap) #Open the csv treemapCsv <- readr::read_csv(\"~/TreeMap/TreeMap2016_tree_table.csv\") head(treemapCsv) ####################################################### # DATA ACCESS SCRIPT ####################################################### # Access treemap data, crop to southern rockies, and save to data store require(glue) require(terra) require(sf) #Access EPA L3 data for cropping epa_l3 <- glue::glue( \"/vsizip/vsicurl/\", #magic remote connection \"https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l3.zip\", #copied link to download location \"/us_eco_l3.shp\") |> #path inside zip file sf::st_read() #get just S.Rockies southernRockies <- epa_l3 |> dplyr::filter(US_L3NAME == \"Southern Rockies\") |> dplyr::group_by(US_L3NAME) |> dplyr::summarize(geometry = sf::st_union(geometry)) #Access treemap data treemap <- glue::glue( \"/vsizip/vsicurl/\", #magic remote connection \"https://s3-us-west-2.amazonaws.com/fs.usda.rds/RDS-2021-0074/RDS-2021-0074_Data.zip\", #copied link to download location \"/Data/TreeMap2016.tif\") |> #path inside zip file terra::rast() #Crop to s.rockies treemapSR <- treemap |> terra::crop(southernRockies, mask = FALSE) #check data terra::plot(treemapSR) #Write to instance terra::writeRaster(treemapSR, filename = '~/treemap2016_southernrockies.tif', overwrite = TRUE, gdal=c(\"COMPRESS=DEFLATE\")) #Move data to data store system(\"cp ~/treemap2016_southernrockies.tif ~/data-store/data/iplant/home/shared/earthlab/forest_carbon_codefest/TreeMap/treemap2016_southernrockies_again.tif \")","title":"TreeMap"},{"location":"project-documentation/methods/","text":"Project methods overview Data Sources List and describe data sources used, including links to cloud-optimized sources. Highlight permissions and compliance with data ownership guidelines. Data Processing Steps Describe data processing steps taken, the order of scripts, etc. Data Analysis Describe steps taken to analyze data and resulting files in team data store file structure. Visualizations Describe visualizations created and any specialized techniques or libraries that users should be aware of. Conclusions Summary of the full workflow and its outcomes. Reflect on the methods used. References Citations of tools, data sources, and other references used.","title":"Methods"},{"location":"project-documentation/methods/#project-methods-overview","text":"","title":"Project methods overview"},{"location":"project-documentation/methods/#data-sources","text":"List and describe data sources used, including links to cloud-optimized sources. Highlight permissions and compliance with data ownership guidelines.","title":"Data Sources"},{"location":"project-documentation/methods/#data-processing-steps","text":"Describe data processing steps taken, the order of scripts, etc.","title":"Data Processing Steps"},{"location":"project-documentation/methods/#data-analysis","text":"Describe steps taken to analyze data and resulting files in team data store file structure.","title":"Data Analysis"},{"location":"project-documentation/methods/#visualizations","text":"Describe visualizations created and any specialized techniques or libraries that users should be aware of.","title":"Visualizations"},{"location":"project-documentation/methods/#conclusions","text":"Summary of the full workflow and its outcomes. Reflect on the methods used.","title":"Conclusions"},{"location":"project-documentation/methods/#references","text":"Citations of tools, data sources, and other references used.","title":"References"},{"location":"project-documentation/project-notes/","text":"Project discussion notes Virtual meeting #3 Team theme, tentative area of interest, or question: Day 1: March 12, 2024 - CU Boulder Selected scientific question: Day 2: March 13, 2024 - CU Boulder Day 3: March 14, 2024 - CU Boulder","title":"Discussion notes"},{"location":"project-documentation/project-notes/#project-discussion-notes","text":"","title":"Project discussion notes"},{"location":"project-documentation/project-notes/#virtual-meeting-3","text":"","title":"Virtual meeting #3"},{"location":"project-documentation/project-notes/#team-theme-tentative-area-of-interest-or-question","text":"","title":"Team theme, tentative area of interest, or question:"},{"location":"project-documentation/project-notes/#day-1-march-12-2024-cu-boulder","text":"","title":"Day 1: March 12, 2024 - CU Boulder"},{"location":"project-documentation/project-notes/#selected-scientific-question","text":"","title":"Selected scientific question:"},{"location":"project-documentation/project-notes/#day-2-march-13-2024-cu-boulder","text":"","title":"Day 2: March 13, 2024 - CU Boulder"},{"location":"project-documentation/project-notes/#day-3-march-14-2024-cu-boulder","text":"","title":"Day 3: March 14, 2024 - CU Boulder"},{"location":"project-documentation/project-presentation/","text":"Project presentation overview All project presentation materials should be made available on this page. Your team may present directly from this page if you would like to; alternatively, if you would prefer to use slides to present, please make sure to export your team's slides as a PDF, add them to your GitHub, and add the link to that PDF here below. Presentation","title":"Project presentation"},{"location":"project-documentation/project-presentation/#project-presentation-overview","text":"All project presentation materials should be made available on this page. Your team may present directly from this page if you would like to; alternatively, if you would prefer to use slides to present, please make sure to export your team's slides as a PDF, add them to your GitHub, and add the link to that PDF here below.","title":"Project presentation overview"},{"location":"project-documentation/project-presentation/#presentation","text":"","title":"Presentation"},{"location":"resources/art%20gallery/","text":"science art 2024-01-25 Ty\u2019s art opinion In the context of the ongoing discussions for the redesign of our ESIIL office space, I would like to offer my personal perspective on the art and aesthetic that might enrich our environment: Urban Realism with a Personal Touch : I have a strong appreciation for artworks that reflect a realistic depiction of nature and urban life but with an imaginative twist. Art that integrates with and elevates our daily surroundings could offer a fresh perspective on the mundane. Nature in the Workplace : On a personal note, I find that art which brings elements of the outdoors inside can create a serene and motivating atmosphere, conducive to the values of sustainability that ESIIL embodies. Interactive Art : I believe that art installations which invite interaction or present a playful exaggeration of reality can energize our space. They have the potential to foster a creative dialogue among the team and with visitors. Dimensionality and Engagement : From my viewpoint, art that breaks out of the traditional two-dimensional space and engages with the viewer in three dimensions can transform the feel of an office. Such dynamic pieces could encourage innovative thinking and collaboration. Art with a Message : It\u2019s my opinion that the art we choose should subtly reflect our collective social and environmental commitments. Pieces that prompt introspection about our role in larger societal issues could resonate well with our team\u2019s ethos. Community Connection : Lastly, I feel that our office should not just be a place for work but also a space that invites community interaction. Art can be a bridge between ESIIL and the public, making our office a hub for inspiration and engagement.","title":"science art"},{"location":"resources/art%20gallery/#science-art","text":"2024-01-25","title":"science art"},{"location":"resources/art%20gallery/#tys-art-opinion","text":"In the context of the ongoing discussions for the redesign of our ESIIL office space, I would like to offer my personal perspective on the art and aesthetic that might enrich our environment: Urban Realism with a Personal Touch : I have a strong appreciation for artworks that reflect a realistic depiction of nature and urban life but with an imaginative twist. Art that integrates with and elevates our daily surroundings could offer a fresh perspective on the mundane. Nature in the Workplace : On a personal note, I find that art which brings elements of the outdoors inside can create a serene and motivating atmosphere, conducive to the values of sustainability that ESIIL embodies. Interactive Art : I believe that art installations which invite interaction or present a playful exaggeration of reality can energize our space. They have the potential to foster a creative dialogue among the team and with visitors. Dimensionality and Engagement : From my viewpoint, art that breaks out of the traditional two-dimensional space and engages with the viewer in three dimensions can transform the feel of an office. Such dynamic pieces could encourage innovative thinking and collaboration. Art with a Message : It\u2019s my opinion that the art we choose should subtly reflect our collective social and environmental commitments. Pieces that prompt introspection about our role in larger societal issues could resonate well with our team\u2019s ethos. Community Connection : Lastly, I feel that our office should not just be a place for work but also a space that invites community interaction. Art can be a bridge between ESIIL and the public, making our office a hub for inspiration and engagement.","title":"Ty\u2019s art opinion"},{"location":"resources/citations/","text":"Citation Management and Notes Collection in Markdown Introduction This document serves as a guide for managing citations and collecting research notes for our project. We'll use a combination of a .bib file for bibliographic references and Markdown for note-taking. Part 1: Setting Up Your .bib File for Citations Creating a .bib File Create a new file with a .bib extension, for example, project_references.bib . Add bibliographic entries to this file. Each entry should follow the BibTeX format. Example of a .bib Entry ```bibtex @article{Doe2021, author = {Jane Doe and John Smith}, title = {Insights into Environmental Data Science}, journal = {Journal of Data Science}, year = {2021}, volume = {15}, number = {4}, pages = {123-145}, doi = {10.1000/jds.2021.15.4} } Part 2: Using Citations in Markdown Citing in Your Markdown Document Refer to works in your .bib file using citation keys, like [@Doe2021] . Converting Markdown to PDF with Citations Use Pandoc: pandoc yourdoc.md --bibliography=project_references.bib --citeproc -o output.pdf Part 3: Collecting Citations and Research Notes Structuring Your Notes Notes on Doe 2021 [@Doe2021] Key Points: Summary of the article's main arguments. Notable methodologies. Relevance to Our Project: How this research informs our project. Applicable methodologies or theories. Notes on Another Article [@Another2021] Key Points: ... Relevance to Our Project: ... Conclusion This document facilitates efficient management of references and collaborative knowledge building for our project.","title":"Citations"},{"location":"resources/citations/#citation-management-and-notes-collection-in-markdown","text":"","title":"Citation Management and Notes Collection in Markdown"},{"location":"resources/citations/#introduction","text":"This document serves as a guide for managing citations and collecting research notes for our project. We'll use a combination of a .bib file for bibliographic references and Markdown for note-taking.","title":"Introduction"},{"location":"resources/citations/#part-1-setting-up-your-bib-file-for-citations","text":"","title":"Part 1: Setting Up Your .bib File for Citations"},{"location":"resources/citations/#creating-a-bib-file","text":"Create a new file with a .bib extension, for example, project_references.bib . Add bibliographic entries to this file. Each entry should follow the BibTeX format.","title":"Creating a .bib File"},{"location":"resources/citations/#example-of-a-bib-entry","text":"```bibtex @article{Doe2021, author = {Jane Doe and John Smith}, title = {Insights into Environmental Data Science}, journal = {Journal of Data Science}, year = {2021}, volume = {15}, number = {4}, pages = {123-145}, doi = {10.1000/jds.2021.15.4} }","title":"Example of a .bib Entry"},{"location":"resources/citations/#part-2-using-citations-in-markdown","text":"","title":"Part 2: Using Citations in Markdown"},{"location":"resources/citations/#citing-in-your-markdown-document","text":"Refer to works in your .bib file using citation keys, like [@Doe2021] .","title":"Citing in Your Markdown Document"},{"location":"resources/citations/#converting-markdown-to-pdf-with-citations","text":"Use Pandoc: pandoc yourdoc.md --bibliography=project_references.bib --citeproc -o output.pdf","title":"Converting Markdown to PDF with Citations"},{"location":"resources/citations/#part-3-collecting-citations-and-research-notes","text":"","title":"Part 3: Collecting Citations and Research Notes"},{"location":"resources/citations/#structuring-your-notes","text":"","title":"Structuring Your Notes"},{"location":"resources/citations/#notes-on-doe-2021-doe2021","text":"Key Points: Summary of the article's main arguments. Notable methodologies. Relevance to Our Project: How this research informs our project. Applicable methodologies or theories.","title":"Notes on Doe 2021 [@Doe2021]"},{"location":"resources/citations/#notes-on-another-article-another2021","text":"Key Points: ... Relevance to Our Project: ...","title":"Notes on Another Article [@Another2021]"},{"location":"resources/citations/#conclusion","text":"This document facilitates efficient management of references and collaborative knowledge building for our project.","title":"Conclusion"},{"location":"resources/cyverse_basics/","text":"Connecting Cyverse to GitHub Log in to Cyverse Go to the Cyverse user account website https://user.cyverse.org/ Click Sign up (if you do not already have an account) Head over to the Cyverse Discovery Environment https://de.cyverse.org , and log in with your new account. You should now see the Discovery Environment: We will give you permissions to access the Hackathon app. If you haven't already, let us know that you need access Open up an analysis with the hackathon environment (Jupyter Lab) From the Cyverse Discovery Environment, click on Apps in the left menu Select JupyterLab ESIIL Configure and launch your analysis - when choosing the disk size, make sure to choose 64GB or greater. The rest of the settings you can change to suit your computing needs: Click Go to analysis : Now you should see Jupyter Lab! Set up your GitHub credentials If you would prefer to follow a video instead of a written outline, we have prepared a video here: From Jupyter Lab, click on the Git Extension icon on the left menu: Click Clone a Repository and Paste the link to the cyverse-utils https://github.com/CU-ESIIL/cyverse-utils.git and click Clone : You should now see the cyverse-utils folder in your directory tree (provided you haven't changed directories from the default /home/jovyan/data-store Go into the cyverse-utils folder: open up the create_github_keypair.ipynb notebook if you prefer Python or the 'create_github_keypair.R' script if you prefer R by double-clicking and then select the default 'macrosystems' kernel: Now you should see the notebook open. Click the play button at the top. You will be prompted to enter your GitHub username and email: You should now see your Public Key. Copy the WHOLE LINE including ssh-ed25519 at the beginning and the jovyan@... at the end Go to your GitHub settings page (you may need to log in to GitHub first): Select SSH and GPG keys Select New SSH key Give your key a descriptive name, paste your ENTIRE public key in the Key input box, and click Add SSH Key . You may need to re-authenticate with your password or two-factor authentication.: You should now see your new SSH key in your Authentication Keys list! Now you will be able to clone private repositories and push changes to GitHub from your Cyverse analysis! NOTE! Your GitHub authentication is ONLY for the analysis you're working with right now. You will be able to use it as long as you want there, but once you start a new analysis you will need to go through this process again. Feel free to delete keys from old analyses that have been shut down.","title":"Connecting Cyverse to GitHub"},{"location":"resources/cyverse_basics/#connecting-cyverse-to-github","text":"","title":"Connecting Cyverse to GitHub"},{"location":"resources/cyverse_basics/#log-in-to-cyverse","text":"Go to the Cyverse user account website https://user.cyverse.org/ Click Sign up (if you do not already have an account) Head over to the Cyverse Discovery Environment https://de.cyverse.org , and log in with your new account. You should now see the Discovery Environment: We will give you permissions to access the Hackathon app. If you haven't already, let us know that you need access","title":"Log in to Cyverse"},{"location":"resources/cyverse_basics/#open-up-an-analysis-with-the-hackathon-environment-jupyter-lab","text":"From the Cyverse Discovery Environment, click on Apps in the left menu Select JupyterLab ESIIL Configure and launch your analysis - when choosing the disk size, make sure to choose 64GB or greater. The rest of the settings you can change to suit your computing needs: Click Go to analysis : Now you should see Jupyter Lab!","title":"Open up an analysis with the hackathon environment (Jupyter Lab)"},{"location":"resources/cyverse_basics/#set-up-your-github-credentials","text":"","title":"Set up your GitHub credentials"},{"location":"resources/cyverse_basics/#if-you-would-prefer-to-follow-a-video-instead-of-a-written-outline-we-have-prepared-a-video-here","text":"From Jupyter Lab, click on the Git Extension icon on the left menu: Click Clone a Repository and Paste the link to the cyverse-utils https://github.com/CU-ESIIL/cyverse-utils.git and click Clone : You should now see the cyverse-utils folder in your directory tree (provided you haven't changed directories from the default /home/jovyan/data-store Go into the cyverse-utils folder: open up the create_github_keypair.ipynb notebook if you prefer Python or the 'create_github_keypair.R' script if you prefer R by double-clicking and then select the default 'macrosystems' kernel: Now you should see the notebook open. Click the play button at the top. You will be prompted to enter your GitHub username and email: You should now see your Public Key. Copy the WHOLE LINE including ssh-ed25519 at the beginning and the jovyan@... at the end Go to your GitHub settings page (you may need to log in to GitHub first): Select SSH and GPG keys Select New SSH key Give your key a descriptive name, paste your ENTIRE public key in the Key input box, and click Add SSH Key . You may need to re-authenticate with your password or two-factor authentication.: You should now see your new SSH key in your Authentication Keys list! Now you will be able to clone private repositories and push changes to GitHub from your Cyverse analysis! NOTE! Your GitHub authentication is ONLY for the analysis you're working with right now. You will be able to use it as long as you want there, but once you start a new analysis you will need to go through this process again. Feel free to delete keys from old analyses that have been shut down.","title":"If you would prefer to follow a video instead of a written outline, we have prepared a video here:"},{"location":"resources/data_analysis/","text":"Data Analysis Documentation Overview Brief overview of the data analysis goals and the analytical questions being addressed. Analysis Methodology Description of the analytical approach, methods used, and justification for the chosen techniques. Code Overview Explanation of the structure of the analysis code, including key functions and their roles. Running the Analysis Instructions and example commands for executing the analysis scripts. python analysis_script.py Analysis Results Summary of key findings from the analysis, including interpretation and relevance. Challenges and Solutions Discussion of challenges faced during the analysis and solutions or workarounds implemented. Conclusions Concluding remarks on the analysis, insights gained, and their potential impact. Future Work Suggestions for extending or refining the analysis and potential areas for further research. References Citations or references to external sources or literature used.","title":"Data Analysis Documentation"},{"location":"resources/data_analysis/#data-analysis-documentation","text":"","title":"Data Analysis Documentation"},{"location":"resources/data_analysis/#overview","text":"Brief overview of the data analysis goals and the analytical questions being addressed.","title":"Overview"},{"location":"resources/data_analysis/#analysis-methodology","text":"Description of the analytical approach, methods used, and justification for the chosen techniques.","title":"Analysis Methodology"},{"location":"resources/data_analysis/#code-overview","text":"Explanation of the structure of the analysis code, including key functions and their roles.","title":"Code Overview"},{"location":"resources/data_analysis/#running-the-analysis","text":"Instructions and example commands for executing the analysis scripts. python analysis_script.py","title":"Running the Analysis"},{"location":"resources/data_analysis/#analysis-results","text":"Summary of key findings from the analysis, including interpretation and relevance.","title":"Analysis Results"},{"location":"resources/data_analysis/#challenges-and-solutions","text":"Discussion of challenges faced during the analysis and solutions or workarounds implemented.","title":"Challenges and Solutions"},{"location":"resources/data_analysis/#conclusions","text":"Concluding remarks on the analysis, insights gained, and their potential impact.","title":"Conclusions"},{"location":"resources/data_analysis/#future-work","text":"Suggestions for extending or refining the analysis and potential areas for further research.","title":"Future Work"},{"location":"resources/data_analysis/#references","text":"Citations or references to external sources or literature used.","title":"References"},{"location":"resources/data_processing/","text":"Data Processing Documentation Overview Brief description of the data processing objectives and scope. Reminder to adhere to data ownership and usage guidelines. Data Sources List and describe data sources used, including links to cloud-optimized sources. Highlight permissions and compliance with data ownership guidelines. CyVerse Discovery Environment Instructions for setting up and using the CyVerse Discovery Environment for data processing. Tips for cloud-based data access and processing. Data Processing Steps Using GDAL VSI Guidance on using GDAL VSI (Virtual System Interface) for data access and processing. Example commands or scripts: gdal_translate /vsicurl/http://example.com/data.tif output.tif Cloud-Optimized Data Advantages of using cloud-optimized data formats and processing data without downloading. Instructions for such processes. Data Storage Information on storing processed data, with guidelines for choosing between the repository and CyVerse Data Store. Best Practices Recommendations for efficient and responsible data processing in the cloud. Tips to ensure data integrity and reproducibility. Challenges and Troubleshooting Common challenges in data processing and potential solutions. Resources for troubleshooting in the CyVerse Discovery Environment. Conclusions Summary of the data processing phase and its outcomes. Reflect on the methods used. References Citations of tools, data sources, and other references used in the data processing phase.","title":"Data Processing Documentation"},{"location":"resources/data_processing/#data-processing-documentation","text":"","title":"Data Processing Documentation"},{"location":"resources/data_processing/#overview","text":"Brief description of the data processing objectives and scope. Reminder to adhere to data ownership and usage guidelines.","title":"Overview"},{"location":"resources/data_processing/#data-sources","text":"List and describe data sources used, including links to cloud-optimized sources. Highlight permissions and compliance with data ownership guidelines.","title":"Data Sources"},{"location":"resources/data_processing/#cyverse-discovery-environment","text":"Instructions for setting up and using the CyVerse Discovery Environment for data processing. Tips for cloud-based data access and processing.","title":"CyVerse Discovery Environment"},{"location":"resources/data_processing/#data-processing-steps","text":"","title":"Data Processing Steps"},{"location":"resources/data_processing/#using-gdal-vsi","text":"Guidance on using GDAL VSI (Virtual System Interface) for data access and processing. Example commands or scripts: gdal_translate /vsicurl/http://example.com/data.tif output.tif","title":"Using GDAL VSI"},{"location":"resources/data_processing/#cloud-optimized-data","text":"Advantages of using cloud-optimized data formats and processing data without downloading. Instructions for such processes.","title":"Cloud-Optimized Data"},{"location":"resources/data_processing/#data-storage","text":"Information on storing processed data, with guidelines for choosing between the repository and CyVerse Data Store.","title":"Data Storage"},{"location":"resources/data_processing/#best-practices","text":"Recommendations for efficient and responsible data processing in the cloud. Tips to ensure data integrity and reproducibility.","title":"Best Practices"},{"location":"resources/data_processing/#challenges-and-troubleshooting","text":"Common challenges in data processing and potential solutions. Resources for troubleshooting in the CyVerse Discovery Environment.","title":"Challenges and Troubleshooting"},{"location":"resources/data_processing/#conclusions","text":"Summary of the data processing phase and its outcomes. Reflect on the methods used.","title":"Conclusions"},{"location":"resources/data_processing/#references","text":"Citations of tools, data sources, and other references used in the data processing phase.","title":"References"},{"location":"resources/esiil_training/","text":"ESIIL Working Groups training sessions Introduction to ESIIL Training Brief overview of the training program. Objectives and expected outcomes for the working groups. Session 1: The Science of Team Science (2 Hours) Part 1: Creating Ethical and Innovative Work Spaces Strategies for fostering ethical and inclusive environments. Techniques for encouraging innovation and creativity in team settings. Part 2: Effective Communication and Collaboration Best practices for ensuring every team member's voice is heard. Approaches for maintaining productivity and positive team dynamics. Overview of the code of conduct and participant agreement. Session 2: Foundations of Environmental Data Science (2 Hours) Part 1: Data Management, Ethics, and GitHub Usage Principles of data management in environmental science. Understanding data ethics and ownership guidelines. Tour of GitHub repositories and setup instructions for effective collaboration. Part 2: Essential Tools and Technologies Introduction to key tools and technologies used in ESIIL. Basic training on software and platforms essential for data analysis. Session 3: Practical Application and Project Execution (2 Hours) Part 1: Travel Planning and Reimbursement Learn how to manage finances and submit paperwork to the University. Part 2: Hands-on Data Analysis Workflow Interactive session on constructing a data analysis pipeline using ESIIL/CyVerse tools. Practical exercises on data processing, analysis, and visualization techniques. Troubleshooting common issues and optimizing workflow efficiency. Part 3: Wrap-up and Project Planning Strategies for sustaining project momentum and managing long-term research goals. Planning for publication, data sharing, and broader impact. Final Q&A session to address any outstanding questions or concerns. Conclusion and Feedback Summary of key learnings from all sessions. Encouragement for participants to apply these skills in their respective projects. Collection of feedback for future training improvements. Additional Resources List of resources for further learning and exploration. Links to community forums or groups for ongoing support and collaboration. Roundtable Event 1: PI/Team Leads Discussion (2 Hours) A roundtable discussion for Principal Investigators and team leads. Sharing experiences, challenges, and strategies among group leaders. Fostering a collaborative network and problem-solving atmosphere. Roundtable Event 2: Technical Leads Office Hours (2 Hours) A roundtable and office hours session for technical leads. Ensuring a thorough understanding of the ESIIL/CyVerse cyberinfrastructure. Providing technical support and knowledge exchange. Conclusion and Feedback Recap of key takeaways from the training sessions and roundtables. Collection of feedback for continuous improvement of the training program. Additional Resources Supplementary materials, reading lists, and links to online tutorials and documentation.","title":"ESIIL Working Groups training sessions"},{"location":"resources/esiil_training/#esiil-working-groups-training-sessions","text":"","title":"ESIIL Working Groups training sessions"},{"location":"resources/esiil_training/#introduction-to-esiil-training","text":"Brief overview of the training program. Objectives and expected outcomes for the working groups.","title":"Introduction to ESIIL Training"},{"location":"resources/esiil_training/#session-1-the-science-of-team-science-2-hours","text":"","title":"Session 1: The Science of Team Science (2 Hours)"},{"location":"resources/esiil_training/#part-1-creating-ethical-and-innovative-work-spaces","text":"Strategies for fostering ethical and inclusive environments. Techniques for encouraging innovation and creativity in team settings.","title":"Part 1: Creating Ethical and Innovative Work Spaces"},{"location":"resources/esiil_training/#part-2-effective-communication-and-collaboration","text":"Best practices for ensuring every team member's voice is heard. Approaches for maintaining productivity and positive team dynamics. Overview of the code of conduct and participant agreement.","title":"Part 2: Effective Communication and Collaboration"},{"location":"resources/esiil_training/#session-2-foundations-of-environmental-data-science-2-hours","text":"","title":"Session 2: Foundations of Environmental Data Science (2 Hours)"},{"location":"resources/esiil_training/#part-1-data-management-ethics-and-github-usage","text":"Principles of data management in environmental science. Understanding data ethics and ownership guidelines. Tour of GitHub repositories and setup instructions for effective collaboration.","title":"Part 1: Data Management, Ethics, and GitHub Usage"},{"location":"resources/esiil_training/#part-2-essential-tools-and-technologies","text":"Introduction to key tools and technologies used in ESIIL. Basic training on software and platforms essential for data analysis.","title":"Part 2: Essential Tools and Technologies"},{"location":"resources/esiil_training/#session-3-practical-application-and-project-execution-2-hours","text":"","title":"Session 3: Practical Application and Project Execution (2 Hours)"},{"location":"resources/esiil_training/#part-1-travel-planning-and-reimbursement","text":"Learn how to manage finances and submit paperwork to the University.","title":"Part 1: Travel Planning and Reimbursement"},{"location":"resources/esiil_training/#part-2-hands-on-data-analysis-workflow","text":"Interactive session on constructing a data analysis pipeline using ESIIL/CyVerse tools. Practical exercises on data processing, analysis, and visualization techniques. Troubleshooting common issues and optimizing workflow efficiency.","title":"Part 2: Hands-on Data Analysis Workflow"},{"location":"resources/esiil_training/#part-3-wrap-up-and-project-planning","text":"Strategies for sustaining project momentum and managing long-term research goals. Planning for publication, data sharing, and broader impact. Final Q&A session to address any outstanding questions or concerns.","title":"Part 3: Wrap-up and Project Planning"},{"location":"resources/esiil_training/#conclusion-and-feedback","text":"Summary of key learnings from all sessions. Encouragement for participants to apply these skills in their respective projects. Collection of feedback for future training improvements.","title":"Conclusion and Feedback"},{"location":"resources/esiil_training/#additional-resources","text":"List of resources for further learning and exploration. Links to community forums or groups for ongoing support and collaboration.","title":"Additional Resources"},{"location":"resources/esiil_training/#roundtable-event-1-piteam-leads-discussion-2-hours","text":"A roundtable discussion for Principal Investigators and team leads. Sharing experiences, challenges, and strategies among group leaders. Fostering a collaborative network and problem-solving atmosphere.","title":"Roundtable Event 1: PI/Team Leads Discussion (2 Hours)"},{"location":"resources/esiil_training/#roundtable-event-2-technical-leads-office-hours-2-hours","text":"A roundtable and office hours session for technical leads. Ensuring a thorough understanding of the ESIIL/CyVerse cyberinfrastructure. Providing technical support and knowledge exchange.","title":"Roundtable Event 2: Technical Leads Office Hours (2 Hours)"},{"location":"resources/esiil_training/#conclusion-and-feedback_1","text":"Recap of key takeaways from the training sessions and roundtables. Collection of feedback for continuous improvement of the training program.","title":"Conclusion and Feedback"},{"location":"resources/esiil_training/#additional-resources_1","text":"Supplementary materials, reading lists, and links to online tutorials and documentation.","title":"Additional Resources"},{"location":"resources/first_meeting_notes/","text":"Primary Meeting 1 Day 1-5: Project Kickoff and Strategy Meeting Details Dates: Times: Location: Facilitator: Attendees List of attendees Daily Agenda Day 1: Setting the Stage Opening Remarks Welcoming speech and outline of the week's objectives. Project Overview Presentation of the project goals and significance. Theoretical Framework Discussion on the theoretical underpinnings of the project. Data Overview Review available data and any gaps that need addressing. Day 2-4: Deep Dives Daily Goals Outline specific goals for each day. Task Assignments Assign tasks and areas of responsibility to team members. Theory and Data Synthesis Host focused discussions on how theory will inform data analysis. Explore different methodological approaches and data integration strategies. Evening Social and Soft Work Sessions Casual gatherings to further discuss ideas and foster team bonding. Day 5: Roadmap and Closure Project Roadmap Draft a detailed plan of action for the project going forward. Responsibilities Confirm individual responsibilities and deadlines. Review and Feedback Reflect on the week's discussions and adjust the project plan as needed. Closing Remarks Summarize achievements and express appreciation for the team's efforts. Detailed Notes Day 1 Notes Summary of discussions, decisions, and key points. Day 2 Notes ... Day 3 Notes ... Day 4 Notes ... Day 5 Notes ... Action Items [ ] Specific task: Assigned to - Deadline [ ] Specific task: Assigned to - Deadline ... Reflections and Comments (Space for any additional thoughts, insights, or personal reflections on the meeting.) Next Steps Schedule for follow-up meetings or checkpoints. Outline of expected progress before the next primary meeting. Additional Documentation (Include or link to any additional documents, charts, or resources that were created or referenced during the meeting.)","title":"Primary Meeting 1"},{"location":"resources/first_meeting_notes/#primary-meeting-1","text":"","title":"Primary Meeting 1"},{"location":"resources/first_meeting_notes/#day-1-5-project-kickoff-and-strategy","text":"","title":"Day 1-5: Project Kickoff and Strategy"},{"location":"resources/first_meeting_notes/#meeting-details","text":"Dates: Times: Location: Facilitator:","title":"Meeting Details"},{"location":"resources/first_meeting_notes/#attendees","text":"List of attendees","title":"Attendees"},{"location":"resources/first_meeting_notes/#daily-agenda","text":"","title":"Daily Agenda"},{"location":"resources/first_meeting_notes/#day-1-setting-the-stage","text":"","title":"Day 1: Setting the Stage"},{"location":"resources/first_meeting_notes/#opening-remarks","text":"Welcoming speech and outline of the week's objectives.","title":"Opening Remarks"},{"location":"resources/first_meeting_notes/#project-overview","text":"Presentation of the project goals and significance.","title":"Project Overview"},{"location":"resources/first_meeting_notes/#theoretical-framework","text":"Discussion on the theoretical underpinnings of the project.","title":"Theoretical Framework"},{"location":"resources/first_meeting_notes/#data-overview","text":"Review available data and any gaps that need addressing.","title":"Data Overview"},{"location":"resources/first_meeting_notes/#day-2-4-deep-dives","text":"","title":"Day 2-4: Deep Dives"},{"location":"resources/first_meeting_notes/#daily-goals","text":"Outline specific goals for each day.","title":"Daily Goals"},{"location":"resources/first_meeting_notes/#task-assignments","text":"Assign tasks and areas of responsibility to team members.","title":"Task Assignments"},{"location":"resources/first_meeting_notes/#theory-and-data-synthesis","text":"Host focused discussions on how theory will inform data analysis. Explore different methodological approaches and data integration strategies.","title":"Theory and Data Synthesis"},{"location":"resources/first_meeting_notes/#evening-social-and-soft-work-sessions","text":"Casual gatherings to further discuss ideas and foster team bonding.","title":"Evening Social and Soft Work Sessions"},{"location":"resources/first_meeting_notes/#day-5-roadmap-and-closure","text":"","title":"Day 5: Roadmap and Closure"},{"location":"resources/first_meeting_notes/#project-roadmap","text":"Draft a detailed plan of action for the project going forward.","title":"Project Roadmap"},{"location":"resources/first_meeting_notes/#responsibilities","text":"Confirm individual responsibilities and deadlines.","title":"Responsibilities"},{"location":"resources/first_meeting_notes/#review-and-feedback","text":"Reflect on the week's discussions and adjust the project plan as needed.","title":"Review and Feedback"},{"location":"resources/first_meeting_notes/#closing-remarks","text":"Summarize achievements and express appreciation for the team's efforts.","title":"Closing Remarks"},{"location":"resources/first_meeting_notes/#detailed-notes","text":"","title":"Detailed Notes"},{"location":"resources/first_meeting_notes/#day-1-notes","text":"Summary of discussions, decisions, and key points.","title":"Day 1 Notes"},{"location":"resources/first_meeting_notes/#day-2-notes","text":"...","title":"Day 2 Notes"},{"location":"resources/first_meeting_notes/#day-3-notes","text":"...","title":"Day 3 Notes"},{"location":"resources/first_meeting_notes/#day-4-notes","text":"...","title":"Day 4 Notes"},{"location":"resources/first_meeting_notes/#day-5-notes","text":"...","title":"Day 5 Notes"},{"location":"resources/first_meeting_notes/#action-items","text":"[ ] Specific task: Assigned to - Deadline [ ] Specific task: Assigned to - Deadline ...","title":"Action Items"},{"location":"resources/first_meeting_notes/#reflections-and-comments","text":"(Space for any additional thoughts, insights, or personal reflections on the meeting.)","title":"Reflections and Comments"},{"location":"resources/first_meeting_notes/#next-steps","text":"Schedule for follow-up meetings or checkpoints. Outline of expected progress before the next primary meeting.","title":"Next Steps"},{"location":"resources/first_meeting_notes/#additional-documentation","text":"(Include or link to any additional documents, charts, or resources that were created or referenced during the meeting.)","title":"Additional Documentation"},{"location":"resources/manuscript/","text":"Manuscript Title Authors Author 1, Affiliation Author 2, Affiliation ... Abstract A brief summary of the research, its objectives, main findings, and conclusions. Introduction Background information and context setting for the research. Statement of the problem and research objectives. Overview of the methodology and approach. Literature Review Discussion of relevant previous work and how this research contributes to the field. Methodology Detailed description of the research methodology. Explanation of data collection and analysis techniques. Justification for methodological choices. Results Presentation of the research findings. Use of tables, graphs, and figures to illustrate key points. Analysis and interpretation of the results. Discussion Discussion of the implications of the findings. Comparison with previous research in the field. Consideration of the limitations of the study. Conclusion Summary of the main findings. Reflection on the research's significance and potential impact. Suggestions for future research directions. Acknowledgements Acknowledgement of any assistance, funding, or contributions from others. References Bibliographic details of the cited works. Use a consistent citation style throughout. Appendices Additional material that supports the manuscript but is too detailed for the main sections.","title":"Manuscript"},{"location":"resources/manuscript/#manuscript-title","text":"","title":"Manuscript Title"},{"location":"resources/manuscript/#authors","text":"Author 1, Affiliation Author 2, Affiliation ...","title":"Authors"},{"location":"resources/manuscript/#abstract","text":"A brief summary of the research, its objectives, main findings, and conclusions.","title":"Abstract"},{"location":"resources/manuscript/#introduction","text":"Background information and context setting for the research. Statement of the problem and research objectives. Overview of the methodology and approach.","title":"Introduction"},{"location":"resources/manuscript/#literature-review","text":"Discussion of relevant previous work and how this research contributes to the field.","title":"Literature Review"},{"location":"resources/manuscript/#methodology","text":"Detailed description of the research methodology. Explanation of data collection and analysis techniques. Justification for methodological choices.","title":"Methodology"},{"location":"resources/manuscript/#results","text":"Presentation of the research findings. Use of tables, graphs, and figures to illustrate key points. Analysis and interpretation of the results.","title":"Results"},{"location":"resources/manuscript/#discussion","text":"Discussion of the implications of the findings. Comparison with previous research in the field. Consideration of the limitations of the study.","title":"Discussion"},{"location":"resources/manuscript/#conclusion","text":"Summary of the main findings. Reflection on the research's significance and potential impact. Suggestions for future research directions.","title":"Conclusion"},{"location":"resources/manuscript/#acknowledgements","text":"Acknowledgement of any assistance, funding, or contributions from others.","title":"Acknowledgements"},{"location":"resources/manuscript/#references","text":"Bibliographic details of the cited works. Use a consistent citation style throughout.","title":"References"},{"location":"resources/manuscript/#appendices","text":"Additional material that supports the manuscript but is too detailed for the main sections.","title":"Appendices"},{"location":"resources/notes_from_readings/","text":"Literature Reading Notes Reference Information Title: Authors: Publication Year: Journal/Source: DOI/URL: Summary Brief summary of the main objective, research question, or thesis of the literature. Key Findings Major findings or conclusions: Finding 1 Finding 2 ... Methodology Description of research methodology, techniques, or approaches. Notable tools, datasets, or analytical methods used. Theoretical Framework Theoretical models or frameworks underpinning the research. Positioning within the broader field. Critical Analysis Strengths: Well-executed aspects or convincing arguments. Limitations: Weaknesses, gaps, or biases. Insights: New understandings or perspectives gained. Connections to Other Work Similarities or differences with other readings. Complementarity to other studies. Quotations and Notes Significant quotes: \"Quote here.\" - Author Name, page number Additional notes or comments. Personal Reflections Influence on understanding or perspective. Potential impact on future research or studies. Action Items Follow-up actions such as readings, discussions, or research activities: [ ] Action item 1 [ ] Action item 2 ...","title":"Notes from readings"},{"location":"resources/notes_from_readings/#literature-reading-notes","text":"","title":"Literature Reading Notes"},{"location":"resources/notes_from_readings/#reference-information","text":"Title: Authors: Publication Year: Journal/Source: DOI/URL:","title":"Reference Information"},{"location":"resources/notes_from_readings/#summary","text":"Brief summary of the main objective, research question, or thesis of the literature.","title":"Summary"},{"location":"resources/notes_from_readings/#key-findings","text":"Major findings or conclusions: Finding 1 Finding 2 ...","title":"Key Findings"},{"location":"resources/notes_from_readings/#methodology","text":"Description of research methodology, techniques, or approaches. Notable tools, datasets, or analytical methods used.","title":"Methodology"},{"location":"resources/notes_from_readings/#theoretical-framework","text":"Theoretical models or frameworks underpinning the research. Positioning within the broader field.","title":"Theoretical Framework"},{"location":"resources/notes_from_readings/#critical-analysis","text":"Strengths: Well-executed aspects or convincing arguments. Limitations: Weaknesses, gaps, or biases. Insights: New understandings or perspectives gained.","title":"Critical Analysis"},{"location":"resources/notes_from_readings/#connections-to-other-work","text":"Similarities or differences with other readings. Complementarity to other studies.","title":"Connections to Other Work"},{"location":"resources/notes_from_readings/#quotations-and-notes","text":"Significant quotes: \"Quote here.\" - Author Name, page number Additional notes or comments.","title":"Quotations and Notes"},{"location":"resources/notes_from_readings/#personal-reflections","text":"Influence on understanding or perspective. Potential impact on future research or studies.","title":"Personal Reflections"},{"location":"resources/notes_from_readings/#action-items","text":"Follow-up actions such as readings, discussions, or research activities: [ ] Action item 1 [ ] Action item 2 ...","title":"Action Items"},{"location":"resources/post_meeting_notes/","text":"Post-Meeting Notes Template Meeting Details Date: Time: Location: Facilitator: Attendees List of attendees Agenda 1. Review of Meeting Goals Recap the primary objectives and if they were met. 2. Manuscript Development Discuss the status of current manuscript drafts. Assign writing and editing tasks for different sections of the manuscript. Set deadlines for draft completion and review. 3. Research Highlights Identify key findings and outcomes that should be emphasized in the publications. Discuss any new research insights that emerged from the meeting. 4. Publication Strategy Decide on target journals or conferences for publication submission. Discuss authorship order and contributions. Plan for any additional data or research needed to strengthen the manuscript. 5. Editing and Review Process Establish a peer-review process within the group for initial feedback. Assign members to focus on specific aspects of editing, such as clarity, grammar, and technical accuracy. Agree on a schedule for review rounds to ensure timely submission. 6. Responsibilities and Expectations Clearly define what is expected from each member before the next meeting. Discuss communication methods for progress updates and questions. 7. Closing Remarks Summarize the discussion and confirm the action plan. Reiterate the importance of meeting the set deadlines and maintaining communication. Action Items [ ] Draft introduction section: Responsible person(s) - Deadline [ ] Compile and analyze additional data: Responsible person(s) - Deadline [ ] Draft methodology section: Responsible person(s) - Deadline ... [ ] Coordinate manuscript peer review: Responsible person(s) - Deadline Next Steps Define the timeline for the submission process. Schedule follow-up meetings or check-ins to monitor progress. Notes (Additional notes, comments, or observations made during the meeting.)","title":"Post-Meeting Notes Template"},{"location":"resources/post_meeting_notes/#post-meeting-notes-template","text":"","title":"Post-Meeting Notes Template"},{"location":"resources/post_meeting_notes/#meeting-details","text":"Date: Time: Location: Facilitator:","title":"Meeting Details"},{"location":"resources/post_meeting_notes/#attendees","text":"List of attendees","title":"Attendees"},{"location":"resources/post_meeting_notes/#agenda","text":"","title":"Agenda"},{"location":"resources/post_meeting_notes/#1-review-of-meeting-goals","text":"Recap the primary objectives and if they were met.","title":"1. Review of Meeting Goals"},{"location":"resources/post_meeting_notes/#2-manuscript-development","text":"Discuss the status of current manuscript drafts. Assign writing and editing tasks for different sections of the manuscript. Set deadlines for draft completion and review.","title":"2. Manuscript Development"},{"location":"resources/post_meeting_notes/#3-research-highlights","text":"Identify key findings and outcomes that should be emphasized in the publications. Discuss any new research insights that emerged from the meeting.","title":"3. Research Highlights"},{"location":"resources/post_meeting_notes/#4-publication-strategy","text":"Decide on target journals or conferences for publication submission. Discuss authorship order and contributions. Plan for any additional data or research needed to strengthen the manuscript.","title":"4. Publication Strategy"},{"location":"resources/post_meeting_notes/#5-editing-and-review-process","text":"Establish a peer-review process within the group for initial feedback. Assign members to focus on specific aspects of editing, such as clarity, grammar, and technical accuracy. Agree on a schedule for review rounds to ensure timely submission.","title":"5. Editing and Review Process"},{"location":"resources/post_meeting_notes/#6-responsibilities-and-expectations","text":"Clearly define what is expected from each member before the next meeting. Discuss communication methods for progress updates and questions.","title":"6. Responsibilities and Expectations"},{"location":"resources/post_meeting_notes/#7-closing-remarks","text":"Summarize the discussion and confirm the action plan. Reiterate the importance of meeting the set deadlines and maintaining communication.","title":"7. Closing Remarks"},{"location":"resources/post_meeting_notes/#action-items","text":"[ ] Draft introduction section: Responsible person(s) - Deadline [ ] Compile and analyze additional data: Responsible person(s) - Deadline [ ] Draft methodology section: Responsible person(s) - Deadline ... [ ] Coordinate manuscript peer review: Responsible person(s) - Deadline","title":"Action Items"},{"location":"resources/post_meeting_notes/#next-steps","text":"Define the timeline for the submission process. Schedule follow-up meetings or check-ins to monitor progress.","title":"Next Steps"},{"location":"resources/post_meeting_notes/#notes","text":"(Additional notes, comments, or observations made during the meeting.)","title":"Notes"},{"location":"resources/pre_meeting_notes/","text":"Pre-Meeting Notes Meeting Details Date: Time: Location: Facilitator: Attendees List of attendees Agenda 1. Opening Remarks Brief welcome and overview of the meeting's objectives. 2. Introductions Roundtable introductions for all attendees. Share a personal note or interesting fact to foster camaraderie. 3. Planning Discuss the agenda for the primary meetings. Outline the key topics and issues to address. Assign roles for note-taking, timekeeping, and facilitation in primary meetings. 4. Goal Setting Establish clear, actionable goals for the upcoming period. Identify specific outcomes desired from the primary meetings. Agree on metrics or indicators of success for these goals. 5. Camaraderie Building Icebreaker activity or team-building exercise. Share expectations and aspirations for the group's progress. Highlight the importance of collaboration and mutual support. 6. Open Discussion Allow for any additional topics, concerns, or ideas to be brought forward. 7. Closing Remarks Summarize the discussions and confirm the next steps. Confirm dates and times for primary meetings. Express appreciation for participation. Action Items [ ] Action item 1: Responsible person(s) - Deadline [ ] Action item 2: Responsible person(s) - Deadline ... Notes (Any additional notes or comments about the meeting.)","title":"Pre-Meeting Notes"},{"location":"resources/pre_meeting_notes/#pre-meeting-notes","text":"","title":"Pre-Meeting Notes"},{"location":"resources/pre_meeting_notes/#meeting-details","text":"Date: Time: Location: Facilitator:","title":"Meeting Details"},{"location":"resources/pre_meeting_notes/#attendees","text":"List of attendees","title":"Attendees"},{"location":"resources/pre_meeting_notes/#agenda","text":"","title":"Agenda"},{"location":"resources/pre_meeting_notes/#1-opening-remarks","text":"Brief welcome and overview of the meeting's objectives.","title":"1. Opening Remarks"},{"location":"resources/pre_meeting_notes/#2-introductions","text":"Roundtable introductions for all attendees. Share a personal note or interesting fact to foster camaraderie.","title":"2. Introductions"},{"location":"resources/pre_meeting_notes/#3-planning","text":"Discuss the agenda for the primary meetings. Outline the key topics and issues to address. Assign roles for note-taking, timekeeping, and facilitation in primary meetings.","title":"3. Planning"},{"location":"resources/pre_meeting_notes/#4-goal-setting","text":"Establish clear, actionable goals for the upcoming period. Identify specific outcomes desired from the primary meetings. Agree on metrics or indicators of success for these goals.","title":"4. Goal Setting"},{"location":"resources/pre_meeting_notes/#5-camaraderie-building","text":"Icebreaker activity or team-building exercise. Share expectations and aspirations for the group's progress. Highlight the importance of collaboration and mutual support.","title":"5. Camaraderie Building"},{"location":"resources/pre_meeting_notes/#6-open-discussion","text":"Allow for any additional topics, concerns, or ideas to be brought forward.","title":"6. Open Discussion"},{"location":"resources/pre_meeting_notes/#7-closing-remarks","text":"Summarize the discussions and confirm the next steps. Confirm dates and times for primary meetings. Express appreciation for participation.","title":"7. Closing Remarks"},{"location":"resources/pre_meeting_notes/#action-items","text":"[ ] Action item 1: Responsible person(s) - Deadline [ ] Action item 2: Responsible person(s) - Deadline ...","title":"Action Items"},{"location":"resources/pre_meeting_notes/#notes","text":"(Any additional notes or comments about the meeting.)","title":"Notes"},{"location":"resources/second_meeting_notes/","text":"Primary Meeting Day 6-10: Progress and Development Meeting Details Dates: Times: Location: Facilitator: Attendees List of attendees Daily Agenda Day 6: Review and Refine Recap of Previous Sessions Summary of progress made since the last meeting. Review of action items and milestones achieved. Refinement of Goals and Tasks Reassessment and adjustment of goals based on current progress. Identification of any new challenges or opportunities. Day 7-9: In-Depth Work Sessions Daily Goals Clear objectives for each day\u2019s work sessions. Task Progress Updates Brief reports from team members on their assigned tasks. Collaborative problem-solving for any issues encountered. Theory and Data Integration Continued discussions on aligning theoretical frameworks with data analysis. Workshops or breakout sessions for detailed aspects of the project. Evening Collaborative Activities Informal sessions to encourage ongoing dialogue and collaboration. Day 10: Mid-Point Review Progress Evaluation Assessment of the work done during the week. Feedback sessions to ensure quality and consistency in outputs. Documentation and Record-Keeping Ensure thorough documentation of methods, results, and decisions. Establish a system for organizing and sharing this documentation. Planning Forward Setting objectives for the next phase of the project. Adjusting the roadmap as necessary based on insights from the week\u2019s work. Detailed Notes Day 6 Notes ... Day 7 Notes ... Day 8 Notes ... Day 9 Notes ... Day 10 Notes ... Action Items [ ] Specific task: Assigned to - Deadline [ ] Specific task: Assigned to - Deadline ... Reflections and Comments (Space for any additional thoughts, insights, or personal reflections on the meeting.)","title":"Primary Meeting Day 6-10: Progress and Development"},{"location":"resources/second_meeting_notes/#primary-meeting-day-6-10-progress-and-development","text":"","title":"Primary Meeting Day 6-10: Progress and Development"},{"location":"resources/second_meeting_notes/#meeting-details","text":"Dates: Times: Location: Facilitator:","title":"Meeting Details"},{"location":"resources/second_meeting_notes/#attendees","text":"List of attendees","title":"Attendees"},{"location":"resources/second_meeting_notes/#daily-agenda","text":"","title":"Daily Agenda"},{"location":"resources/second_meeting_notes/#day-6-review-and-refine","text":"","title":"Day 6: Review and Refine"},{"location":"resources/second_meeting_notes/#recap-of-previous-sessions","text":"Summary of progress made since the last meeting. Review of action items and milestones achieved.","title":"Recap of Previous Sessions"},{"location":"resources/second_meeting_notes/#refinement-of-goals-and-tasks","text":"Reassessment and adjustment of goals based on current progress. Identification of any new challenges or opportunities.","title":"Refinement of Goals and Tasks"},{"location":"resources/second_meeting_notes/#day-7-9-in-depth-work-sessions","text":"","title":"Day 7-9: In-Depth Work Sessions"},{"location":"resources/second_meeting_notes/#daily-goals","text":"Clear objectives for each day\u2019s work sessions.","title":"Daily Goals"},{"location":"resources/second_meeting_notes/#task-progress-updates","text":"Brief reports from team members on their assigned tasks. Collaborative problem-solving for any issues encountered.","title":"Task Progress Updates"},{"location":"resources/second_meeting_notes/#theory-and-data-integration","text":"Continued discussions on aligning theoretical frameworks with data analysis. Workshops or breakout sessions for detailed aspects of the project.","title":"Theory and Data Integration"},{"location":"resources/second_meeting_notes/#evening-collaborative-activities","text":"Informal sessions to encourage ongoing dialogue and collaboration.","title":"Evening Collaborative Activities"},{"location":"resources/second_meeting_notes/#day-10-mid-point-review","text":"","title":"Day 10: Mid-Point Review"},{"location":"resources/second_meeting_notes/#progress-evaluation","text":"Assessment of the work done during the week. Feedback sessions to ensure quality and consistency in outputs.","title":"Progress Evaluation"},{"location":"resources/second_meeting_notes/#documentation-and-record-keeping","text":"Ensure thorough documentation of methods, results, and decisions. Establish a system for organizing and sharing this documentation.","title":"Documentation and Record-Keeping"},{"location":"resources/second_meeting_notes/#planning-forward","text":"Setting objectives for the next phase of the project. Adjusting the roadmap as necessary based on insights from the week\u2019s work.","title":"Planning Forward"},{"location":"resources/second_meeting_notes/#detailed-notes","text":"","title":"Detailed Notes"},{"location":"resources/second_meeting_notes/#day-6-notes","text":"...","title":"Day 6 Notes"},{"location":"resources/second_meeting_notes/#day-7-notes","text":"...","title":"Day 7 Notes"},{"location":"resources/second_meeting_notes/#day-8-notes","text":"...","title":"Day 8 Notes"},{"location":"resources/second_meeting_notes/#day-9-notes","text":"...","title":"Day 9 Notes"},{"location":"resources/second_meeting_notes/#day-10-notes","text":"...","title":"Day 10 Notes"},{"location":"resources/second_meeting_notes/#action-items","text":"[ ] Specific task: Assigned to - Deadline [ ] Specific task: Assigned to - Deadline ...","title":"Action Items"},{"location":"resources/second_meeting_notes/#reflections-and-comments","text":"(Space for any additional thoughts, insights, or personal reflections on the meeting.)","title":"Reflections and Comments"},{"location":"resources/third_meeting_notes/","text":"Primary Meeting Day 11-15: Finalization and Conclusion Meeting Details Dates: Times: Location: Facilitator: Attendees List of attendees Daily Agenda Day 11: Alignment and Focus Realigning Objectives Review the project's main goals to ensure alignment with the final output. Address any misalignments or deviations from the original plan. Prioritization of Tasks Identify critical tasks that need to be completed. Allocate resources and efforts to ensure these priorities are met. Day 12-14: Intensive Work Period Task Completion Dedicated time for team members to complete their individual contributions. Regular check-ins to track progress and address any blockers. Integration of Work Begin to combine individual contributions into a cohesive whole. Review the integration to ensure consistency and coherency across the project. Final Reviews and Edits Conduct thorough reviews of the project's outputs. Perform final edits to refine the quality of the work. Day 15: Closure and Celebration Final Presentation Present the completed project to the group. Discuss any last-minute adjustments or refinements needed. Reflective Session Reflect on the achievements and learnings from the project. Share appreciation for the team's hard work and dedication. Celebration Acknowledge the successful completion of the project. Plan for any dissemination of the project's findings or outputs. Detailed Notes Day 11 Notes ... Day 12 Notes ... Day 13 Notes ... Day 14 Notes ... Day 15 Notes ... Action Items [ ] Finalize manuscript for publication: Assigned to - Deadline [ ] Prepare data for repository submission: Assigned to - Deadline [ ] Organize project materials for archival: Assigned to - Deadline ... Reflections and Comments (Space for any additional thoughts, insights, or personal reflections on the meeting and the project as a whole.) Next Steps Define the publication and dissemination plan. Outline any follow-up research or projects that have stemmed from this work. Additional Documentation (Include or link to any additional documents, charts, or resources that were created or referenced during the meeting.)","title":"Primary Meeting Day 11-15: Finalization and Conclusion"},{"location":"resources/third_meeting_notes/#primary-meeting-day-11-15-finalization-and-conclusion","text":"","title":"Primary Meeting Day 11-15: Finalization and Conclusion"},{"location":"resources/third_meeting_notes/#meeting-details","text":"Dates: Times: Location: Facilitator:","title":"Meeting Details"},{"location":"resources/third_meeting_notes/#attendees","text":"List of attendees","title":"Attendees"},{"location":"resources/third_meeting_notes/#daily-agenda","text":"","title":"Daily Agenda"},{"location":"resources/third_meeting_notes/#day-11-alignment-and-focus","text":"","title":"Day 11: Alignment and Focus"},{"location":"resources/third_meeting_notes/#realigning-objectives","text":"Review the project's main goals to ensure alignment with the final output. Address any misalignments or deviations from the original plan.","title":"Realigning Objectives"},{"location":"resources/third_meeting_notes/#prioritization-of-tasks","text":"Identify critical tasks that need to be completed. Allocate resources and efforts to ensure these priorities are met.","title":"Prioritization of Tasks"},{"location":"resources/third_meeting_notes/#day-12-14-intensive-work-period","text":"","title":"Day 12-14: Intensive Work Period"},{"location":"resources/third_meeting_notes/#task-completion","text":"Dedicated time for team members to complete their individual contributions. Regular check-ins to track progress and address any blockers.","title":"Task Completion"},{"location":"resources/third_meeting_notes/#integration-of-work","text":"Begin to combine individual contributions into a cohesive whole. Review the integration to ensure consistency and coherency across the project.","title":"Integration of Work"},{"location":"resources/third_meeting_notes/#final-reviews-and-edits","text":"Conduct thorough reviews of the project's outputs. Perform final edits to refine the quality of the work.","title":"Final Reviews and Edits"},{"location":"resources/third_meeting_notes/#day-15-closure-and-celebration","text":"","title":"Day 15: Closure and Celebration"},{"location":"resources/third_meeting_notes/#final-presentation","text":"Present the completed project to the group. Discuss any last-minute adjustments or refinements needed.","title":"Final Presentation"},{"location":"resources/third_meeting_notes/#reflective-session","text":"Reflect on the achievements and learnings from the project. Share appreciation for the team's hard work and dedication.","title":"Reflective Session"},{"location":"resources/third_meeting_notes/#celebration","text":"Acknowledge the successful completion of the project. Plan for any dissemination of the project's findings or outputs.","title":"Celebration"},{"location":"resources/third_meeting_notes/#detailed-notes","text":"","title":"Detailed Notes"},{"location":"resources/third_meeting_notes/#day-11-notes","text":"...","title":"Day 11 Notes"},{"location":"resources/third_meeting_notes/#day-12-notes","text":"...","title":"Day 12 Notes"},{"location":"resources/third_meeting_notes/#day-13-notes","text":"...","title":"Day 13 Notes"},{"location":"resources/third_meeting_notes/#day-14-notes","text":"...","title":"Day 14 Notes"},{"location":"resources/third_meeting_notes/#day-15-notes","text":"...","title":"Day 15 Notes"},{"location":"resources/third_meeting_notes/#action-items","text":"[ ] Finalize manuscript for publication: Assigned to - Deadline [ ] Prepare data for repository submission: Assigned to - Deadline [ ] Organize project materials for archival: Assigned to - Deadline ...","title":"Action Items"},{"location":"resources/third_meeting_notes/#reflections-and-comments","text":"(Space for any additional thoughts, insights, or personal reflections on the meeting and the project as a whole.)","title":"Reflections and Comments"},{"location":"resources/third_meeting_notes/#next-steps","text":"Define the publication and dissemination plan. Outline any follow-up research or projects that have stemmed from this work.","title":"Next Steps"},{"location":"resources/third_meeting_notes/#additional-documentation","text":"(Include or link to any additional documents, charts, or resources that were created or referenced during the meeting.)","title":"Additional Documentation"},{"location":"resources/visualizations/","text":"Visualization Strategy and Development Documentation Overview Brief overview of the visualization goals and their alignment with the overall project objectives. Visualization Strategy Identifying Key Messages Discuss main messages or insights to communicate through visualizations. Identify target audience and their specific needs. Selecting Appropriate Visualization Types Explore different types of visualizations (charts, graphs, 3D, interactive elements) suitable for the data and message. Brainstorm creative visualization approaches. Visualization Development Code-Generated Visualizations Outline initial visualizations generated from the data pipeline. Include code snippets and explanations. # Example Python code for a basic plot import matplotlib.pyplot as plt plt.plot(data['x'], data['y']) plt.show() Enhancing Visualizations Steps for annotating, animating, creating 3D, immersive, or interactive visualizations. Discuss challenges and solutions in enhancing visuals. Versioning and Iterations Document different versions and iterations of visualizations. Reflect on improvements or changes in each version. Finalizing Visualizations Process of finalizing visuals for presentation or publication. Feedback incorporation from team or test audiences. Documentation of Tools and Resources List software, libraries, and tools used for visualization. Reference external resources or tutorials. Conclusions Summarize the visualization process and contributions to the project. Reflect on lessons learned and potential future improvements. References Cite external sources, inspirations, or frameworks used in visualization.","title":"Visualization Strategy and Development Documentation"},{"location":"resources/visualizations/#visualization-strategy-and-development-documentation","text":"","title":"Visualization Strategy and Development Documentation"},{"location":"resources/visualizations/#overview","text":"Brief overview of the visualization goals and their alignment with the overall project objectives.","title":"Overview"},{"location":"resources/visualizations/#visualization-strategy","text":"","title":"Visualization Strategy"},{"location":"resources/visualizations/#identifying-key-messages","text":"Discuss main messages or insights to communicate through visualizations. Identify target audience and their specific needs.","title":"Identifying Key Messages"},{"location":"resources/visualizations/#selecting-appropriate-visualization-types","text":"Explore different types of visualizations (charts, graphs, 3D, interactive elements) suitable for the data and message. Brainstorm creative visualization approaches.","title":"Selecting Appropriate Visualization Types"},{"location":"resources/visualizations/#visualization-development","text":"","title":"Visualization Development"},{"location":"resources/visualizations/#code-generated-visualizations","text":"Outline initial visualizations generated from the data pipeline. Include code snippets and explanations. # Example Python code for a basic plot import matplotlib.pyplot as plt plt.plot(data['x'], data['y']) plt.show()","title":"Code-Generated Visualizations"},{"location":"resources/visualizations/#enhancing-visualizations","text":"Steps for annotating, animating, creating 3D, immersive, or interactive visualizations. Discuss challenges and solutions in enhancing visuals.","title":"Enhancing Visualizations"},{"location":"resources/visualizations/#versioning-and-iterations","text":"Document different versions and iterations of visualizations. Reflect on improvements or changes in each version.","title":"Versioning and Iterations"},{"location":"resources/visualizations/#finalizing-visualizations","text":"Process of finalizing visuals for presentation or publication. Feedback incorporation from team or test audiences.","title":"Finalizing Visualizations"},{"location":"resources/visualizations/#documentation-of-tools-and-resources","text":"List software, libraries, and tools used for visualization. Reference external resources or tutorials.","title":"Documentation of Tools and Resources"},{"location":"resources/visualizations/#conclusions","text":"Summarize the visualization process and contributions to the project. Reflect on lessons learned and potential future improvements.","title":"Conclusions"},{"location":"resources/visualizations/#references","text":"Cite external sources, inspirations, or frameworks used in visualization.","title":"References"},{"location":"resources/working_groups_and_postdocs/","text":"ESIIL Postdoctoral Researcher Responsibilities and Opportunities Primary Responsibilities Independent Research: Conducting self-proposed research projects. Adhering to open data principles. Data and Code Storage: Storing all research code and data in the designated ESIIL repository. Use of CyVerse: Utilizing CyVerse as the primary computational platform. Opportunities for Collaboration Joining Working Groups: Opportunity to collaborate with working groups within ESIIL, subject to invitation. Networking and Collaboration: Engaging in regular meetings and seminars for networking. Additional Responsibilities Reviewing Working Group Applications: Assisting in the review process of working group applications. Supporting Working Groups: Providing support to working groups in various capacities, even if not an author. Note Primary research commitments should be prioritized unless otherwise directed by supervisors or ESIIL's administrative body. This framework ensures that ESIIL postdocs balance independent research with collaborative opportunities, adhering to open data principles, and utilizing designated platforms for their work.","title":"ESIIL Postdoctoral Researcher Responsibilities and Opportunities"},{"location":"resources/working_groups_and_postdocs/#esiil-postdoctoral-researcher-responsibilities-and-opportunities","text":"","title":"ESIIL Postdoctoral Researcher Responsibilities and Opportunities"},{"location":"resources/working_groups_and_postdocs/#primary-responsibilities","text":"Independent Research: Conducting self-proposed research projects. Adhering to open data principles. Data and Code Storage: Storing all research code and data in the designated ESIIL repository. Use of CyVerse: Utilizing CyVerse as the primary computational platform.","title":"Primary Responsibilities"},{"location":"resources/working_groups_and_postdocs/#opportunities-for-collaboration","text":"Joining Working Groups: Opportunity to collaborate with working groups within ESIIL, subject to invitation. Networking and Collaboration: Engaging in regular meetings and seminars for networking.","title":"Opportunities for Collaboration"},{"location":"resources/working_groups_and_postdocs/#additional-responsibilities","text":"Reviewing Working Group Applications: Assisting in the review process of working group applications. Supporting Working Groups: Providing support to working groups in various capacities, even if not an author.","title":"Additional Responsibilities"},{"location":"resources/working_groups_and_postdocs/#note","text":"Primary research commitments should be prioritized unless otherwise directed by supervisors or ESIIL's administrative body. This framework ensures that ESIIL postdocs balance independent research with collaborative opportunities, adhering to open data principles, and utilizing designated platforms for their work.","title":"Note"}]}